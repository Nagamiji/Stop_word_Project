{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic Analysis & Recommendations\n",
    "\n",
    "This final notebook analyzes the linguistic composition of the Khmer stopword list. \n",
    "We aim to understand **which types** of words (e.g., Prepositions, Particles, Pronouns) contribute most to the \"noise\" in Khmer text.\n",
    "\n",
    "## Key Differences from Previous Steps\n",
    "- **Notebook 02**: Analyzed frequency at the *word* level (TF-IDF).\n",
    "- **Notebook 03**: Evaluated the *system* performance (IR).\n",
    "- **Notebook 04 (This)**: Analyzes at the **Linguistic Category** level to form standard recommendations.\n",
    "\n",
    "## Goal\n",
    "Provide a standardized recommendation for Khmer Stopword Removal: **Which categories should always be removed?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "# Ensure plots look nice\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['font.family'] = 'sans-serif' # Fallback for non-Khmer fonts in charts if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Analyze the Stopword List Structure\n",
    "First, we load the annotated stopword file to see the distribution of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS_PATH = \"../stopwords/FIle_Stopwords.csv\"\n",
    "\n",
    "if not os.path.exists(STOPWORDS_PATH):\n",
    "    print(\"Stopword file not found! Please check the path.\")\n",
    "else:\n",
    "    # Load CSV\n",
    "    df_sw = pd.read_csv(STOPWORDS_PATH)\n",
    "    \n",
    "    # Clean column names (strip spaces)\n",
    "    df_sw.columns = [c.strip() for c in df_sw.columns]\n",
    "    \n",
    "    # Standardize Group Names\n",
    "    df_sw['linguistic_group'] = df_sw['linguistic_group'].str.strip().str.title()\n",
    "    \n",
    "    print(f\"Total unique terms loaded: {len(df_sw)}\")\n",
    "    print(\"Columns:\", df_sw.columns.tolist())\n",
    "    display(df_sw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count terms per category\n",
    "category_counts = df_sw['linguistic_group'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=category_counts.values, y=category_counts.index, palette=\"viridis\")\n",
    "plt.title(\"Number of Stopwords per Linguistic Category\")\n",
    "plt.xlabel(\"Count of Unique Terms\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyze Usage Frequency in Corpus\n",
    "It's not just about how many *words* are in a category, but **how often** they appear in real text.\n",
    "A category with only 5 words (like 'Particles') might account for 20% of the total word count in a document!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Corpus (Sample)\n",
    "DATA_PATH = \"../data/raw/news_text_file_150k.txt\"\n",
    "\n",
    "def get_corpus_word_counts(filepath, limit=5000):\n",
    "    total_word_counts = Counter()\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        return total_word_counts\n",
    "    \n",
    "    try:\n",
    "        from khmernltk import word_tokenize\n",
    "    except ImportError:\n",
    "        print(\"Please install khmer-nltk first!\")\n",
    "        return total_word_counts\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= limit: break\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                tokens = word_tokenize(line)\n",
    "                total_word_counts.update(tokens)\n",
    "    return total_word_counts\n",
    "\n",
    "# Get counts from sample\n",
    "corpus_counts = get_corpus_word_counts(DATA_PATH, limit=2000)\n",
    "print(f\"Analyzed sample corpus. Found {len(corpus_counts)} unique tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each stopword to its total frequency in the corpus\n",
    "df_sw['corpus_frequency'] = df_sw['term'].apply(lambda x: corpus_counts.get(x, 0))\n",
    "\n",
    "# Group by Category to see total noise contribution\n",
    "category_impact = df_sw.groupby('linguistic_group')['corpus_frequency'].sum().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=category_impact.values, y=category_impact.index, palette=\"magma\")\n",
    "plt.title(\"Total Frequency in Corpus by Category (Noise Contribution)\")\n",
    "plt.xlabel(\"Total Occurrences in Corpus Sample\")\n",
    "plt.grid(axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Findings & Recommendations\n",
    "\n",
    "Based on the data above, we categorize stopword groups into **Tiers of Importance** for removal.\n",
    "\n",
    "### Tier 1: Critical to Remove (High Frequency / Low Meaning)\n",
    "These are purely functional and appear constantly.\n",
    "- **Particles (Ex: ក៏, នូវ, នៃ)**: These often have the highest frequency and zero semantic value in search.\n",
    "- **Prepositions (Ex: នៅ, ក្នុង, ពី)**: Necessary for grammar but noise for topic modeling.\n",
    "- **Conjunctions (Ex: និង, ហើយ, ប៉ុន្តែ)**: Connectors that don't change the topic.\n",
    "\n",
    "### Tier 2: Recommended to Remove (Medium Frequency)\n",
    "- **Pronouns (Ex: ខ្ញុំ, គេ, ឯង)**: Usually safe to remove, unless doing \"Author Identification\" or specific Entity Extraction.\n",
    "- **Determiners/Quantifiers (Ex: នេះ, នោះ, ខ្លះ)**: Very frequent, low information content.\n",
    "\n",
    "### Tier 3: Context Dependent (Keep for some tasks)\n",
    "- **Auxilliary Verbs (Ex: បាន, កំពុង)**: Temporal markers. Useful for Sentiment Analysis (tense matters) but not for Keyword Search.\n",
    "- **Numbers**: Often removed, but crucial for financial/technical documents.\n",
    "\n",
    "### Tier 4: Do Not Remove (Content Words)\n",
    "- If any nouns or specific verbs ended up in the list, they should be filtered out (as we did in Notebook 03)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of 'noise' removed if we drop Tier 1 & 2\n",
    "total_corpus_tokens = sum(corpus_counts.values())\n",
    "stopwords_total_tokens = df_sw['corpus_frequency'].sum()\n",
    "\n",
    "print(f\"Total Tokens in Sample: {total_corpus_tokens}\")\n",
    "print(f\"Tokens identified as Stopwords: {stopwords_total_tokens}\")\n",
    "print(f\"Percentage of Text Reduced: {stopwords_total_tokens / total_corpus_tokens * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nThis massive reduction (~30-50% usually) explains why IR performance improves: \")\n",
    "print(\"The model can focus on the remaining content words that actually carry meaning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Liguistic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
