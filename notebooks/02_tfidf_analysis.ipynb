{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "297aa556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d74f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess / normalize Khmer text\n",
    "\n",
    "def normalize_khmer_spaces(text):\n",
    "    \"\"\"Replace invisible spaces and normalize multiple spaces\"\"\"\n",
    "    text = text.replace('\\u200b', ' ')  # zero-width space\n",
    "    text = text.replace('\\xa0', ' ')    # non-breaking space\n",
    "    text = text.replace('\\u200c', ' ')  # zero-width non-joiner\n",
    "    text = text.replace('\\u200d', ' ')  # zero-width joiner\n",
    "    text = re.sub(r'\\s+', ' ', text)    # replace multiple spaces with single space\n",
    "    return text.strip()\n",
    "\n",
    "def load_documents(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        docs = [line.strip() for line in f if line.strip()]\n",
    "    # Normalize spaces\n",
    "    docs = [normalize_khmer_spaces(doc) for doc in docs]\n",
    "    return docs\n",
    "\n",
    "docs_with_sw = load_documents(\"dataset_with_stopwords.txt\")\n",
    "docs_without_sw = load_documents(\"dataset_remove_stopwords.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bbd29e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize documents (split by space)\n",
    "tokenized_docs_with_sw = [doc.split(\" \") for doc in docs_with_sw]\n",
    "tokenized_docs_without_sw = [doc.split(\" \") for doc in docs_without_sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1540ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(doc_tokens):\n",
    "    \"\"\"Compute term frequency for a single document\"\"\"\n",
    "    tf = {}\n",
    "    word_count = Counter(doc_tokens)\n",
    "    total_words = len(doc_tokens)\n",
    "    for word, count in word_count.items():\n",
    "        tf[word] = count / total_words\n",
    "    return tf\n",
    "\n",
    "tf_docs_with_sw = [compute_tf(doc) for doc in tokenized_docs_with_sw]\n",
    "tf_docs_without_sw = [compute_tf(doc) for doc in tokenized_docs_without_sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52e923db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Optimized IDF computation\n",
    "def compute_idf_fast(tokenized_docs):\n",
    "    \"\"\"Compute IDF using Counter (single pass, much faster)\"\"\"\n",
    "    N = len(tokenized_docs)\n",
    "    df_counter = Counter()\n",
    "\n",
    "    # Count in how many documents each word appears\n",
    "    for doc in tokenized_docs:\n",
    "        unique_words = set(doc)  # only count each word once per doc\n",
    "        df_counter.update(unique_words)\n",
    "\n",
    "    # Compute IDF\n",
    "    idf = {word: math.log(N / (df_counter[word] + 1)) for word in df_counter}\n",
    "    return idf\n",
    "\n",
    "idf_with_sw = compute_idf_fast(tokenized_docs_with_sw)\n",
    "idf_without_sw = compute_idf_fast(tokenized_docs_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4c4f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(tf_doc, idf):\n",
    "    \"\"\"Compute TF-IDF for a single document\"\"\"\n",
    "    return {word: tf_value * idf.get(word, 0) for word, tf_value in tf_doc.items()}\n",
    "\n",
    "tfidf_docs_with_sw = [compute_tfidf(tf, idf_with_sw) for tf in tf_docs_with_sw]\n",
    "tfidf_docs_without_sw = [compute_tfidf(tf, idf_without_sw) for tf in tf_docs_without_sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f947b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words with stopwords (first doc):\n",
      "ឥណ្ឌា: 0.3013\n",
      "របៀបវារៈ: 0.1445\n",
      "ជ្រុងជ្រោយ: 0.1444\n",
      "ការធ្វើដំណើរ: 0.1366\n",
      "កិច្ចពិភាក្សា: 0.1366\n",
      "ការអញ្ជើញ: 0.1357\n",
      "សារៈសំខាន់: 0.1269\n",
      "សេចក្តីថ្លែងការណ៍: 0.1197\n",
      "ទាំងមូល: 0.1172\n",
      "ទស្សនកិច្ច: 0.1146\n",
      "\n",
      "Top words without stopwords (first doc):\n",
      "ឥណ្ឌា: 0.5214\n",
      "របៀបវារៈ: 0.2501\n",
      "ជ្រុងជ្រោយ: 0.2499\n",
      "ការធ្វើដំណើរ: 0.2364\n",
      "កិច្ចពិភាក្សា: 0.2364\n",
      "ការអញ្ជើញ: 0.2349\n",
      "សារៈសំខាន់: 0.2196\n",
      "សេចក្តីថ្លែងការណ៍: 0.2072\n",
      "ទាំងមូល: 0.2029\n",
      "ទស្សនកិច្ច: 0.1983\n"
     ]
    }
   ],
   "source": [
    "#  Memory-efficient: Get top N words per document\n",
    "def top_tfidf_words(tfidf_doc, top_n=10):\n",
    "    \"\"\"Return top N words and their TF-IDF scores\"\"\"\n",
    "    return sorted(tfidf_doc.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "# Example: top 10 words for first document\n",
    "print(\"Top words with stopwords (first doc):\")\n",
    "for w, s in top_tfidf_words(tfidf_docs_with_sw[0]):\n",
    "    print(f\"{w}: {s:.4f}\")\n",
    "\n",
    "print(\"\\nTop words without stopwords (first doc):\")\n",
    "for w, s in top_tfidf_words(tfidf_docs_without_sw[0]):\n",
    "    print(f\"{w}: {s:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ad9db4",
   "metadata": {},
   "source": [
    "The TF-IDF scores increase for meaningful content words after removing stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8fc8efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 words WITH stopwords (corpus-level):\n",
      "ឯកច្ឆន្ច: 6.6530\n",
      "អសុពលភាព: 6.6530\n",
      "ហ្សូអ៊ី: 5.5442\n",
      "គតេ: 4.4353\n",
      "ឣនុធម្មចារី: 3.6961\n",
      "វេទព្វជាតក: 3.6961\n",
      "កិម្បក្កជាតក: 3.6961\n",
      "សុទ្ធ់: 3.6961\n",
      "សីហធម្មជាតក: 3.6961\n",
      "កន្ថរ៉ា: 3.6961\n",
      "ហ្វហ: 3.6961\n",
      "វានរជាតក: 3.6961\n",
      "បច្ចុប្បន្នៈ: 3.6961\n",
      "ចូច: 3.6961\n",
      "ពុទ្ធារី: 3.6961\n",
      "វីតា: 3.6961\n",
      "ឧទាយិវគ្គ: 3.6961\n",
      "អត្ថទ: 3.6961\n",
      "ព្រះតន្តិ: 3.6961\n",
      "ព្រះតន្តី: 3.6961\n",
      "\n",
      "Top 20 words WITHOUT stopwords (corpus-level):\n",
      "អសុពលភាព: 11.0883\n",
      "ប្រូតូកាទុ: 11.0883\n",
      "ត្រូវៈ: 11.0883\n",
      "ឯកច្ឆន្ច: 8.3162\n",
      "អ្នកសង្កេតឃើញ: 5.5442\n",
      "ទៅកាន់ខ្សែភាពយន្ត: 5.5442\n",
      "សាលួត: 5.5442\n",
      "ឃុយ: 5.5442\n",
      "អឿង: 5.5442\n",
      "ដូលីន: 5.5442\n",
      "បទិគះ: 5.5442\n",
      "ភេញ: 5.5442\n",
      "យមកៈ: 5.5442\n",
      "រូបរាងៈ: 5.5442\n",
      "រ៉ាដូ: 5.5442\n",
      "បឹងឆ្មារ: 5.5442\n",
      "ហ្សូអ៊ី: 5.5442\n",
      "ភេក្នុង: 5.5442\n",
      "ប៉ូកាសុី: 5.5442\n",
      "ព្រះពុទ្ធញ្ញាណ: 5.5442\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Compute average TF-IDF for each word across corpus\n",
    "def average_tfidf(tfidf_docs):\n",
    "    word_sum = defaultdict(float)\n",
    "    word_count = defaultdict(int)\n",
    "    \n",
    "    for doc in tfidf_docs:\n",
    "        for word, score in doc.items():\n",
    "            word_sum[word] += score\n",
    "            word_count[word] += 1\n",
    "    \n",
    "    avg_tfidf = {w: word_sum[w] / word_count[w] for w in word_sum}\n",
    "    return dict(sorted(avg_tfidf.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "avg_with_sw = average_tfidf(tfidf_docs_with_sw)\n",
    "avg_without_sw = average_tfidf(tfidf_docs_without_sw)\n",
    "\n",
    "# Display top 20 words across corpus\n",
    "print(\"Top 20 words WITH stopwords (corpus-level):\")\n",
    "for w, s in list(avg_with_sw.items())[:20]:\n",
    "    print(f\"{w}: {s:.4f}\")\n",
    "\n",
    "print(\"\\nTop 20 words WITHOUT stopwords (corpus-level):\")\n",
    "for w, s in list(avg_without_sw.items())[:20]:\n",
    "    print(f\"{w}: {s:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69391d3f",
   "metadata": {},
   "source": [
    "##### integrate Khmer stop-word dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8e5bb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4449b48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORD_GROUPS = {\n",
    "    \"Conjunctions\",\n",
    "    \"Pronouns\",\n",
    "    \"Determiners & Quantifiers\",\n",
    "    \"Prepositions / Relational Words\",\n",
    "    \"Auxiliary Verbs / Aspect Markers\",\n",
    "    \"Particles & Discourse Markers\",\n",
    "    \"Question & Negation Words\",\n",
    "    \"Function Nouns\",\n",
    "    \"Numbers & Time Expressions\",\n",
    "    \"Politeness & Honorifics\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b69975d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected columns: ['term', 'linguistic_group']\n",
      "Loaded Khmer stopwords: 615\n"
     ]
    }
   ],
   "source": [
    "def load_stopwords_from_annotated_csv(csv_path):\n",
    "    stopwords = set()\n",
    "\n",
    "    with open(csv_path, encoding=\"utf-8-sig\") as f:\n",
    "        reader = csv.DictReader(f, delimiter=\",\")\n",
    "\n",
    "        print(\"Detected columns:\", reader.fieldnames)\n",
    "\n",
    "        for row in reader:\n",
    "            term = row[\"term\"].strip()\n",
    "            group = row[\"linguistic_group\"].strip().lower()\n",
    "\n",
    "            # Remove everything EXCEPT content words\n",
    "            if \"content word\" not in group:\n",
    "                stopwords.add(term)\n",
    "\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "KHMER_STOPWORDS = load_stopwords_from_annotated_csv(\"FIle_Stopwords.csv\")\n",
    "print(\"Loaded Khmer stopwords:\", len(KHMER_STOPWORDS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7854ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'កន្លែងណាមួយ',\n",
       " 'កម្រិត',\n",
       " 'កាន់តែ',\n",
       " 'កាន់តែច្រើន',\n",
       " 'ការជ្រៀតជ្រែក',\n",
       " 'ការបញ្ចប់',\n",
       " 'ការបើកចំហ',\n",
       " 'ការផ្លាស់ប្តូរ',\n",
       " 'ការរៀបចំ',\n",
       " 'ការសម្រេច',\n",
       " 'កាល',\n",
       " 'កាលណា',\n",
       " 'កាលនោះ',\n",
       " 'កាលបើ',\n",
       " 'កាលពី',\n",
       " 'កាលពីមុន',\n",
       " 'កើត',\n",
       " 'កំពុង',\n",
       " 'ក៏',\n",
       " 'ក៏ត្រូវ',\n",
       " 'ក្នុង',\n",
       " 'ក្បែរ',\n",
       " 'ក្បែរនេះ',\n",
       " 'ក្រោម',\n",
       " 'ក្រោយ',\n",
       " 'ខ្លួន',\n",
       " 'គាត់',\n",
       " 'គួរតែ',\n",
       " 'គួរសម',\n",
       " 'គួរឲ្យកត់សម្គាល់',\n",
       " 'គេ',\n",
       " 'គេប៉ុណ្ណោះ',\n",
       " 'គ្មានកន្លែង',\n",
       " 'គ្មាននរណា',\n",
       " 'គ្រប់',\n",
       " 'គ្រប់គ្នា',\n",
       " 'គ្រាន់តែ',\n",
       " 'ឃើញតែ',\n",
       " 'ចង់',\n",
       " 'ចាប់តាំងពី',\n",
       " 'ចុះ',\n",
       " 'ចូល',\n",
       " 'ចូលទៅ',\n",
       " 'ចូលរួម',\n",
       " 'ចេញ',\n",
       " 'ចេញពី',\n",
       " 'ចំណែក',\n",
       " 'ចំណែកឯ',\n",
       " 'ចំនួន',\n",
       " 'ចំនួនច្រើន',\n",
       " 'ចំនួនបន្ថែម',\n",
       " 'ចំពោះ',\n",
       " 'ចំពោះបញ្ហា',\n",
       " 'ច្បាស់ណាស់',\n",
       " 'ច្បាស់លាស់',\n",
       " 'ច្រើន',\n",
       " 'ច្រើនជាងគេ',\n",
       " 'ច្រើនណាស់',\n",
       " 'ឆ្លាស់គ្នា',\n",
       " 'ឆ្លាស់ទៅ',\n",
       " 'ឆ្លើយ',\n",
       " 'ឆ្លើយឆ្លង',\n",
       " 'ឆ្លើយតប',\n",
       " 'ឆ្លៀតកន្លែង',\n",
       " 'ឆ្លៀតការបញ្ចប់',\n",
       " 'ឆ្លៀតគ្នា',\n",
       " 'ឆ្លៀតឃើញ',\n",
       " 'ឆ្លៀតចាប់ផ្តើម',\n",
       " 'ឆ្លៀតចូលរួម',\n",
       " 'ឆ្លៀតចំណេញ',\n",
       " 'ឆ្លៀតចំណែក',\n",
       " 'ឆ្លៀតជាមួយនឹង',\n",
       " 'ឆ្លៀតដាក់',\n",
       " 'ឆ្លៀតតែងតាំង',\n",
       " 'ឆ្លៀតតំរង់',\n",
       " 'ឆ្លៀតត្រូវ',\n",
       " 'ឆ្លៀតទៅ',\n",
       " 'ឆ្លៀតធ្វើ',\n",
       " 'ឆ្លៀតបង្កើត',\n",
       " 'ឆ្លៀតបន្ថែម',\n",
       " 'ឆ្លៀតបើកបង្ហាញ',\n",
       " 'ឆ្លៀតផ្តល់',\n",
       " 'ឆ្លៀតពេល',\n",
       " 'ឆ្លៀតមក',\n",
       " 'ឆ្លៀតយក',\n",
       " 'ឆ្លៀតលេង',\n",
       " 'ឆ្លៀតវេលា',\n",
       " 'ឆ្លៀតសន្ទក',\n",
       " 'ឆ្លៀតសន្ទនា',\n",
       " 'ឆ្លៀតសម័យ',\n",
       " 'ឆ្លៀតសម្រាក',\n",
       " 'ឆ្លៀតសម្លៀកបំពាក់',\n",
       " 'ឆ្លៀតសួរគ្នា',\n",
       " 'ឆ្លៀតអាន',\n",
       " 'ឆ្លៀតអ្វីដែល',\n",
       " 'ជា',\n",
       " 'ជាដរាប',\n",
       " 'ជាថ្មី',\n",
       " 'ជាធម្មតា',\n",
       " 'ជានិច្ច',\n",
       " 'ជាប់នឹង',\n",
       " 'ជាមួយនឹង',\n",
       " 'ជិត',\n",
       " 'ជំរាប',\n",
       " 'ជំរាបសួរ',\n",
       " 'ឈប់',\n",
       " 'ឈប់និយាយ',\n",
       " 'ឈានដល់',\n",
       " 'ឈានទៅ',\n",
       " 'ញែក',\n",
       " 'ដកចេញ',\n",
       " 'ដល់',\n",
       " 'ដល់កន្លែង',\n",
       " 'ដល់ហើយ',\n",
       " 'ដឹងច្បាស់',\n",
       " 'ដឹងត្រូវ',\n",
       " 'ដឹងពី',\n",
       " 'ដឹងហើយ',\n",
       " 'ដឹងហើយឯង',\n",
       " 'ដូច',\n",
       " 'ដូចគ្នា',\n",
       " 'ដូចជា',\n",
       " 'ដូចដែល',\n",
       " 'ដូចទៀង',\n",
       " 'ដូចនេះហើយ',\n",
       " 'ដូចបែបនេះ',\n",
       " 'ដូចរបៀប',\n",
       " 'ដូច្នេះ',\n",
       " 'ដូច្នេះហើយ',\n",
       " 'ដើម្បី',\n",
       " 'ដើម្បីឆ្លើយ',\n",
       " 'ដើម្បីឲ្យ',\n",
       " 'ដើរតួ',\n",
       " 'ដើរទៅ',\n",
       " 'ដែរ',\n",
       " 'ដែរគ្រប់',\n",
       " 'ដែល',\n",
       " 'ដែលជា',\n",
       " 'ដែលនឹង',\n",
       " 'ដែលអាច',\n",
       " 'ដោយសារ',\n",
       " 'ដោះស្រាយ',\n",
       " 'ដំណាក់កាល',\n",
       " 'ដំណឹង',\n",
       " 'ដំណើរ',\n",
       " 'ដំណើរការ',\n",
       " 'ដំណោះស្រាយ',\n",
       " 'ដ៏',\n",
       " 'ណា',\n",
       " 'ណាស់',\n",
       " 'ណាស់ណា',\n",
       " 'តម្លៃ',\n",
       " 'តាម',\n",
       " 'តាមចន្លោះ',\n",
       " 'តាមទំលាប់',\n",
       " 'តាមរយៈ',\n",
       " 'តើ',\n",
       " 'តើធ្វើម្តេច',\n",
       " 'តើនរណា',\n",
       " 'តើបែបណា',\n",
       " 'តើបែបនេះ',\n",
       " 'តើហើយ',\n",
       " 'តើហេតុអ្វី',\n",
       " 'តើអាចយ៉ាងណា',\n",
       " 'តើអ្វី',\n",
       " 'តែ',\n",
       " 'តែប៉ុណ្ណោះ',\n",
       " 'ត្រង់តែ',\n",
       " 'ត្រង់នោះ',\n",
       " 'ត្រលប់វិញ',\n",
       " 'ត្រលះ',\n",
       " 'ត្រឹម',\n",
       " 'ត្រឹមតែ',\n",
       " 'ត្រឹមនោះ',\n",
       " 'ត្រូវជាមួយ',\n",
       " 'ត្រូវតែ',\n",
       " 'ត្រូវបាន',\n",
       " 'ត្រៀម',\n",
       " 'ត្រៀមចេញ',\n",
       " 'ទទួល',\n",
       " 'ទទួលខុសត្រូវ',\n",
       " 'ទទួលយក',\n",
       " 'ទល់នឹង',\n",
       " 'ទស្សនា',\n",
       " 'ទាក់ទង',\n",
       " 'ទាន់',\n",
       " 'ទាន់ពេល',\n",
       " 'ទាស់',\n",
       " 'ទាស់ដាច់',\n",
       " 'ទាំង',\n",
       " 'ទាំងនេះ',\n",
       " 'ទាំងនេះហើយ',\n",
       " 'ទាំងបី',\n",
       " 'ទាំងអស់',\n",
       " 'ទាំងអស់គ្នា',\n",
       " 'ទុក',\n",
       " 'ទុកចិត្ត',\n",
       " 'ទុកសម្រាក',\n",
       " 'ទូទៅដែរ',\n",
       " 'ទូទៅប៉ុន្មាន',\n",
       " 'ទេ',\n",
       " 'ទោះបី',\n",
       " 'ទោះបីជា',\n",
       " 'ទោះបីយ៉ាងណា',\n",
       " 'ទៅកាន់',\n",
       " 'ទៅជាមួយ',\n",
       " 'ទៅជាយ៉ាងណា',\n",
       " 'ទៅជាអ្វី',\n",
       " 'ទៅដល់ទីនោះ',\n",
       " 'ទៅណាក៏ដោយ',\n",
       " 'ទៅបើក',\n",
       " 'ទៅមើល',\n",
       " 'ទៅលើ',\n",
       " 'ទៅលើសទៀត',\n",
       " 'ទៅវិញ',\n",
       " 'ទៅហើយ',\n",
       " 'ទំនងជា',\n",
       " 'ទំនេរ',\n",
       " 'ទំនេរជាប្រការ',\n",
       " 'ធ្លាក់ចុះ',\n",
       " 'ធ្លាក់ទាំងស្រុង',\n",
       " 'ធ្លាប់ជាមុន',\n",
       " 'ធ្លាប់និយាយ',\n",
       " 'ធ្វើការងារ',\n",
       " 'ធ្វើបច្ចុប្បន្នភាព',\n",
       " 'ធ្វើឱ្យ',\n",
       " 'ធ្វើឲ្យទាន់ពេល',\n",
       " 'ធ្វើឲ្យល្អ',\n",
       " 'នរណា',\n",
       " 'នាង',\n",
       " 'និង',\n",
       " 'នឹកចិត្ត',\n",
       " 'នឹកស្មាន',\n",
       " 'នឹកស្រឡាញ់',\n",
       " 'នឹង',\n",
       " 'នឹងកើតឡើង',\n",
       " 'នឹងកំណត់',\n",
       " 'នឹងតែ',\n",
       " 'នឹងត្រូវ',\n",
       " 'នឹងបន្ដិច',\n",
       " 'នឹងអាចធ្វើបាន',\n",
       " 'នូវ',\n",
       " 'នេះ',\n",
       " 'នេះក៏បាន',\n",
       " 'នេះក៏អាច',\n",
       " 'នេះជា',\n",
       " 'នេះតែប៉ុណ្ណោះ',\n",
       " 'នេះត្រូវជា',\n",
       " 'នេះហើយ',\n",
       " 'នៃ',\n",
       " 'នោះ',\n",
       " 'នោះជា',\n",
       " 'នៅកន្លែង',\n",
       " 'នៅចំពោះមុខ',\n",
       " 'នៅជិតតែ',\n",
       " 'នៅតាម',\n",
       " 'នៅតាមកន្លែង',\n",
       " 'នៅតែ',\n",
       " 'នៅទីនោះ',\n",
       " 'នៅមកវិញ',\n",
       " 'នៅមុខគេ',\n",
       " 'នៅឡើយទេ',\n",
       " 'បញ្ចូល',\n",
       " 'បញ្ជាក់',\n",
       " 'បញ្ជាក់ថា',\n",
       " 'បន្តិច',\n",
       " 'បន្ថយ',\n",
       " 'បន្ថែមទៀត',\n",
       " 'បន្ទាប់',\n",
       " 'បន្ទាប់ពី',\n",
       " 'បន្ទាប់ពីនេះ',\n",
       " 'បន្ទាប់មក',\n",
       " 'បាន',\n",
       " 'បើក',\n",
       " 'បើកទ្វារ',\n",
       " 'បើតម្រូវ',\n",
       " 'បើតែប៉ុណ្ណោះ',\n",
       " 'បើត្រូវជា',\n",
       " 'បើមាន',\n",
       " 'បើមិន',\n",
       " 'បំពេញការងារ',\n",
       " 'ប៉ុណ្ណឹងហើយ',\n",
       " 'ប៉ុណ្ណោះ',\n",
       " 'ប៉ុន្តែ',\n",
       " 'ប៉ុន្នឹងក៏បាន',\n",
       " 'ប៉ុន្មានដង',\n",
       " 'ប៉ុន្មានពេលនោះ',\n",
       " 'ប្រកប',\n",
       " 'ប្រកបដោយ',\n",
       " 'ប្រកបដោយភាព',\n",
       " 'ប្រគល់ទៅ',\n",
       " 'ប្រគល់ឲ្យ',\n",
       " 'ប្រតិបត្តិ',\n",
       " 'ប្រសិនបើ',\n",
       " 'ប្រហែល',\n",
       " 'ប្រហែលជា',\n",
       " 'ប្រហែលជាត្រូវ',\n",
       " 'ប្រហែលជាមិន',\n",
       " 'ប្រហែលតែ',\n",
       " 'ប្រាកដ',\n",
       " 'ប្រាកដជាទេ',\n",
       " 'ប្រាកដថា',\n",
       " 'ប្រាកដមែន',\n",
       " 'ប្រាកដហើយ',\n",
       " 'ប្រាក់',\n",
       " 'ប្រាប់ខ្លួនឯង',\n",
       " 'ប្រាប់ទៅ',\n",
       " 'ប្រាំបី',\n",
       " 'ប្រាំបួន',\n",
       " 'ប្រាំពីរ',\n",
       " 'ប្រើប្រាស់',\n",
       " 'ប្រែ',\n",
       " 'ប្រែជាអ្វី',\n",
       " 'ប្រែទៅជាអ្វី',\n",
       " 'ផ្គត់ផ្គង់',\n",
       " 'ផ្គុំពាក្យ',\n",
       " 'ផ្ដល់តម្លៃដល់',\n",
       " 'ផ្ដើមជាមួយ',\n",
       " 'ផ្ដើមពី',\n",
       " 'ផ្ដើមសម្រាប់',\n",
       " 'ផ្តល់កម្រិតខ្ពស់',\n",
       " 'ផ្តល់កាតសន្តិភាព',\n",
       " 'ផ្តល់ការគិតគូរ',\n",
       " 'ផ្តល់ការងាយស្រួល',\n",
       " 'ផ្តល់ការងាយស្រួលយ៉ាងណា',\n",
       " 'ផ្តល់ការងារចេញ',\n",
       " 'ផ្តល់ការងារច្រើន',\n",
       " 'ផ្តល់ការងារអ្នក',\n",
       " 'ផ្តល់ការជូនដំណឹង',\n",
       " 'ផ្តល់ការទំនុកចិត្ត',\n",
       " 'ផ្តល់ការបរិបូរ',\n",
       " 'ផ្តល់ការផ្តោតអារម្មណ៍',\n",
       " 'ផ្តល់ការពន្យល់',\n",
       " 'ផ្តល់ការយល់ព្រម',\n",
       " 'ផ្តល់ការរាប់បញ្ចូល',\n",
       " 'ផ្តល់ការអនុវត្ត',\n",
       " 'ផ្តល់គុណតម្លៃ',\n",
       " 'ផ្តល់គោលបំណង',\n",
       " 'ផ្តល់ចំណែក',\n",
       " 'ផ្តល់ចំត្រូវ',\n",
       " 'ផ្តល់ចំអិន',\n",
       " 'ផ្តល់ជាអ្វី',\n",
       " 'ផ្តល់ជំនួយ',\n",
       " 'ផ្តល់តួនាទី',\n",
       " 'ផ្តល់ទំនុកចិត្តក្នុង',\n",
       " 'ផ្តល់ទំព័រ',\n",
       " 'ផ្តល់ទ្រទ្រង់',\n",
       " 'ផ្តល់នូវ',\n",
       " 'ផ្តល់នូវសេចក្តីឆ្លើយ',\n",
       " 'ផ្តល់បច្ចេកវិទ្យា',\n",
       " 'ផ្តល់បញ្ជូនគ្នា',\n",
       " 'ផ្តល់ផ្នែក',\n",
       " 'ផ្តល់ពេលវេលាឲ្យគ្នា',\n",
       " 'ផ្តល់ភាពងាយស្រួល',\n",
       " 'ផ្តល់ភាពជឿជាក់',\n",
       " 'ផ្តល់ភាពជោគជ័យ',\n",
       " 'ផ្តល់ភាពល្អ',\n",
       " 'ផ្តល់ភាពសំរាប់',\n",
       " 'ផ្តល់មុខងារចូល',\n",
       " 'ផ្តល់មេរៀន',\n",
       " 'ផ្តល់សិទ្ធិចូលរួម',\n",
       " 'ផ្តល់សុខភាពនិងសន្តិភាព',\n",
       " 'ផ្តល់សេចក្ដីណែនាំ',\n",
       " 'ផ្តល់សេចក្ដីពេញចិត្ត',\n",
       " 'ផ្តល់សេចក្តីព្រាង',\n",
       " 'ផ្តល់អត្តន័យ',\n",
       " 'ផ្តល់អត្ថន័យ',\n",
       " 'ផ្តល់អាការៈ',\n",
       " 'ផ្តល់អានុភាព',\n",
       " 'ផ្តល់អារម្មណ៍ដល់គ្នា',\n",
       " 'ផ្តល់អាហារូបករណ៍',\n",
       " 'ផ្តល់អំណោយ',\n",
       " 'ផ្តល់ឲ្យនរណាម្នាក់',\n",
       " 'ផ្ទាល់',\n",
       " 'ផ្ទាល់ខ្លួន',\n",
       " 'ផ្ទុយ',\n",
       " 'ផ្ទុយនឹង',\n",
       " 'ផ្ទុយពី',\n",
       " 'ផ្លាស់ប្ដូរជាច្រើន',\n",
       " 'ផ្លាស់ប្ដូរតាមពេលវេលា',\n",
       " 'ផ្លាស់ប្ដូរត្រឹមត្រូវ',\n",
       " 'ផ្លាស់ប្ដូរនិស្ស័យ',\n",
       " 'ផ្លាស់ប្ដូរពេលវេលា',\n",
       " 'ផ្លាស់ប្ដូរសម័យ',\n",
       " 'ពី',\n",
       " 'ពីព្រោះ',\n",
       " 'ពុំទាន់',\n",
       " 'ពុំមាន',\n",
       " 'ពេញចិត្ត',\n",
       " 'ពេញមួយថ្ងៃ',\n",
       " 'ពេញល្បង',\n",
       " 'ពេល',\n",
       " 'ពេលដែលកន្លង',\n",
       " 'ពេលដែលឆាប់',\n",
       " 'ពេលដែលត្រូវ',\n",
       " 'ពេលដែលមាន',\n",
       " 'ពេលដែលអាច',\n",
       " 'ពេលណា',\n",
       " 'ពេលណានោះហើយ',\n",
       " 'ពេលណាមិញ',\n",
       " 'ពេលណាមិញទេ',\n",
       " 'ពេលណាមិនទាន់',\n",
       " 'ពេលណាមួយ',\n",
       " 'ពេលណាមួយក៏បាន',\n",
       " 'ពេលនេះ',\n",
       " 'ពេលនេះហើយ',\n",
       " 'ពេលវេលាខ្លះ',\n",
       " 'ពេលវេលាច្រើន',\n",
       " 'ពេលវេលាដែល',\n",
       " 'ពេលវេលាប៉ុណ្ណោះ',\n",
       " 'ពោលគឺ',\n",
       " 'ពោលទៅ',\n",
       " 'ព្យាបាទច្រើនណាស់',\n",
       " 'ព្យាយាមតែប៉ុណ្ណោះ',\n",
       " 'ព្យាយាមយ៉ាងខ្លាំង',\n",
       " 'ព្រមទទួល',\n",
       " 'ព្រមទាំង',\n",
       " 'ព្រមសន្មត់',\n",
       " 'ព្រាត់ចិត្ត',\n",
       " 'ព្រួយ',\n",
       " 'ព្រោះតែ',\n",
       " 'ព្រោះតែវា',\n",
       " 'ព្រោះថា',\n",
       " 'ព្រោះថាអ្នក',\n",
       " 'ព្រោះហេតុនេះ',\n",
       " 'ព្រោះអញ្ចឹង',\n",
       " 'ភាគណា',\n",
       " 'ភាគបន្ថែម',\n",
       " 'ភាគពេញ',\n",
       " 'ភាគរយ',\n",
       " 'ភ្លឺ',\n",
       " 'ភ្លេចចង់',\n",
       " 'ភ្លេចឆ្លើយ',\n",
       " 'ភ្លេចដល់',\n",
       " 'ភ្លេចត្រូវ',\n",
       " 'ភ្លេចទៅ',\n",
       " 'ភ្លេចទៅវិញ',\n",
       " 'ភ្លេចបញ្ជាក់',\n",
       " 'ភ្លេចមិនបាន',\n",
       " 'ភ្លេចយក',\n",
       " 'ភ្លេចសុំព្យាបាល',\n",
       " 'ភ្លេចសូម្បីតែ',\n",
       " 'ភ្លេចសំខាន់',\n",
       " 'ភ្លេចស្នាម',\n",
       " 'ភ្លេចស្រឡាញ់',\n",
       " 'ភ្លេចហើយឯង',\n",
       " 'ភ្លេចអស់',\n",
       " 'ភ្លេចអ្វី',\n",
       " 'មាន',\n",
       " 'មិន',\n",
       " 'មិនខុស',\n",
       " 'មិនគិត',\n",
       " 'មិនគួរមើល',\n",
       " 'មិនឃើញ',\n",
       " 'មិនឃើញខុសគ្នា',\n",
       " 'មិនឃើញស្រួល',\n",
       " 'មិនងាយ',\n",
       " 'មិនងាយស្រួល',\n",
       " 'មិនចង់',\n",
       " 'មិនចាំបាច់',\n",
       " 'មិនច្បាស់',\n",
       " 'មិនជាក់',\n",
       " 'មិនជឿ',\n",
       " 'មិនដឹង',\n",
       " 'មិនត្រឹមតែ',\n",
       " 'មិនត្រឹមត្រូវ',\n",
       " 'មិនត្រូវការ',\n",
       " 'មិនត្រូវនិយាយ',\n",
       " 'មិនទទួល',\n",
       " 'មិនទាក់ទង',\n",
       " 'មិនទាន់',\n",
       " 'មិនទាន់ឃើញ',\n",
       " 'មិនទាន់ចប់',\n",
       " 'មិនទាន់ច្បាស់',\n",
       " 'មិនទាន់ដឹង',\n",
       " 'មិនទាន់ត្រូវ',\n",
       " 'មិនទាន់បាន',\n",
       " 'មិនទាន់បានទេ',\n",
       " 'មិនទាន់ល្អ',\n",
       " 'មិនទាន់អាច',\n",
       " 'មិនទំនេរ',\n",
       " 'មិនបង្ហាញ',\n",
       " 'មិនបញ្ចូល',\n",
       " 'មិនបន្ថែម',\n",
       " 'មិនបាច់',\n",
       " 'មិនបាច់ខ្លាំង',\n",
       " 'មិនបាច់ឃើញ',\n",
       " 'មិនបាច់ទាំងអស់',\n",
       " 'មិនបាច់បន្ថែម',\n",
       " 'មិនបាច់ព្រួយ',\n",
       " 'មិនបាន',\n",
       " 'មិនបានច្បាស់',\n",
       " 'មិនបានត្រឹមតែ',\n",
       " 'មិនបាននិយាយតែ',\n",
       " 'មិនបានលើកឡើង',\n",
       " 'មិនបានស្រួល',\n",
       " 'មិនព្រមទទួល',\n",
       " 'មិនព្រមទេ',\n",
       " 'មិនមាន',\n",
       " 'មិនមើលឃើញ',\n",
       " 'មិនមែន',\n",
       " 'មិនមែនទេ',\n",
       " 'មិនលើកលែង',\n",
       " 'មិនសម្ដែង',\n",
       " 'មិនសូវ',\n",
       " 'មិនសូវនិយាយ',\n",
       " 'មិនសំខាន់',\n",
       " 'មិនស្រប',\n",
       " 'មិនស្រលាញ់',\n",
       " 'មិនស្អិត',\n",
       " 'មិនស្អិតជាប់',\n",
       " 'មិនអនុញ្ញាត',\n",
       " 'មិនអស់សំណើច',\n",
       " 'មិនអាច',\n",
       " 'មិនអើពើ',\n",
       " 'មិនឲ្យ',\n",
       " 'មិនឲ្យសង្ឃឹម',\n",
       " 'មុខ',\n",
       " 'មួយ',\n",
       " 'មួយចំនួន',\n",
       " 'មួយឆ្នាំ',\n",
       " 'មួយទៀត',\n",
       " 'មួយភ្លែត',\n",
       " 'មែន',\n",
       " 'មែនទេ',\n",
       " 'មែនទែន',\n",
       " 'ម្នាក់',\n",
       " 'ម្នាក់ឯង',\n",
       " 'ម្យ៉ាងទៀត',\n",
       " 'យើង',\n",
       " 'យើងខ្ញុំ',\n",
       " 'យើងទាំងអស់',\n",
       " 'យ៉ាង',\n",
       " 'យ៉ាងខ្លាំង',\n",
       " 'យ៉ាងណា',\n",
       " 'យ៉ាងណាក៏ដោយ',\n",
       " 'យ៉ាងណាមិញ',\n",
       " 'របស់',\n",
       " 'របស់ខ្ញុំ',\n",
       " 'របស់គាត់',\n",
       " 'របស់នាង',\n",
       " 'របស់ពួកគេ',\n",
       " 'រហូត',\n",
       " 'រហូតដល់',\n",
       " 'រហូតទៅដល់',\n",
       " 'រីឯ',\n",
       " 'រឺក៏',\n",
       " 'រួមទាំង',\n",
       " 'លើ',\n",
       " 'លើកលែង',\n",
       " 'លោក',\n",
       " 'ល្អ',\n",
       " 'ល្អណាស់',\n",
       " 'ល្អបំផុត',\n",
       " 'វា',\n",
       " 'សព្វ',\n",
       " 'សព្វពេល',\n",
       " 'សម្រាប់',\n",
       " 'សរុបមក',\n",
       " 'សុទ្ធតែ',\n",
       " 'សូម',\n",
       " 'សូមជ្រាប',\n",
       " 'សូមទន្ទឹង',\n",
       " 'សូមទោស',\n",
       " 'សូមធ្វើ',\n",
       " 'សូមបញ្ជាក់',\n",
       " 'សូមប្រាប់',\n",
       " 'សូមពិនិត្យ',\n",
       " 'សូមព្យាបាល',\n",
       " 'សូមរង់ចាំ',\n",
       " 'សូមស្វាគមន៍',\n",
       " 'សូមឲ្យ',\n",
       " 'សោះ',\n",
       " 'សំរាប់',\n",
       " 'ស្គាល់',\n",
       " 'ស្ទើរតែ',\n",
       " 'ហាក់ដូចជា',\n",
       " 'ហាក់ទំនេរ',\n",
       " 'ហាក់នៅ',\n",
       " 'ហាក់បង្កប់',\n",
       " 'ហាក់បង្កើន',\n",
       " 'ហាក់បង្ហាញ',\n",
       " 'ហាក់បាក់',\n",
       " 'ហាក់បីដូចជា',\n",
       " 'ហាក់បីដូចម្តេច',\n",
       " 'ហាក់បែបនោះ',\n",
       " 'ហាក់មាន',\n",
       " 'ហើយ',\n",
       " 'ហើយត្រូវ',\n",
       " 'ហើយនឹង',\n",
       " 'ហេតុនេះ',\n",
       " 'ហេតុនេះហើយ',\n",
       " 'ហេតុអ្វី',\n",
       " 'ហ្នឹង',\n",
       " 'ឡើង',\n",
       " 'អញ្ចឹង',\n",
       " 'អត់',\n",
       " 'អាច',\n",
       " 'អាចជា',\n",
       " 'អាចនឹង',\n",
       " 'អំពី',\n",
       " 'អ៊ីចឹង',\n",
       " 'អ្នក',\n",
       " 'អ្នកណា',\n",
       " 'អ្នកទាំងអស់',\n",
       " 'អ្វី',\n",
       " 'អ្វីខ្លះ',\n",
       " 'អ្វីដែល',\n",
       " 'អ្វីៗ',\n",
       " 'ឥតឈប់ឈរ',\n",
       " 'ឥឡូវ',\n",
       " 'ឥឡូវនេះ',\n",
       " 'ឯ',\n",
       " 'ឯង',\n",
       " 'ឱកាស'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KHMER_STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c76062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords from tokenized docs\n",
    "\n",
    "def remove_stopwords(tokenized_docs, stopwords):\n",
    "    \"\"\"Return tokenized documents with stopwords removed\"\"\"\n",
    "    return [[w for w in doc if w not in stopwords] for doc in tokenized_docs]\n",
    "\n",
    "tokenized_docs_refined = remove_stopwords(tokenized_docs_with_sw, KHMER_STOPWORDS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52fe5787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompute TF, IDF, TF-IDF for refined docs\n",
    "\n",
    "tf_docs_refined = [compute_tf(doc) for doc in tokenized_docs_refined]\n",
    "idf_refined = compute_idf_fast(tokenized_docs_refined)\n",
    "tfidf_docs_refined = [compute_tfidf(tf, idf_refined) for tf in tf_docs_refined]\n",
    "\n",
    "avg_refined = average_tfidf(tfidf_docs_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67d8c43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 words WITH original stopwords\n",
      "ឯកច្ឆន្ច: 6.6530\n",
      "អសុពលភាព: 6.6530\n",
      "ហ្សូអ៊ី: 5.5442\n",
      "គតេ: 4.4353\n",
      "ឣនុធម្មចារី: 3.6961\n",
      "វេទព្វជាតក: 3.6961\n",
      "កិម្បក្កជាតក: 3.6961\n",
      "សុទ្ធ់: 3.6961\n",
      "សីហធម្មជាតក: 3.6961\n",
      "កន្ថរ៉ា: 3.6961\n",
      "ហ្វហ: 3.6961\n",
      "វានរជាតក: 3.6961\n",
      "បច្ចុប្បន្នៈ: 3.6961\n",
      "ចូច: 3.6961\n",
      "ពុទ្ធារី: 3.6961\n",
      "វីតា: 3.6961\n",
      "ឧទាយិវគ្គ: 3.6961\n",
      "អត្ថទ: 3.6961\n",
      "ព្រះតន្តិ: 3.6961\n",
      "ព្រះតន្តី: 3.6961\n",
      "\n",
      "Top 20 words WITHOUT initial stopwords\n",
      "អសុពលភាព: 11.0883\n",
      "ប្រូតូកាទុ: 11.0883\n",
      "ត្រូវៈ: 11.0883\n",
      "ឯកច្ឆន្ច: 8.3162\n",
      "អ្នកសង្កេតឃើញ: 5.5442\n",
      "ទៅកាន់ខ្សែភាពយន្ត: 5.5442\n",
      "សាលួត: 5.5442\n",
      "ឃុយ: 5.5442\n",
      "អឿង: 5.5442\n",
      "ដូលីន: 5.5442\n",
      "បទិគះ: 5.5442\n",
      "ភេញ: 5.5442\n",
      "យមកៈ: 5.5442\n",
      "រូបរាងៈ: 5.5442\n",
      "រ៉ាដូ: 5.5442\n",
      "បឹងឆ្មារ: 5.5442\n",
      "ហ្សូអ៊ី: 5.5442\n",
      "ភេក្នុង: 5.5442\n",
      "ប៉ូកាសុី: 5.5442\n",
      "ព្រះពុទ្ធញ្ញាណ: 5.5442\n",
      "\n",
      "Top 20 words AFTER applying Khmer stopword list\n",
      "អសុពលភាព: 11.0883\n",
      "ប្រូតូកាទុ: 11.0883\n",
      "ឯកច្ឆន្ច: 8.3162\n",
      "សាលួត: 5.5442\n",
      "ឃុយ: 5.5442\n",
      "អឿង: 5.5442\n",
      "ដូលីន: 5.5442\n",
      "បទិគះ: 5.5442\n",
      "យមកៈ: 5.5442\n",
      "រូបរាងៈ: 5.5442\n",
      "រ៉ាដូ: 5.5442\n",
      "បឹងឆ្មារ: 5.5442\n",
      "ហ្សូអ៊ី: 5.5442\n",
      "ប៉ូកាសុី: 5.5442\n",
      "ធម្មខក្ខន្ធ: 5.5442\n",
      "គីស៊ុន: 5.5442\n",
      "កកសស: 5.5442\n",
      "បរិប័ន្ន: 4.7521\n",
      "គតេ: 4.4353\n",
      "ព្រះព្រហ្មាធិរាជ: 3.6961\n"
     ]
    }
   ],
   "source": [
    "# Display comparison\n",
    "\n",
    "def print_top_words(title, avg_tfidf, top_n=20):\n",
    "    print(f\"\\n{title}\")\n",
    "    for w, s in list(avg_tfidf.items())[:top_n]:\n",
    "        print(f\"{w}: {s:.4f}\")\n",
    "\n",
    "print_top_words(\"Top 20 words WITH original stopwords\", avg_with_sw)\n",
    "print_top_words(\"Top 20 words WITHOUT initial stopwords\", avg_without_sw)\n",
    "print_top_words(\"Top 20 words AFTER applying Khmer stopword list\", avg_refined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1a0d8b",
   "metadata": {},
   "source": [
    "The TF-IDF computation highlights the most statistically significant words in the corpus. As expected, the top TF-IDF words are content words, proper nouns, or domain-specific terms rather than grammatical stopwords (ex: អសុពលភាព, ប្រូតូកាទុ), because true stopwords appear in almost all documents, giving them very low IDF values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63d5d805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Candidate stopwords to consider adding (manual review):\n",
      "['អសុពលភាព', 'ប្រូតូកាទុ', 'ឯកច្ឆន្ច', 'សាលួត', 'ឃុយ', 'អឿង', 'ដូលីន', 'បទិគះ', 'យមកៈ', 'រូបរាងៈ', 'រ៉ាដូ', 'បឹងឆ្មារ', 'ហ្សូអ៊ី', 'ប៉ូកាសុី', 'ធម្មខក្ខន្ធ', 'គីស៊ុន', 'កកសស', 'បរិប័ន្ន', 'គតេ', 'ព្រះព្រហ្មាធិរាជ', 'ព្រងិល', 'ឣនុបាទិយានោ', 'ឣនុធម្មចារី', 'វេទព្វជាតក', 'កិម្បក្កជាតក', 'សុទ្ធ់', 'សីហធម្មជាតក', 'បច្ចេកវិ', 'កន្ថរ៉ា', 'ហ្វហ', 'ហ្គ្រីឡូ', 'សទ្ទកម្ម', 'វានរជាតក', 'ប្រវត្តិការងារ', 'បច្ចុប្បន្នៈ', 'អ្នកសង្កេតឃើញ', 'ដើជ្រៃ', 'ចូច', 'ពុទ្ធារី', 'វីតា', 'ឧទាយិវគ្គ', 'អត្ថទ', 'ព្រះតន្តិ', 'ព្រះតន្តី', 'ទៅកាន់ខ្សែភាពយន្ត', 'ដារ៉', 'ម៉ាញ់', 'ឡឿក', 'បាពហុ', 'សន្ធាយ']\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Suggest candidate stopwords (optional)\n",
    "# Words still high in TF-IDF but not content words\n",
    "\n",
    "top_candidates = [w for w, s in list(avg_refined.items())[:50]]  # top 50\n",
    "print(\"\\nCandidate stopwords to consider adding (manual review):\")\n",
    "print(top_candidates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ac6e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate stopwords (from WITH original stopwords):\n",
      "[]\n",
      "\n",
      "Candidate stopwords (from WITHOUT initial stopwords):\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# def suggest_candidate_stopwords(tfidf_docs, top_n=50, min_doc_ratio=0.01):\n",
    "#     \"\"\"\n",
    "#     Suggest candidate stopwords based on:\n",
    "#     - Top N words by average TF-IDF\n",
    "#     - Appear in at least `min_doc_ratio` fraction of documents\n",
    "#     \"\"\"\n",
    "#     N_docs = len(tfidf_docs)\n",
    "    \n",
    "#     # Compute average TF-IDF\n",
    "#     avg_tfidf = average_tfidf(tfidf_docs)\n",
    "    \n",
    "#     # Take top N words\n",
    "#     top_words = list(avg_tfidf.keys())[:top_n]\n",
    "    \n",
    "#     # Count in how many documents each word appears\n",
    "#     word_doc_count = Counter()\n",
    "#     for doc in tfidf_docs:\n",
    "#         for word in doc:\n",
    "#             if word in top_words:\n",
    "#                 word_doc_count[word] += 1\n",
    "    \n",
    "#     # Filter by document frequency ratio\n",
    "#     candidates = [word for word, count in word_doc_count.items() if count / N_docs >= min_doc_ratio]\n",
    "#     return candidates\n",
    "\n",
    "# # Example\n",
    "# candidate_stopwords_with_sw = suggest_candidate_stopwords(tfidf_docs_with_sw, top_n=50, min_doc_ratio=0.01)\n",
    "# candidate_stopwords_without_sw = suggest_candidate_stopwords(tfidf_docs_without_sw, top_n=50, min_doc_ratio=0.01)\n",
    "\n",
    "# print(\"Candidate stopwords (from WITH original stopwords):\")\n",
    "# print(candidate_stopwords_with_sw)\n",
    "\n",
    "# print(\"\\nCandidate stopwords (from WITHOUT initial stopwords):\")\n",
    "# print(candidate_stopwords_without_sw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e93ddc",
   "metadata": {},
   "source": [
    "###### Simple IR Model Using TF-IDF and Top-K Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "283d47a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 similar documents WITH stopwords:\n",
      "[(0, 1.0000000000000002), (20044, 0.3732776475978078), (39001, 0.3665936743440593), (83277, 0.35764972611312484), (62285, 0.3469664539784011)]\n",
      "\n",
      "Top 5 similar documents WITHOUT stopwords:\n",
      "[(0, 0.9036564461309989), (20044, 0.3591125468901498), (116781, 0.3560611253905191), (62285, 0.354057607202223), (83277, 0.3520607628912101)]\n",
      "\n",
      "Overlap in Top-5 documents: 4 / 5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Compute cosine similarity between two sparse TF-IDF dicts\"\"\"\n",
    "    common_words = set(vec1.keys()) & set(vec2.keys())\n",
    "    num = sum(vec1[w] * vec2[w] for w in common_words)\n",
    "    denom = (np.sqrt(sum(v**2 for v in vec1.values())) * \n",
    "             np.sqrt(sum(v**2 for v in vec2.values())))\n",
    "    return num / denom if denom != 0 else 0\n",
    "\n",
    "def rank_documents(query_vec, tfidf_docs, top_k=5):\n",
    "    \"\"\"Return top K document indices ranked by similarity\"\"\"\n",
    "    scores = [(i, cosine_similarity(query_vec, doc)) for i, doc in enumerate(tfidf_docs)]\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scores[:top_k]\n",
    "\n",
    "# Example: use first document as query\n",
    "query_doc = tfidf_docs_with_sw[0]\n",
    "\n",
    "top_k_with_sw = rank_documents(query_doc, tfidf_docs_with_sw, top_k=5)\n",
    "top_k_without_sw = rank_documents(query_doc, tfidf_docs_without_sw, top_k=5)\n",
    "\n",
    "print(\"Top 5 similar documents WITH stopwords:\")\n",
    "print(top_k_with_sw)\n",
    "\n",
    "print(\"\\nTop 5 similar documents WITHOUT stopwords:\")\n",
    "print(top_k_without_sw)\n",
    "\n",
    "# Compare overlap\n",
    "indices_with_sw = set(idx for idx, _ in top_k_with_sw)\n",
    "indices_without_sw = set(idx for idx, _ in top_k_without_sw)\n",
    "overlap = indices_with_sw & indices_without_sw\n",
    "print(f\"\\nOverlap in Top-5 documents: {len(overlap)} / 5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ce66ba",
   "metadata": {},
   "source": [
    "We tested the impact of stopword removal on a TF-IDF based IR model. The Top-5 document retrieval using cosine similarity showed 4/5 overlap between with and without stopwords, indicating that removing stopwords slightly changed the ranking. \n",
    "\n",
    "This confirms that the Khmer stopword removal helps reduce noise without significantly impacting the retrieval of relevant documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ed7b2b",
   "metadata": {},
   "source": [
    "# Apply Stop word \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c34764e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DIAGNOSING: original_segmented_sentences.txt\n",
      "======================================================================\n",
      "\n",
      "📄 Sample Documents (first 5):\n",
      "  1. សេចក្តីថ្លែងការណ៍ ពាក់ព័ន្ធ នឹង ព័ត៌មាន នេះ   បាន ឲ្យដឹង ថា   លោក   នឹង ទៅ បំពេញ ទស្សនកិច្ច នៅ ប្រទេស ឥណ្ឌា   តាម ការអញ្ជើញ របស់ នាយក រដ្ឋមន្ត្រី ឥណ្ឌា   លោក   ដោយ ចាត់ទុក ការធ្វើដំណើរ នេះ ថា   មាន សា...\n",
      "  2. ផ្តន្ទាទោស ដាក់ ពន្ធនាការ ដោយ ព្យួរ ទោស   រឺ មិន ព្យួរ ទោស ក្នុង រយ   ពេល ឆ្នាំ កន្លងទៅ ទេ   នោះ តុលាការ អាច សំរេច មិន ដាក់ទោស   ប៉ុន្តែ ត្រូវ\n",
      "  3. នៅ ឆ្នាំ   ភាគ កណ្ដាល នៃ ចក្រភព បាន បែកបាក់   បង្កើត បានជា រាជាណាចក្រ តូច ឯករាជ្យ ជាច្រើន\n",
      "  4. ក្នុង នោះ រួម មាន   បួរ ហ្គូញ លើ   បួរ ហ្គូញ ត្រង់ ហ្ស៊ូរ៉ាន   និង អ៊ីតាលី\n",
      "  5. ចំណែកឯ តំបន់ ឡូរ៉ែ ន   ត្រូវបាន បញ្ចូល ទៅ ភាគ ខាងកើត   ក្លាយជា រាជាណាចក្រ ចំណុះ\n",
      "\n",
      "📊 Token Composition Analysis (30,036 tokens analyzed):\n",
      "  • Pure Khmer tokens:    30,036 (100.0%)\n",
      "  • Latin/Alphanumeric:        0 (  0.0%)\n",
      "  • Numeric:                   0 (  0.0%)\n",
      "  • Mixed script:              0 (  0.0%)\n",
      "\n",
      "📏 Token Length Analysis:\n",
      "  • All tokens - Avg: 4.9, Min: 1, Max: 20\n",
      "  • Khmer tokens - Avg: 4.9, Min: 1, Max: 20\n",
      "\n",
      "⚠️  Suspicious Non-Khmer Tokens (sample of 20):\n",
      "\n",
      "✓ Sample Khmer Tokens (first 20 unique):\n",
      "   1. ទ្រទ្រង់\n",
      "   2. ធេង\n",
      "   3. អ្នកវង្ស\n",
      "   4. ការផលលំបាក\n",
      "   5. សារី\n",
      "   6. អ្នកស្រី\n",
      "   7. សណ្ឋាគារ\n",
      "   8. ជាពិសេស\n",
      "   9. ដំណើរទស្សនកិច្ច\n",
      "  10. ទា\n",
      "  11. ប្រដាប់ប្រដា\n",
      "  12. កងកម្លាំង\n",
      "  13. កប្បនេះ\n",
      "  14. មហាវិទ្យាល័យ\n",
      "  15. អ្វី\n",
      "  16. ប្រ\n",
      "  17. សង្ខត\n",
      "  18. សៅ\n",
      "  19. តែម្តង\n",
      "  20. យ៉ាងហោចណាស់\n",
      "\n",
      "🔬 Character-Level Analysis (first 5 non-Khmer tokens):\n",
      "\n",
      "======================================================================\n",
      "DIAGNOSING: no_all_stopwords.txt\n",
      "======================================================================\n",
      "\n",
      "📄 Sample Documents (first 5):\n",
      "  1. សេចក្តីថ្លែងការណ៍ ពាក់ព័ន្ធ ព័ត៌មាន ឲ្យដឹង ថា ទៅ បំពេញ ទស្សនកិច្ច នៅ ប្រទេស ឥណ្ឌា ការអញ្ជើញ នាយក រដ្ឋមន្ត្រី ឥណ្ឌា ដោយ ចាត់ទុក ការធ្វើដំណើរ ថា សារៈសំខាន់ ដោយ ផ្តល់ ឲ្យ កិច្ចពិភាក្សា ជ្រុងជ្រោយ របៀបវារ...\n",
      "  2. ផ្តន្ទាទោស ដាក់ ពន្ធនាការ ដោយ ព្យួរ ទោស រឺ ព្យួរ ទោស រយ ឆ្នាំ កន្លងទៅ តុលាការ សំរេច ដាក់ទោស ត្រូវ\n",
      "  3. នៅ ឆ្នាំ ភាគ កណ្ដាល ចក្រភព បែកបាក់ បង្កើត បានជា រាជាណាចក្រ តូច ឯករាជ្យ ជាច្រើន\n",
      "  4. រួម បួរ ហ្គូញ បួរ ហ្គូញ ត្រង់ ហ្ស៊ូរ៉ាន អ៊ីតាលី\n",
      "  5. តំបន់ ឡូរ៉ែ ន ទៅ ភាគ ខាងកើត ក្លាយជា រាជាណាចក្រ ចំណុះ\n",
      "\n",
      "📊 Token Composition Analysis (21,334 tokens analyzed):\n",
      "  • Pure Khmer tokens:    21,334 (100.0%)\n",
      "  • Latin/Alphanumeric:        0 (  0.0%)\n",
      "  • Numeric:                   0 (  0.0%)\n",
      "  • Mixed script:              0 (  0.0%)\n",
      "\n",
      "📏 Token Length Analysis:\n",
      "  • All tokens - Avg: 5.4, Min: 1, Max: 20\n",
      "  • Khmer tokens - Avg: 5.4, Min: 1, Max: 20\n",
      "\n",
      "⚠️  Suspicious Non-Khmer Tokens (sample of 20):\n",
      "\n",
      "✓ Sample Khmer Tokens (first 20 unique):\n",
      "   1. ទ្រទ្រង់\n",
      "   2. ធេង\n",
      "   3. អ្នកវង្ស\n",
      "   4. ការផលលំបាក\n",
      "   5. សារី\n",
      "   6. អ្នកស្រី\n",
      "   7. សណ្ឋាគារ\n",
      "   8. ជាពិសេស\n",
      "   9. ដំណើរទស្សនកិច្ច\n",
      "  10. ទា\n",
      "  11. ប្រដាប់ប្រដា\n",
      "  12. កងកម្លាំង\n",
      "  13. កប្បនេះ\n",
      "  14. មហាវិទ្យាល័យ\n",
      "  15. ប្រ\n",
      "  16. សង្ខត\n",
      "  17. សៅ\n",
      "  18. តែម្តង\n",
      "  19. យ៉ាងហោចណាស់\n",
      "  20. អូន\n",
      "\n",
      "🔬 Character-Level Analysis (first 5 non-Khmer tokens):\n",
      "\n",
      "======================================================================\n",
      "DIAGNOSING: no_Auxiliary_Verbs___Aspect_Markers.txt\n",
      "======================================================================\n",
      "\n",
      "📄 Sample Documents (first 5):\n",
      "  1. សេចក្តីថ្លែងការណ៍ ពាក់ព័ន្ធ ព័ត៌មាន នេះ ឲ្យដឹង ថា លោក ទៅ បំពេញ ទស្សនកិច្ច នៅ ប្រទេស ឥណ្ឌា តាម ការអញ្ជើញ របស់ នាយក រដ្ឋមន្ត្រី ឥណ្ឌា លោក ដោយ ចាត់ទុក ការធ្វើដំណើរ នេះ ថា មាន សារៈសំខាន់ ដោយ ផ្តល់ ឱកាស ឲ្...\n",
      "  2. ផ្តន្ទាទោស ដាក់ ពន្ធនាការ ដោយ ព្យួរ ទោស រឺ មិន ព្យួរ ទោស ក្នុង រយ ពេល ឆ្នាំ កន្លងទៅ ទេ នោះ តុលាការ សំរេច មិន ដាក់ទោស ប៉ុន្តែ ត្រូវ\n",
      "  3. នៅ ឆ្នាំ ភាគ កណ្ដាល នៃ ចក្រភព បែកបាក់ បង្កើត បានជា រាជាណាចក្រ តូច ឯករាជ្យ ជាច្រើន\n",
      "  4. ក្នុង នោះ រួម មាន បួរ ហ្គូញ លើ បួរ ហ្គូញ ត្រង់ ហ្ស៊ូរ៉ាន និង អ៊ីតាលី\n",
      "  5. ចំណែកឯ តំបន់ ឡូរ៉ែ ន បញ្ចូល ទៅ ភាគ ខាងកើត ក្លាយជា រាជាណាចក្រ ចំណុះ\n",
      "\n",
      "📊 Token Composition Analysis (28,880 tokens analyzed):\n",
      "  • Pure Khmer tokens:    28,880 (100.0%)\n",
      "  • Latin/Alphanumeric:        0 (  0.0%)\n",
      "  • Numeric:                   0 (  0.0%)\n",
      "  • Mixed script:              0 (  0.0%)\n",
      "\n",
      "📏 Token Length Analysis:\n",
      "  • All tokens - Avg: 4.9, Min: 1, Max: 20\n",
      "  • Khmer tokens - Avg: 4.9, Min: 1, Max: 20\n",
      "\n",
      "⚠️  Suspicious Non-Khmer Tokens (sample of 20):\n",
      "\n",
      "✓ Sample Khmer Tokens (first 20 unique):\n",
      "   1. ទ្រទ្រង់\n",
      "   2. ធេង\n",
      "   3. អ្នកវង្ស\n",
      "   4. ការផលលំបាក\n",
      "   5. សារី\n",
      "   6. អ្នកស្រី\n",
      "   7. សណ្ឋាគារ\n",
      "   8. ជាពិសេស\n",
      "   9. ដំណើរទស្សនកិច្ច\n",
      "  10. ទា\n",
      "  11. ប្រដាប់ប្រដា\n",
      "  12. កងកម្លាំង\n",
      "  13. កប្បនេះ\n",
      "  14. មហាវិទ្យាល័យ\n",
      "  15. អ្វី\n",
      "  16. ប្រ\n",
      "  17. សង្ខត\n",
      "  18. សៅ\n",
      "  19. តែម្តង\n",
      "  20. យ៉ាងហោចណាស់\n",
      "\n",
      "🔬 Character-Level Analysis (first 5 non-Khmer tokens):\n",
      "\n",
      "\n",
      "======================================================================\n",
      "COMPARATIVE SUMMARY\n",
      "======================================================================\n",
      "\n",
      "File                                          | Khmer %    | Status\n",
      "----------------------------------------------+------------+---------------------\n",
      "original_segmented_sentences.txt              |  100.0%   | ✓ Good\n",
      "no_all_stopwords.txt                          |  100.0%   | ✓ Good\n",
      "no_Auxiliary_Verbs___Aspect_Markers.txt       |  100.0%   | ✓ Good\n",
      "\n",
      "\n",
      "======================================================================\n",
      "DIAGNOSTIC COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Next Steps:\n",
      "1. Check if 'original_segmented_sentences.txt' has proper Khmer content\n",
      "2. If original is corrupted, review your text extraction/segmentation code\n",
      "3. If original is fine but filtered versions are not, review stopword removal\n",
      "4. Look for encoding issues in file reading/writing\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "# Diagnostic function to analyze text composition\n",
    "def diagnose_text_issues(filepath, sample_size=100):\n",
    "    \"\"\"\n",
    "    Diagnose what's actually in your processed text files\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"DIAGNOSING: {os.path.basename(filepath)}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    if not lines:\n",
    "        print(\"⚠️  Empty file!\")\n",
    "        return\n",
    "    \n",
    "    # Sample analysis\n",
    "    print(f\"\\n📄 Sample Documents (first 5):\")\n",
    "    for i, line in enumerate(lines[:5], 1):\n",
    "        print(f\"  {i}. {line[:200]}{'...' if len(line) > 200 else ''}\")\n",
    "    \n",
    "    # Collect all tokens\n",
    "    all_tokens = []\n",
    "    for line in lines[:sample_size]:\n",
    "        all_tokens.extend(line.split())\n",
    "    \n",
    "    if not all_tokens:\n",
    "        print(\"⚠️  No tokens found!\")\n",
    "        return\n",
    "    \n",
    "    # Character set analysis\n",
    "    khmer_pattern = re.compile(r'[\\u1780-\\u17FF]')  # Khmer Unicode range\n",
    "    latin_pattern = re.compile(r'[a-zA-Z]')\n",
    "    digit_pattern = re.compile(r'[0-9]')\n",
    "    \n",
    "    khmer_tokens = [t for t in all_tokens if khmer_pattern.search(t)]\n",
    "    latin_tokens = [t for t in all_tokens if latin_pattern.search(t) and not khmer_pattern.search(t)]\n",
    "    digit_tokens = [t for t in all_tokens if digit_pattern.search(t) and not khmer_pattern.search(t)]\n",
    "    mixed_tokens = [t for t in all_tokens if khmer_pattern.search(t) and (latin_pattern.search(t) or digit_pattern.search(t))]\n",
    "    \n",
    "    total = len(all_tokens)\n",
    "    \n",
    "    print(f\"\\n📊 Token Composition Analysis ({total:,} tokens analyzed):\")\n",
    "    print(f\"  • Pure Khmer tokens:    {len(khmer_tokens):6,} ({len(khmer_tokens)/total*100:5.1f}%)\")\n",
    "    print(f\"  • Latin/Alphanumeric:   {len(latin_tokens):6,} ({len(latin_tokens)/total*100:5.1f}%)\")\n",
    "    print(f\"  • Numeric:              {len(digit_tokens):6,} ({len(digit_tokens)/total*100:5.1f}%)\")\n",
    "    print(f\"  • Mixed script:         {len(mixed_tokens):6,} ({len(mixed_tokens)/total*100:5.1f}%)\")\n",
    "    \n",
    "    # Token length analysis\n",
    "    token_lengths = [len(t) for t in all_tokens]\n",
    "    khmer_token_lengths = [len(t) for t in khmer_tokens]\n",
    "    \n",
    "    print(f\"\\n📏 Token Length Analysis:\")\n",
    "    print(f\"  • All tokens - Avg: {sum(token_lengths)/len(token_lengths):.1f}, Min: {min(token_lengths)}, Max: {max(token_lengths)}\")\n",
    "    if khmer_token_lengths:\n",
    "        print(f\"  • Khmer tokens - Avg: {sum(khmer_token_lengths)/len(khmer_token_lengths):.1f}, Min: {min(khmer_token_lengths)}, Max: {max(khmer_token_lengths)}\")\n",
    "    \n",
    "    # Show suspicious tokens (too short, non-Khmer)\n",
    "    print(f\"\\n⚠️  Suspicious Non-Khmer Tokens (sample of 20):\")\n",
    "    suspicious = [t for t in all_tokens if not khmer_pattern.search(t)]\n",
    "    for token in Counter(suspicious).most_common(20):\n",
    "        print(f\"  • '{token[0]}' (appears {token[1]} times)\")\n",
    "    \n",
    "    # Show sample Khmer tokens\n",
    "    print(f\"\\n✓ Sample Khmer Tokens (first 20 unique):\")\n",
    "    khmer_unique = list(set(khmer_tokens))[:20]\n",
    "    for i, token in enumerate(khmer_unique, 1):\n",
    "        print(f\"  {i:2d}. {token}\")\n",
    "    \n",
    "    # Character-level breakdown of a few tokens\n",
    "    print(f\"\\n🔬 Character-Level Analysis (first 5 non-Khmer tokens):\")\n",
    "    for token in suspicious[:5]:\n",
    "        chars = [f\"'{c}' (U+{ord(c):04X})\" for c in token[:10]]\n",
    "        print(f\"  '{token}' → {', '.join(chars)}\")\n",
    "    \n",
    "    return {\n",
    "        'total_tokens': total,\n",
    "        'khmer_tokens': len(khmer_tokens),\n",
    "        'latin_tokens': len(latin_tokens),\n",
    "        'digit_tokens': len(digit_tokens),\n",
    "        'suspicious_tokens': len(suspicious),\n",
    "        'khmer_percentage': len(khmer_tokens)/total*100 if total > 0 else 0\n",
    "    }\n",
    "\n",
    "\n",
    "# Run diagnostics on your files\n",
    "processed_dir = r'D:\\Year 5\\S1\\Information Retrieval\\StopwordProject\\khmer_stopword_project\\data\\processed'\n",
    "\n",
    "files_to_check = [\n",
    "    'original_segmented_sentences.txt',\n",
    "    'no_all_stopwords.txt',\n",
    "    'no_Auxiliary_Verbs___Aspect_Markers.txt'\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for filename in files_to_check:\n",
    "    filepath = os.path.join(processed_dir, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        results[filename] = diagnose_text_issues(filepath, sample_size=1000)\n",
    "    else:\n",
    "        print(f\"\\n⚠️  File not found: {filepath}\")\n",
    "\n",
    "# Summary comparison\n",
    "if results:\n",
    "    print(f\"\\n\\n{'='*70}\")\n",
    "    print(\"COMPARATIVE SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\n{'File':<45} | {'Khmer %':<10} | {'Status'}\")\n",
    "    print(f\"{'-'*45}-+-{'-'*10}-+-{'-'*20}\")\n",
    "    \n",
    "    for filename, data in results.items():\n",
    "        status = \"✓ Good\" if data['khmer_percentage'] > 80 else \"⚠️  Problem!\"\n",
    "        print(f\"{filename:<45} | {data['khmer_percentage']:>6.1f}%   | {status}\")\n",
    "\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(\"DIAGNOSTIC COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Check if 'original_segmented_sentences.txt' has proper Khmer content\")\n",
    "print(\"2. If original is corrupted, review your text extraction/segmentation code\")\n",
    "print(\"3. If original is fine but filtered versions are not, review stopword removal\")\n",
    "print(\"4. Look for encoding issues in file reading/writing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b720cf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      " KHMER TEXT DIAGNOSTIC TOOL\n",
      " Comprehensive Analysis of Processed Corpora\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ANALYZING: Original\n",
      "File: original_segmented_sentences.txt\n",
      "================================================================================\n",
      "✓ Total documents (lines): 58,409\n",
      "\n",
      "📄 SAMPLE DOCUMENTS (first 3):\n",
      "  1. សេចក្តីថ្លែងការណ៍ ពាក់ព័ន្ធ នឹង ព័ត៌មាន នេះ   បាន ឲ្យដឹង ថា   លោក   នឹង ទៅ បំពេញ ទស្សនកិច្ច នៅ ប្រទេស ឥណ្ឌា   តាម ការអញ្ជើញ របស់ នាយក រដ្ឋមន្ត្រី ឥណ្ឌ...\n",
      "  2. ផ្តន្ទាទោស ដាក់ ពន្ធនាការ ដោយ ព្យួរ ទោស   រឺ មិន ព្យួរ ទោស ក្នុង រយ   ពេល ឆ្នាំ កន្លងទៅ ទេ   នោះ តុលាការ អាច សំរេច មិន ដាក់ទោស   ប៉ុន្តែ ត្រូវ\n",
      "  3. នៅ ឆ្នាំ   ភាគ កណ្ដាល នៃ ចក្រភព បាន បែកបាក់   បង្កើត បានជា រាជាណាចក្រ តូច ឯករាជ្យ ជាច្រើន\n",
      "\n",
      "✓ Analyzing 30,036 tokens from first 1000 documents\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📊 TOKEN COMPOSITION ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Category                       |      Count |   Percentage\n",
      "-------------------------------+------------+-------------\n",
      "Pure Khmer script              |     30,036 |      100.0%\n",
      "Latin/Alphanumeric only        |          0 |        0.0%\n",
      "Numeric only                   |          0 |        0.0%\n",
      "Mixed script                   |          0 |        0.0%\n",
      "Other                          |          0 |        0.0%\n",
      "-------------------------------+------------+-------------\n",
      "TOTAL                          |     30,036 |      100.0%\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📏 TOKEN LENGTH ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Khmer tokens:  Avg=4.9, Min=1, Max=20, Unique=5,381\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "⚠️  TOP 20 NON-KHMER TOKENS (Suspicious)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  ✓ No non-Khmer tokens found!\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ SAMPLE KHMER TOKENS (first 30 unique)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  ទ្រទ្រង់     | ធេង          | អ្នកវង្ស     | ការផលលំបាក   | សារី        \n",
      "  អ្នកស្រី     | សណ្ឋាគារ     | ជាពិសេស      | ដំណើរទស្សនកិច្ច | ទា          \n",
      "  ប្រដាប់ប្រដា | កងកម្លាំង    | កប្បនេះ      | មហាវិទ្យាល័យ | អ្វី        \n",
      "  ប្រ          | សង្ខត        | សៅ           | តែម្តង       | យ៉ាងហោចណាស់ \n",
      "  អូន          | អធីបតី       | យ៉ាងតិច      | ការ          | នឹម         \n",
      "  ក្រឹត្យ      | សិុន         | ចង្វាត់      | ស៊ូឡាំង      | រត្ន        \n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🔬 CHARACTER-LEVEL BREAKDOWN (first 5 non-Khmer tokens)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🎯 QUALITY ASSESSMENT\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Status: ✓ GOOD\n",
      "\n",
      "================================================================================\n",
      "ANALYZING: No_All_Stopwords\n",
      "File: no_all_stopwords.txt\n",
      "================================================================================\n",
      "✓ Total documents (lines): 58,409\n",
      "\n",
      "📄 SAMPLE DOCUMENTS (first 3):\n",
      "  1. សេចក្តីថ្លែងការណ៍ ពាក់ព័ន្ធ ព័ត៌មាន ឲ្យដឹង ថា ទៅ បំពេញ ទស្សនកិច្ច នៅ ប្រទេស ឥណ្ឌា ការអញ្ជើញ នាយក រដ្ឋមន្ត្រី ឥណ្ឌា ដោយ ចាត់ទុក ការធ្វើដំណើរ ថា សារៈសំខ...\n",
      "  2. ផ្តន្ទាទោស ដាក់ ពន្ធនាការ ដោយ ព្យួរ ទោស រឺ ព្យួរ ទោស រយ ឆ្នាំ កន្លងទៅ តុលាការ សំរេច ដាក់ទោស ត្រូវ\n",
      "  3. នៅ ឆ្នាំ ភាគ កណ្ដាល ចក្រភព បែកបាក់ បង្កើត បានជា រាជាណាចក្រ តូច ឯករាជ្យ ជាច្រើន\n",
      "\n",
      "✓ Analyzing 21,334 tokens from first 1000 documents\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📊 TOKEN COMPOSITION ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Category                       |      Count |   Percentage\n",
      "-------------------------------+------------+-------------\n",
      "Pure Khmer script              |     21,334 |      100.0%\n",
      "Latin/Alphanumeric only        |          0 |        0.0%\n",
      "Numeric only                   |          0 |        0.0%\n",
      "Mixed script                   |          0 |        0.0%\n",
      "Other                          |          0 |        0.0%\n",
      "-------------------------------+------------+-------------\n",
      "TOTAL                          |     21,334 |      100.0%\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📏 TOKEN LENGTH ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Khmer tokens:  Avg=5.4, Min=1, Max=20, Unique=5,202\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "⚠️  TOP 20 NON-KHMER TOKENS (Suspicious)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  ✓ No non-Khmer tokens found!\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ SAMPLE KHMER TOKENS (first 30 unique)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  ទ្រទ្រង់     | ធេង          | អ្នកវង្ស     | ការផលលំបាក   | សារី        \n",
      "  អ្នកស្រី     | សណ្ឋាគារ     | ជាពិសេស      | ដំណើរទស្សនកិច្ច | ទា          \n",
      "  ប្រដាប់ប្រដា | កងកម្លាំង    | កប្បនេះ      | មហាវិទ្យាល័យ | ប្រ         \n",
      "  សង្ខត        | សៅ           | តែម្តង       | យ៉ាងហោចណាស់  | អូន         \n",
      "  អធីបតី       | យ៉ាងតិច      | ការ          | នឹម          | ក្រឹត្យ     \n",
      "  សិុន         | ចង្វាត់      | ស៊ូឡាំង      | រត្ន         | សហរដ្ឋ      \n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🔬 CHARACTER-LEVEL BREAKDOWN (first 5 non-Khmer tokens)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🎯 QUALITY ASSESSMENT\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Status: ✓ GOOD\n",
      "\n",
      "================================================================================\n",
      "ANALYZING: No_Auxiliary_Verbs\n",
      "File: no_Auxiliary_Verbs___Aspect_Markers.txt\n",
      "================================================================================\n",
      "✓ Total documents (lines): 58,409\n",
      "\n",
      "📄 SAMPLE DOCUMENTS (first 3):\n",
      "  1. សេចក្តីថ្លែងការណ៍ ពាក់ព័ន្ធ ព័ត៌មាន នេះ ឲ្យដឹង ថា លោក ទៅ បំពេញ ទស្សនកិច្ច នៅ ប្រទេស ឥណ្ឌា តាម ការអញ្ជើញ របស់ នាយក រដ្ឋមន្ត្រី ឥណ្ឌា លោក ដោយ ចាត់ទុក កា...\n",
      "  2. ផ្តន្ទាទោស ដាក់ ពន្ធនាការ ដោយ ព្យួរ ទោស រឺ មិន ព្យួរ ទោស ក្នុង រយ ពេល ឆ្នាំ កន្លងទៅ ទេ នោះ តុលាការ សំរេច មិន ដាក់ទោស ប៉ុន្តែ ត្រូវ\n",
      "  3. នៅ ឆ្នាំ ភាគ កណ្ដាល នៃ ចក្រភព បែកបាក់ បង្កើត បានជា រាជាណាចក្រ តូច ឯករាជ្យ ជាច្រើន\n",
      "\n",
      "✓ Analyzing 28,880 tokens from first 1000 documents\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📊 TOKEN COMPOSITION ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Category                       |      Count |   Percentage\n",
      "-------------------------------+------------+-------------\n",
      "Pure Khmer script              |     28,880 |      100.0%\n",
      "Latin/Alphanumeric only        |          0 |        0.0%\n",
      "Numeric only                   |          0 |        0.0%\n",
      "Mixed script                   |          0 |        0.0%\n",
      "Other                          |          0 |        0.0%\n",
      "-------------------------------+------------+-------------\n",
      "TOTAL                          |     28,880 |      100.0%\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📏 TOKEN LENGTH ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Khmer tokens:  Avg=4.9, Min=1, Max=20, Unique=5,366\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "⚠️  TOP 20 NON-KHMER TOKENS (Suspicious)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  ✓ No non-Khmer tokens found!\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ SAMPLE KHMER TOKENS (first 30 unique)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  ទ្រទ្រង់     | ធេង          | អ្នកវង្ស     | ការផលលំបាក   | សារី        \n",
      "  អ្នកស្រី     | សណ្ឋាគារ     | ជាពិសេស      | ដំណើរទស្សនកិច្ច | ទា          \n",
      "  ប្រដាប់ប្រដា | កងកម្លាំង    | កប្បនេះ      | មហាវិទ្យាល័យ | អ្វី        \n",
      "  ប្រ          | សង្ខត        | សៅ           | តែម្តង       | យ៉ាងហោចណាស់ \n",
      "  អូន          | អធីបតី       | យ៉ាងតិច      | ការ          | នឹម         \n",
      "  ក្រឹត្យ      | សិុន         | ចង្វាត់      | ស៊ូឡាំង      | រត្ន        \n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🔬 CHARACTER-LEVEL BREAKDOWN (first 5 non-Khmer tokens)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🎯 QUALITY ASSESSMENT\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Status: ✓ GOOD\n",
      "\n",
      "================================================================================\n",
      "ANALYZING: No_Conjunctions\n",
      "File: no_Conjunctions.txt\n",
      "================================================================================\n",
      "✓ Total documents (lines): 58,409\n",
      "\n",
      "📄 SAMPLE DOCUMENTS (first 3):\n",
      "  1. សេចក្តីថ្លែងការណ៍ ពាក់ព័ន្ធ នឹង ព័ត៌មាន នេះ បាន ឲ្យដឹង ថា លោក នឹង ទៅ បំពេញ ទស្សនកិច្ច នៅ ប្រទេស ឥណ្ឌា តាម ការអញ្ជើញ របស់ នាយក រដ្ឋមន្ត្រី ឥណ្ឌា លោក ដោ...\n",
      "  2. ផ្តន្ទាទោស ដាក់ ពន្ធនាការ ដោយ ព្យួរ ទោស រឺ មិន ព្យួរ ទោស ក្នុង រយ ពេល ឆ្នាំ កន្លងទៅ ទេ នោះ តុលាការ អាច សំរេច មិន ដាក់ទោស ត្រូវ\n",
      "  3. នៅ ឆ្នាំ ភាគ កណ្ដាល នៃ ចក្រភព បាន បែកបាក់ បង្កើត បានជា រាជាណាចក្រ តូច ឯករាជ្យ ជាច្រើន\n",
      "\n",
      "✓ Analyzing 28,241 tokens from first 1000 documents\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📊 TOKEN COMPOSITION ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Category                       |      Count |   Percentage\n",
      "-------------------------------+------------+-------------\n",
      "Pure Khmer script              |     28,241 |      100.0%\n",
      "Latin/Alphanumeric only        |          0 |        0.0%\n",
      "Numeric only                   |          0 |        0.0%\n",
      "Mixed script                   |          0 |        0.0%\n",
      "Other                          |          0 |        0.0%\n",
      "-------------------------------+------------+-------------\n",
      "TOTAL                          |     28,241 |      100.0%\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📏 TOKEN LENGTH ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Khmer tokens:  Avg=5.0, Min=1, Max=20, Unique=5,346\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "⚠️  TOP 20 NON-KHMER TOKENS (Suspicious)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  ✓ No non-Khmer tokens found!\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ SAMPLE KHMER TOKENS (first 30 unique)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  ទ្រទ្រង់     | ធេង          | អ្នកវង្ស     | ការផលលំបាក   | សារី        \n",
      "  អ្នកស្រី     | សណ្ឋាគារ     | ជាពិសេស      | ដំណើរទស្សនកិច្ច | ទា          \n",
      "  ប្រដាប់ប្រដា | កងកម្លាំង    | កប្បនេះ      | មហាវិទ្យាល័យ | អ្វី        \n",
      "  ប្រ          | សង្ខត        | សៅ           | តែម្តង       | យ៉ាងហោចណាស់ \n",
      "  អូន          | អធីបតី       | យ៉ាងតិច      | ការ          | នឹម         \n",
      "  ក្រឹត្យ      | សិុន         | ចង្វាត់      | ស៊ូឡាំង      | រត្ន        \n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🔬 CHARACTER-LEVEL BREAKDOWN (first 5 non-Khmer tokens)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🎯 QUALITY ASSESSMENT\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Status: ✓ GOOD\n",
      "\n",
      "================================================================================\n",
      "ANALYZING: No_Determiners\n",
      "File: no_Determiners_and_Quantifiers.txt\n",
      "================================================================================\n",
      "✓ Total documents (lines): 58,409\n",
      "\n",
      "📄 SAMPLE DOCUMENTS (first 3):\n",
      "  1. សេចក្តីថ្លែងការណ៍ ពាក់ព័ន្ធ នឹង ព័ត៌មាន បាន ឲ្យដឹង ថា លោក នឹង ទៅ បំពេញ ទស្សនកិច្ច នៅ ប្រទេស ឥណ្ឌា តាម ការអញ្ជើញ របស់ នាយក រដ្ឋមន្ត្រី ឥណ្ឌា លោក ដោយ ចា...\n",
      "  2. ផ្តន្ទាទោស ដាក់ ពន្ធនាការ ដោយ ព្យួរ ទោស រឺ មិន ព្យួរ ទោស ក្នុង រយ ពេល ឆ្នាំ កន្លងទៅ ទេ តុលាការ អាច សំរេច មិន ដាក់ទោស ប៉ុន្តែ ត្រូវ\n",
      "  3. នៅ ឆ្នាំ ភាគ កណ្ដាល នៃ ចក្រភព បាន បែកបាក់ បង្កើត បានជា រាជាណាចក្រ តូច ឯករាជ្យ ជាច្រើន\n",
      "\n",
      "✓ Analyzing 29,265 tokens from first 1000 documents\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📊 TOKEN COMPOSITION ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Category                       |      Count |   Percentage\n",
      "-------------------------------+------------+-------------\n",
      "Pure Khmer script              |     29,265 |      100.0%\n",
      "Latin/Alphanumeric only        |          0 |        0.0%\n",
      "Numeric only                   |          0 |        0.0%\n",
      "Mixed script                   |          0 |        0.0%\n",
      "Other                          |          0 |        0.0%\n",
      "-------------------------------+------------+-------------\n",
      "TOTAL                          |     29,265 |      100.0%\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📏 TOKEN LENGTH ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Khmer tokens:  Avg=4.9, Min=1, Max=20, Unique=5,366\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "⚠️  TOP 20 NON-KHMER TOKENS (Suspicious)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  ✓ No non-Khmer tokens found!\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ SAMPLE KHMER TOKENS (first 30 unique)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  ទ្រទ្រង់     | ធេង          | អ្នកវង្ស     | ការផលលំបាក   | សារី        \n",
      "  អ្នកស្រី     | សណ្ឋាគារ     | ជាពិសេស      | ដំណើរទស្សនកិច្ច | ទា          \n",
      "  ប្រដាប់ប្រដា | កងកម្លាំង    | កប្បនេះ      | មហាវិទ្យាល័យ | អ្វី        \n",
      "  ប្រ          | សង្ខត        | សៅ           | តែម្តង       | យ៉ាងហោចណាស់ \n",
      "  អូន          | អធីបតី       | យ៉ាងតិច      | ការ          | នឹម         \n",
      "  ក្រឹត្យ      | សិុន         | ចង្វាត់      | ស៊ូឡាំង      | រត្ន        \n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🔬 CHARACTER-LEVEL BREAKDOWN (first 5 non-Khmer tokens)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🎯 QUALITY ASSESSMENT\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Status: ✓ GOOD\n",
      "\n",
      "================================================================================\n",
      "ANALYZING: No_Function_Nouns\n",
      "File: no_Function_Nouns.txt\n",
      "================================================================================\n",
      "✓ Total documents (lines): 58,409\n",
      "\n",
      "📄 SAMPLE DOCUMENTS (first 3):\n",
      "  1. សេចក្តីថ្លែងការណ៍ ពាក់ព័ន្ធ នឹង ព័ត៌មាន នេះ បាន ឲ្យដឹង ថា លោក នឹង ទៅ បំពេញ ទស្សនកិច្ច នៅ ប្រទេស ឥណ្ឌា តាម ការអញ្ជើញ របស់ នាយក រដ្ឋមន្ត្រី ឥណ្ឌា លោក ដោ...\n",
      "  2. ផ្តន្ទាទោស ដាក់ ពន្ធនាការ ដោយ ព្យួរ ទោស រឺ មិន ព្យួរ ទោស ក្នុង រយ ពេល ឆ្នាំ កន្លងទៅ ទេ នោះ តុលាការ អាច សំរេច មិន ដាក់ទោស ប៉ុន្តែ ត្រូវ\n",
      "  3. នៅ ឆ្នាំ ភាគ កណ្ដាល នៃ ចក្រភព បាន បែកបាក់ បង្កើត បានជា រាជាណាចក្រ តូច ឯករាជ្យ ជាច្រើន\n",
      "\n",
      "✓ Analyzing 29,154 tokens from first 1000 documents\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📊 TOKEN COMPOSITION ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Category                       |      Count |   Percentage\n",
      "-------------------------------+------------+-------------\n",
      "Pure Khmer script              |     29,154 |      100.0%\n",
      "Latin/Alphanumeric only        |          0 |        0.0%\n",
      "Numeric only                   |          0 |        0.0%\n",
      "Mixed script                   |          0 |        0.0%\n",
      "Other                          |          0 |        0.0%\n",
      "-------------------------------+------------+-------------\n",
      "TOTAL                          |     29,154 |      100.0%\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📏 TOKEN LENGTH ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Khmer tokens:  Avg=4.9, Min=1, Max=20, Unique=5,349\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "⚠️  TOP 20 NON-KHMER TOKENS (Suspicious)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  ✓ No non-Khmer tokens found!\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ SAMPLE KHMER TOKENS (first 30 unique)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  ទ្រទ្រង់     | ធេង          | អ្នកវង្ស     | ការផលលំបាក   | សារី        \n",
      "  អ្នកស្រី     | សណ្ឋាគារ     | ជាពិសេស      | ដំណើរទស្សនកិច្ច | ទា          \n",
      "  ប្រដាប់ប្រដា | កងកម្លាំង    | កប្បនេះ      | មហាវិទ្យាល័យ | ប្រ         \n",
      "  សង្ខត        | សៅ           | តែម្តង       | យ៉ាងហោចណាស់  | អូន         \n",
      "  អធីបតី       | យ៉ាងតិច      | ការ          | នឹម          | ក្រឹត្យ     \n",
      "  សិុន         | ចង្វាត់      | ស៊ូឡាំង      | រត្ន         | សហរដ្ឋ      \n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🔬 CHARACTER-LEVEL BREAKDOWN (first 5 non-Khmer tokens)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🎯 QUALITY ASSESSMENT\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Status: ✓ GOOD\n",
      "\n",
      "================================================================================\n",
      "ANALYZING: No_Numbers\n",
      "File: no_Numbers_and_Time_Expressions.txt\n",
      "================================================================================\n",
      "✓ Total documents (lines): 58,409\n",
      "\n",
      "📄 SAMPLE DOCUMENTS (first 3):\n",
      "  1. សេចក្តីថ្លែងការណ៍ ពាក់ព័ន្ធ នឹង ព័ត៌មាន នេះ បាន ឲ្យដឹង ថា លោក នឹង ទៅ បំពេញ ទស្សនកិច្ច នៅ ប្រទេស ឥណ្ឌា តាម ការអញ្ជើញ របស់ នាយក រដ្ឋមន្ត្រី ឥណ្ឌា លោក ដោ...\n",
      "  2. ផ្តន្ទាទោស ដាក់ ពន្ធនាការ ដោយ ព្យួរ ទោស រឺ មិន ព្យួរ ទោស ក្នុង រយ ឆ្នាំ កន្លងទៅ ទេ នោះ តុលាការ អាច សំរេច មិន ដាក់ទោស ប៉ុន្តែ ត្រូវ\n",
      "  3. នៅ ឆ្នាំ ភាគ កណ្ដាល នៃ ចក្រភព បាន បែកបាក់ បង្កើត បានជា រាជាណាចក្រ តូច ឯករាជ្យ ជាច្រើន\n",
      "\n",
      "✓ Analyzing 29,740 tokens from first 1000 documents\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📊 TOKEN COMPOSITION ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Category                       |      Count |   Percentage\n",
      "-------------------------------+------------+-------------\n",
      "Pure Khmer script              |     29,740 |      100.0%\n",
      "Latin/Alphanumeric only        |          0 |        0.0%\n",
      "Numeric only                   |          0 |        0.0%\n",
      "Mixed script                   |          0 |        0.0%\n",
      "Other                          |          0 |        0.0%\n",
      "-------------------------------+------------+-------------\n",
      "TOTAL                          |     29,740 |      100.0%\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📏 TOKEN LENGTH ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Khmer tokens:  Avg=4.9, Min=1, Max=20, Unique=5,371\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "⚠️  TOP 20 NON-KHMER TOKENS (Suspicious)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  ✓ No non-Khmer tokens found!\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ SAMPLE KHMER TOKENS (first 30 unique)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  ទ្រទ្រង់     | ធេង          | អ្នកវង្ស     | ការផលលំបាក   | សារី        \n",
      "  អ្នកស្រី     | សណ្ឋាគារ     | ជាពិសេស      | ដំណើរទស្សនកិច្ច | ទា          \n",
      "  ប្រដាប់ប្រដា | កងកម្លាំង    | កប្បនេះ      | មហាវិទ្យាល័យ | អ្វី        \n",
      "  ប្រ          | សង្ខត        | សៅ           | តែម្តង       | យ៉ាងហោចណាស់ \n",
      "  អូន          | អធីបតី       | យ៉ាងតិច      | ការ          | នឹម         \n",
      "  ក្រឹត្យ      | សិុន         | ចង្វាត់      | ស៊ូឡាំង      | រត្ន        \n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🔬 CHARACTER-LEVEL BREAKDOWN (first 5 non-Khmer tokens)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🎯 QUALITY ASSESSMENT\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Status: ✓ GOOD\n",
      "\n",
      "================================================================================\n",
      "ANALYZING: No_Particles\n",
      "File: no_Particles_and_Discourse_Markers.txt\n",
      "================================================================================\n",
      "✓ Total documents (lines): 58,409\n",
      "\n",
      "📄 SAMPLE DOCUMENTS (first 3):\n",
      "  1. សេចក្តីថ្លែងការណ៍ ពាក់ព័ន្ធ នឹង ព័ត៌មាន នេះ បាន ឲ្យដឹង ថា លោក នឹង ទៅ បំពេញ ទស្សនកិច្ច នៅ ប្រទេស ឥណ្ឌា តាម ការអញ្ជើញ របស់ នាយក រដ្ឋមន្ត្រី ឥណ្ឌា លោក ដោ...\n",
      "  2. ផ្តន្ទាទោស ដាក់ ពន្ធនាការ ដោយ ព្យួរ ទោស រឺ មិន ព្យួរ ទោស ក្នុង រយ ពេល ឆ្នាំ កន្លងទៅ ទេ នោះ តុលាការ អាច សំរេច មិន ដាក់ទោស ប៉ុន្តែ ត្រូវ\n",
      "  3. នៅ ឆ្នាំ ភាគ កណ្ដាល នៃ ចក្រភព បាន បែកបាក់ បង្កើត បានជា រាជាណាចក្រ តូច ឯករាជ្យ ជាច្រើន\n",
      "\n",
      "✓ Analyzing 29,244 tokens from first 1000 documents\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📊 TOKEN COMPOSITION ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Category                       |      Count |   Percentage\n",
      "-------------------------------+------------+-------------\n",
      "Pure Khmer script              |     29,244 |      100.0%\n",
      "Latin/Alphanumeric only        |          0 |        0.0%\n",
      "Numeric only                   |          0 |        0.0%\n",
      "Mixed script                   |          0 |        0.0%\n",
      "Other                          |          0 |        0.0%\n",
      "-------------------------------+------------+-------------\n",
      "TOTAL                          |     29,244 |      100.0%\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📏 TOKEN LENGTH ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Khmer tokens:  Avg=4.9, Min=1, Max=20, Unique=5,358\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "⚠️  TOP 20 NON-KHMER TOKENS (Suspicious)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  ✓ No non-Khmer tokens found!\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ SAMPLE KHMER TOKENS (first 30 unique)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  ទ្រទ្រង់     | ធេង          | អ្នកវង្ស     | ការផលលំបាក   | សារី        \n",
      "  អ្នកស្រី     | សណ្ឋាគារ     | ជាពិសេស      | ដំណើរទស្សនកិច្ច | ទា          \n",
      "  ប្រដាប់ប្រដា | កងកម្លាំង    | កប្បនេះ      | មហាវិទ្យាល័យ | អ្វី        \n",
      "  ប្រ          | សង្ខត        | សៅ           | តែម្តង       | យ៉ាងហោចណាស់ \n",
      "  អូន          | អធីបតី       | យ៉ាងតិច      | ការ          | នឹម         \n",
      "  ក្រឹត្យ      | សិុន         | ចង្វាត់      | ស៊ូឡាំង      | រត្ន        \n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🔬 CHARACTER-LEVEL BREAKDOWN (first 5 non-Khmer tokens)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🎯 QUALITY ASSESSMENT\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Status: ✓ GOOD\n",
      "\n",
      "================================================================================\n",
      "ANALYZING: No_Politeness\n",
      "File: no_Politeness_and_Honorifics.txt\n",
      "================================================================================\n",
      "✓ Total documents (lines): 58,409\n",
      "\n",
      "📄 SAMPLE DOCUMENTS (first 3):\n",
      "  1. សេចក្តីថ្លែងការណ៍ ពាក់ព័ន្ធ នឹង ព័ត៌មាន នេះ បាន ឲ្យដឹង ថា នឹង ទៅ បំពេញ ទស្សនកិច្ច នៅ ប្រទេស ឥណ្ឌា តាម ការអញ្ជើញ របស់ នាយក រដ្ឋមន្ត្រី ឥណ្ឌា ដោយ ចាត់ទុ...\n",
      "  2. ផ្តន្ទាទោស ដាក់ ពន្ធនាការ ដោយ ព្យួរ ទោស រឺ មិន ព្យួរ ទោស ក្នុង រយ ពេល ឆ្នាំ កន្លងទៅ ទេ នោះ តុលាការ អាច សំរេច មិន ដាក់ទោស ប៉ុន្តែ ត្រូវ\n",
      "  3. នៅ ឆ្នាំ ភាគ កណ្ដាល នៃ ចក្រភព បាន បែកបាក់ បង្កើត បានជា រាជាណាចក្រ តូច ឯករាជ្យ ជាច្រើន\n",
      "\n",
      "✓ Analyzing 29,833 tokens from first 1000 documents\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📊 TOKEN COMPOSITION ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Category                       |      Count |   Percentage\n",
      "-------------------------------+------------+-------------\n",
      "Pure Khmer script              |     29,833 |      100.0%\n",
      "Latin/Alphanumeric only        |          0 |        0.0%\n",
      "Numeric only                   |          0 |        0.0%\n",
      "Mixed script                   |          0 |        0.0%\n",
      "Other                          |          0 |        0.0%\n",
      "-------------------------------+------------+-------------\n",
      "TOTAL                          |     29,833 |      100.0%\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📏 TOKEN LENGTH ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Khmer tokens:  Avg=4.9, Min=1, Max=20, Unique=5,377\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "⚠️  TOP 20 NON-KHMER TOKENS (Suspicious)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  ✓ No non-Khmer tokens found!\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ SAMPLE KHMER TOKENS (first 30 unique)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  ទ្រទ្រង់     | ធេង          | អ្នកវង្ស     | ការផលលំបាក   | សារី        \n",
      "  អ្នកស្រី     | សណ្ឋាគារ     | ជាពិសេស      | ដំណើរទស្សនកិច្ច | ទា          \n",
      "  ប្រដាប់ប្រដា | កងកម្លាំង    | កប្បនេះ      | មហាវិទ្យាល័យ | អ្វី        \n",
      "  ប្រ          | សង្ខត        | សៅ           | តែម្តង       | យ៉ាងហោចណាស់ \n",
      "  អូន          | អធីបតី       | យ៉ាងតិច      | ការ          | នឹម         \n",
      "  ក្រឹត្យ      | សិុន         | ចង្វាត់      | ស៊ូឡាំង      | រត្ន        \n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🔬 CHARACTER-LEVEL BREAKDOWN (first 5 non-Khmer tokens)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🎯 QUALITY ASSESSMENT\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Status: ✓ GOOD\n",
      "\n",
      "================================================================================\n",
      "ANALYZING: No_Prepositions\n",
      "File: no_Prepositions___Relational_Words.txt\n",
      "================================================================================\n",
      "✓ Total documents (lines): 58,409\n",
      "\n",
      "📄 SAMPLE DOCUMENTS (first 3):\n",
      "  1. សេចក្តីថ្លែងការណ៍ ពាក់ព័ន្ធ នឹង ព័ត៌មាន នេះ បាន ឲ្យដឹង ថា លោក នឹង ទៅ បំពេញ ទស្សនកិច្ច នៅ ប្រទេស ឥណ្ឌា ការអញ្ជើញ នាយក រដ្ឋមន្ត្រី ឥណ្ឌា លោក ដោយ ចាត់ទុក...\n",
      "  2. ផ្តន្ទាទោស ដាក់ ពន្ធនាការ ដោយ ព្យួរ ទោស រឺ មិន ព្យួរ ទោស រយ ពេល ឆ្នាំ កន្លងទៅ ទេ នោះ តុលាការ អាច សំរេច មិន ដាក់ទោស ប៉ុន្តែ ត្រូវ\n",
      "  3. នៅ ឆ្នាំ ភាគ កណ្ដាល ចក្រភព បាន បែកបាក់ បង្កើត បានជា រាជាណាចក្រ តូច ឯករាជ្យ ជាច្រើន\n",
      "\n",
      "✓ Analyzing 27,995 tokens from first 1000 documents\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📊 TOKEN COMPOSITION ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Category                       |      Count |   Percentage\n",
      "-------------------------------+------------+-------------\n",
      "Pure Khmer script              |     27,995 |      100.0%\n",
      "Latin/Alphanumeric only        |          0 |        0.0%\n",
      "Numeric only                   |          0 |        0.0%\n",
      "Mixed script                   |          0 |        0.0%\n",
      "Other                          |          0 |        0.0%\n",
      "-------------------------------+------------+-------------\n",
      "TOTAL                          |     27,995 |      100.0%\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📏 TOKEN LENGTH ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Khmer tokens:  Avg=5.0, Min=1, Max=20, Unique=5,352\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "⚠️  TOP 20 NON-KHMER TOKENS (Suspicious)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  ✓ No non-Khmer tokens found!\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ SAMPLE KHMER TOKENS (first 30 unique)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  ទ្រទ្រង់     | ធេង          | អ្នកវង្ស     | ការផលលំបាក   | សារី        \n",
      "  អ្នកស្រី     | សណ្ឋាគារ     | ជាពិសេស      | ដំណើរទស្សនកិច្ច | ទា          \n",
      "  ប្រដាប់ប្រដា | កងកម្លាំង    | កប្បនេះ      | មហាវិទ្យាល័យ | អ្វី        \n",
      "  ប្រ          | សង្ខត        | សៅ           | តែម្តង       | យ៉ាងហោចណាស់ \n",
      "  អូន          | អធីបតី       | យ៉ាងតិច      | ការ          | នឹម         \n",
      "  ក្រឹត្យ      | សិុន         | ចង្វាត់      | ស៊ូឡាំង      | រត្ន        \n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🔬 CHARACTER-LEVEL BREAKDOWN (first 5 non-Khmer tokens)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🎯 QUALITY ASSESSMENT\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Status: ✓ GOOD\n",
      "\n",
      "================================================================================\n",
      "ANALYZING: No_Pronouns\n",
      "File: no_Pronouns.txt\n",
      "================================================================================\n",
      "✓ Total documents (lines): 58,409\n",
      "\n",
      "📄 SAMPLE DOCUMENTS (first 3):\n",
      "  1. សេចក្តីថ្លែងការណ៍ ពាក់ព័ន្ធ នឹង ព័ត៌មាន នេះ បាន ឲ្យដឹង ថា លោក នឹង ទៅ បំពេញ ទស្សនកិច្ច នៅ ប្រទេស ឥណ្ឌា តាម ការអញ្ជើញ របស់ នាយក រដ្ឋមន្ត្រី ឥណ្ឌា លោក ដោ...\n",
      "  2. ផ្តន្ទាទោស ដាក់ ពន្ធនាការ ដោយ ព្យួរ ទោស រឺ មិន ព្យួរ ទោស ក្នុង រយ ពេល ឆ្នាំ កន្លងទៅ ទេ នោះ តុលាការ អាច សំរេច មិន ដាក់ទោស ប៉ុន្តែ ត្រូវ\n",
      "  3. នៅ ឆ្នាំ ភាគ កណ្ដាល នៃ ចក្រភព បាន បែកបាក់ បង្កើត បានជា រាជាណាចក្រ តូច ឯករាជ្យ ជាច្រើន\n",
      "\n",
      "✓ Analyzing 29,580 tokens from first 1000 documents\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📊 TOKEN COMPOSITION ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Category                       |      Count |   Percentage\n",
      "-------------------------------+------------+-------------\n",
      "Pure Khmer script              |     29,580 |      100.0%\n",
      "Latin/Alphanumeric only        |          0 |        0.0%\n",
      "Numeric only                   |          0 |        0.0%\n",
      "Mixed script                   |          0 |        0.0%\n",
      "Other                          |          0 |        0.0%\n",
      "-------------------------------+------------+-------------\n",
      "TOTAL                          |     29,580 |      100.0%\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📏 TOKEN LENGTH ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Khmer tokens:  Avg=4.9, Min=1, Max=20, Unique=5,371\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "⚠️  TOP 20 NON-KHMER TOKENS (Suspicious)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  ✓ No non-Khmer tokens found!\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ SAMPLE KHMER TOKENS (first 30 unique)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  ទ្រទ្រង់     | ធេង          | អ្នកវង្ស     | ការផលលំបាក   | សារី        \n",
      "  អ្នកស្រី     | សណ្ឋាគារ     | ជាពិសេស      | ដំណើរទស្សនកិច្ច | ទា          \n",
      "  ប្រដាប់ប្រដា | កងកម្លាំង    | កប្បនេះ      | មហាវិទ្យាល័យ | អ្វី        \n",
      "  ប្រ          | សង្ខត        | សៅ           | តែម្តង       | យ៉ាងហោចណាស់ \n",
      "  អូន          | អធីបតី       | យ៉ាងតិច      | ការ          | នឹម         \n",
      "  ក្រឹត្យ      | សិុន         | ចង្វាត់      | ស៊ូឡាំង      | រត្ន        \n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🔬 CHARACTER-LEVEL BREAKDOWN (first 5 non-Khmer tokens)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🎯 QUALITY ASSESSMENT\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Status: ✓ GOOD\n",
      "\n",
      "================================================================================\n",
      "ANALYZING: No_Questions\n",
      "File: no_Question_and_Negation_Words.txt\n",
      "================================================================================\n",
      "✓ Total documents (lines): 58,409\n",
      "\n",
      "📄 SAMPLE DOCUMENTS (first 3):\n",
      "  1. សេចក្តីថ្លែងការណ៍ ពាក់ព័ន្ធ នឹង ព័ត៌មាន នេះ បាន ឲ្យដឹង ថា លោក នឹង ទៅ បំពេញ ទស្សនកិច្ច នៅ ប្រទេស ឥណ្ឌា តាម ការអញ្ជើញ របស់ នាយក រដ្ឋមន្ត្រី ឥណ្ឌា លោក ដោ...\n",
      "  2. ផ្តន្ទាទោស ដាក់ ពន្ធនាការ ដោយ ព្យួរ ទោស រឺ ព្យួរ ទោស ក្នុង រយ ពេល ឆ្នាំ កន្លងទៅ នោះ តុលាការ អាច សំរេច ដាក់ទោស ប៉ុន្តែ ត្រូវ\n",
      "  3. នៅ ឆ្នាំ ភាគ កណ្ដាល នៃ ចក្រភព បាន បែកបាក់ បង្កើត បានជា រាជាណាចក្រ តូច ឯករាជ្យ ជាច្រើន\n",
      "\n",
      "✓ Analyzing 29,726 tokens from first 1000 documents\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📊 TOKEN COMPOSITION ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Category                       |      Count |   Percentage\n",
      "-------------------------------+------------+-------------\n",
      "Pure Khmer script              |     29,726 |      100.0%\n",
      "Latin/Alphanumeric only        |          0 |        0.0%\n",
      "Numeric only                   |          0 |        0.0%\n",
      "Mixed script                   |          0 |        0.0%\n",
      "Other                          |          0 |        0.0%\n",
      "-------------------------------+------------+-------------\n",
      "TOTAL                          |     29,726 |      100.0%\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📏 TOKEN LENGTH ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Khmer tokens:  Avg=4.9, Min=1, Max=20, Unique=5,375\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "⚠️  TOP 20 NON-KHMER TOKENS (Suspicious)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  ✓ No non-Khmer tokens found!\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ SAMPLE KHMER TOKENS (first 30 unique)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  ទ្រទ្រង់     | ធេង          | អ្នកវង្ស     | ការផលលំបាក   | សារី        \n",
      "  អ្នកស្រី     | សណ្ឋាគារ     | ជាពិសេស      | ដំណើរទស្សនកិច្ច | ទា          \n",
      "  ប្រដាប់ប្រដា | កងកម្លាំង    | កប្បនេះ      | មហាវិទ្យាល័យ | អ្វី        \n",
      "  ប្រ          | សង្ខត        | សៅ           | តែម្តង       | យ៉ាងហោចណាស់ \n",
      "  អូន          | អធីបតី       | យ៉ាងតិច      | ការ          | នឹម         \n",
      "  ក្រឹត្យ      | សិុន         | ចង្វាត់      | ស៊ូឡាំង      | រត្ន        \n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🔬 CHARACTER-LEVEL BREAKDOWN (first 5 non-Khmer tokens)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🎯 QUALITY ASSESSMENT\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Status: ✓ GOOD\n",
      "\n",
      "\n",
      "================================================================================\n",
      "GENERATING COMPREHENSIVE SUMMARY REPORT\n",
      "================================================================================\n",
      "\n",
      "📊 SUMMARY TABLE\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "            Corpus  Documents  Total_Tokens  Khmer_Tokens  Khmer_%  Latin_Tokens  Latin_% Status\n",
      "          Original      58409         30036         30036    100.0             0      0.0 ✓ GOOD\n",
      "  No_All_Stopwords      58409         21334         21334    100.0             0      0.0 ✓ GOOD\n",
      "No_Auxiliary_Verbs      58409         28880         28880    100.0             0      0.0 ✓ GOOD\n",
      "   No_Conjunctions      58409         28241         28241    100.0             0      0.0 ✓ GOOD\n",
      "    No_Determiners      58409         29265         29265    100.0             0      0.0 ✓ GOOD\n",
      " No_Function_Nouns      58409         29154         29154    100.0             0      0.0 ✓ GOOD\n",
      "        No_Numbers      58409         29740         29740    100.0             0      0.0 ✓ GOOD\n",
      "      No_Particles      58409         29244         29244    100.0             0      0.0 ✓ GOOD\n",
      "     No_Politeness      58409         29833         29833    100.0             0      0.0 ✓ GOOD\n",
      "   No_Prepositions      58409         27995         27995    100.0             0      0.0 ✓ GOOD\n",
      "       No_Pronouns      58409         29580         29580    100.0             0      0.0 ✓ GOOD\n",
      "      No_Questions      58409         29726         29726    100.0             0      0.0 ✓ GOOD\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "✓ Summary saved to: D:\\Year 5\\S1\\Information Retrieval\\StopwordProject\\khmer_stopword_project\\data\\diagnostics\\diagnostic_summary.csv\n",
      "✓ Detailed report saved to: D:\\Year 5\\S1\\Information Retrieval\\StopwordProject\\khmer_stopword_project\\data\\diagnostics\\diagnostic_report.md\n",
      "✓ JSON results saved to: D:\\Year 5\\S1\\Information Retrieval\\StopwordProject\\khmer_stopword_project\\data\\diagnostics\\diagnostic_results.json\n",
      "\n",
      "================================================================================\n",
      "🎯 RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "✓ ALL FILES LOOK GOOD!\n",
      "   Khmer content is well-preserved across all corpora.\n",
      "\n",
      "\n",
      "================================================================================\n",
      " DIAGNOSTIC COMPLETE\n",
      "================================================================================\n",
      "\n",
      "📁 All results saved to: D:\\Year 5\\S1\\Information Retrieval\\StopwordProject\\khmer_stopword_project\\data\\diagnostics\n",
      "\n",
      "Files created:\n",
      "  • diagnostic_summary.csv - Quick overview table\n",
      "  • diagnostic_report.md - Detailed findings and recommendations\n",
      "  • diagnostic_results.json - Complete results in JSON format\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import os\n",
    "import json\n",
    "\n",
    "# ============================================================================\n",
    "# COMPREHENSIVE KHMER TEXT DIAGNOSTIC TOOL\n",
    "# ============================================================================\n",
    "\n",
    "class KhmerTextDiagnostic:\n",
    "    def __init__(self, base_dir):\n",
    "        self.base_dir = base_dir\n",
    "        self.khmer_pattern = re.compile(r'[\\u1780-\\u17FF]')  # Khmer Unicode range\n",
    "        self.latin_pattern = re.compile(r'[a-zA-Z]')\n",
    "        self.digit_pattern = re.compile(r'[0-9]')\n",
    "        self.results = {}\n",
    "        \n",
    "    def analyze_file(self, filepath, label, sample_size=1000):\n",
    "        \"\"\"Comprehensive analysis of a single file\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ANALYZING: {label}\")\n",
    "        print(f\"File: {os.path.basename(filepath)}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"❌ File not found: {filepath}\")\n",
    "            return None\n",
    "        \n",
    "        # Read file\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                lines = [line.strip() for line in f if line.strip()]\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error reading file: {e}\")\n",
    "            return None\n",
    "        \n",
    "        if not lines:\n",
    "            print(\"⚠️  Empty file - no lines to analyze!\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"✓ Total documents (lines): {len(lines):,}\")\n",
    "        \n",
    "        # Show sample documents\n",
    "        print(f\"\\n📄 SAMPLE DOCUMENTS (first 3):\")\n",
    "        for i, line in enumerate(lines[:3], 1):\n",
    "            preview = line[:150] + '...' if len(line) > 150 else line\n",
    "            print(f\"  {i}. {preview}\")\n",
    "        \n",
    "        # Tokenize sample\n",
    "        all_tokens = []\n",
    "        for line in lines[:sample_size]:\n",
    "            all_tokens.extend(line.split())\n",
    "        \n",
    "        if not all_tokens:\n",
    "            print(\"⚠️  No tokens found after splitting!\")\n",
    "            return None\n",
    "        \n",
    "        total_tokens = len(all_tokens)\n",
    "        print(f\"\\n✓ Analyzing {total_tokens:,} tokens from first {min(sample_size, len(lines))} documents\")\n",
    "        \n",
    "        # Categorize tokens by script\n",
    "        khmer_tokens = []\n",
    "        latin_tokens = []\n",
    "        digit_tokens = []\n",
    "        mixed_tokens = []\n",
    "        other_tokens = []\n",
    "        \n",
    "        for token in all_tokens:\n",
    "            has_khmer = bool(self.khmer_pattern.search(token))\n",
    "            has_latin = bool(self.latin_pattern.search(token))\n",
    "            has_digit = bool(self.digit_pattern.search(token))\n",
    "            \n",
    "            if has_khmer and not has_latin and not has_digit:\n",
    "                khmer_tokens.append(token)\n",
    "            elif has_latin and not has_khmer:\n",
    "                latin_tokens.append(token)\n",
    "            elif has_digit and not has_khmer and not has_latin:\n",
    "                digit_tokens.append(token)\n",
    "            elif has_khmer and (has_latin or has_digit):\n",
    "                mixed_tokens.append(token)\n",
    "            else:\n",
    "                other_tokens.append(token)\n",
    "        \n",
    "        # Calculate percentages\n",
    "        khmer_pct = (len(khmer_tokens) / total_tokens * 100) if total_tokens > 0 else 0\n",
    "        latin_pct = (len(latin_tokens) / total_tokens * 100) if total_tokens > 0 else 0\n",
    "        digit_pct = (len(digit_tokens) / total_tokens * 100) if total_tokens > 0 else 0\n",
    "        mixed_pct = (len(mixed_tokens) / total_tokens * 100) if total_tokens > 0 else 0\n",
    "        other_pct = (len(other_tokens) / total_tokens * 100) if total_tokens > 0 else 0\n",
    "        \n",
    "        # Display composition\n",
    "        print(f\"\\n{'─'*80}\")\n",
    "        print(f\"📊 TOKEN COMPOSITION ANALYSIS\")\n",
    "        print(f\"{'─'*80}\")\n",
    "        print(f\"{'Category':<30} | {'Count':>10} | {'Percentage':>12}\")\n",
    "        print(f\"{'-'*30}-+-{'-'*10}-+-{'-'*12}\")\n",
    "        print(f\"{'Pure Khmer script':<30} | {len(khmer_tokens):>10,} | {khmer_pct:>10.1f}%\")\n",
    "        print(f\"{'Latin/Alphanumeric only':<30} | {len(latin_tokens):>10,} | {latin_pct:>10.1f}%\")\n",
    "        print(f\"{'Numeric only':<30} | {len(digit_tokens):>10,} | {digit_pct:>10.1f}%\")\n",
    "        print(f\"{'Mixed script':<30} | {len(mixed_tokens):>10,} | {mixed_pct:>10.1f}%\")\n",
    "        print(f\"{'Other':<30} | {len(other_tokens):>10,} | {other_pct:>10.1f}%\")\n",
    "        print(f\"{'-'*30}-+-{'-'*10}-+-{'-'*12}\")\n",
    "        print(f\"{'TOTAL':<30} | {total_tokens:>10,} | {100.0:>10.1f}%\")\n",
    "        \n",
    "        # Token length analysis\n",
    "        print(f\"\\n{'─'*80}\")\n",
    "        print(f\"📏 TOKEN LENGTH ANALYSIS\")\n",
    "        print(f\"{'─'*80}\")\n",
    "        \n",
    "        if khmer_tokens:\n",
    "            khmer_lengths = [len(t) for t in khmer_tokens]\n",
    "            print(f\"Khmer tokens:  Avg={sum(khmer_lengths)/len(khmer_lengths):.1f}, \"\n",
    "                  f\"Min={min(khmer_lengths)}, Max={max(khmer_lengths)}, \"\n",
    "                  f\"Unique={len(set(khmer_tokens)):,}\")\n",
    "        \n",
    "        if latin_tokens:\n",
    "            latin_lengths = [len(t) for t in latin_tokens]\n",
    "            print(f\"Latin tokens:  Avg={sum(latin_lengths)/len(latin_lengths):.1f}, \"\n",
    "                  f\"Min={min(latin_lengths)}, Max={max(latin_lengths)}, \"\n",
    "                  f\"Unique={len(set(latin_tokens)):,}\")\n",
    "        \n",
    "        # Show most common suspicious tokens\n",
    "        print(f\"\\n{'─'*80}\")\n",
    "        print(f\"⚠️  TOP 20 NON-KHMER TOKENS (Suspicious)\")\n",
    "        print(f\"{'─'*80}\")\n",
    "        \n",
    "        non_khmer = latin_tokens + digit_tokens + other_tokens\n",
    "        if non_khmer:\n",
    "            for i, (token, count) in enumerate(Counter(non_khmer).most_common(20), 1):\n",
    "                print(f\"{i:2d}. '{token}' → appears {count:,} times\")\n",
    "        else:\n",
    "            print(\"  ✓ No non-Khmer tokens found!\")\n",
    "        \n",
    "        # Show sample Khmer tokens\n",
    "        print(f\"\\n{'─'*80}\")\n",
    "        print(f\"✓ SAMPLE KHMER TOKENS (first 30 unique)\")\n",
    "        print(f\"{'─'*80}\")\n",
    "        \n",
    "        if khmer_tokens:\n",
    "            unique_khmer = list(set(khmer_tokens))[:30]\n",
    "            for i in range(0, len(unique_khmer), 5):\n",
    "                row = unique_khmer[i:i+5]\n",
    "                print(f\"  {' | '.join(f'{t:<12}' for t in row)}\")\n",
    "        else:\n",
    "            print(\"  ❌ NO KHMER TOKENS FOUND!\")\n",
    "        \n",
    "        # Character-level analysis of suspicious tokens\n",
    "        print(f\"\\n{'─'*80}\")\n",
    "        print(f\"🔬 CHARACTER-LEVEL BREAKDOWN (first 5 non-Khmer tokens)\")\n",
    "        print(f\"{'─'*80}\")\n",
    "        \n",
    "        for token in list(set(non_khmer))[:5]:\n",
    "            chars_info = []\n",
    "            for char in token[:15]:  # Show first 15 chars\n",
    "                chars_info.append(f\"'{char}'(U+{ord(char):04X})\")\n",
    "            print(f\"  Token: '{token}'\")\n",
    "            print(f\"    → {', '.join(chars_info)}\")\n",
    "        \n",
    "        # Quality assessment\n",
    "        print(f\"\\n{'─'*80}\")\n",
    "        print(f\"🎯 QUALITY ASSESSMENT\")\n",
    "        print(f\"{'─'*80}\")\n",
    "        \n",
    "        status = \"✓ GOOD\" if khmer_pct > 80 else \"⚠️  PROBLEMATIC\" if khmer_pct > 50 else \"❌ CRITICAL\"\n",
    "        print(f\"Status: {status}\")\n",
    "        \n",
    "        if khmer_pct < 80:\n",
    "            print(f\"\\n⚠️  ISSUES DETECTED:\")\n",
    "            if khmer_pct < 50:\n",
    "                print(f\"  • Less than 50% Khmer content - severe data quality issue\")\n",
    "            if latin_pct > 20:\n",
    "                print(f\"  • High proportion of Latin/alphanumeric tokens ({latin_pct:.1f}%)\")\n",
    "            if len(khmer_tokens) == 0:\n",
    "                print(f\"  • NO KHMER TOKENS - complete data loss!\")\n",
    "            if khmer_tokens and sum(len(t) for t in khmer_tokens[:100])/min(100, len(khmer_tokens)) < 3:\n",
    "                print(f\"  • Very short Khmer tokens - possible over-segmentation\")\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'label': label,\n",
    "            'filename': os.path.basename(filepath),\n",
    "            'total_documents': len(lines),\n",
    "            'total_tokens': total_tokens,\n",
    "            'khmer_tokens': len(khmer_tokens),\n",
    "            'khmer_percentage': khmer_pct,\n",
    "            'latin_tokens': len(latin_tokens),\n",
    "            'latin_percentage': latin_pct,\n",
    "            'digit_tokens': len(digit_tokens),\n",
    "            'mixed_tokens': len(mixed_tokens),\n",
    "            'unique_khmer': len(set(khmer_tokens)),\n",
    "            'unique_non_khmer': len(set(non_khmer)),\n",
    "            'status': status,\n",
    "            'avg_khmer_length': sum(len(t) for t in khmer_tokens)/len(khmer_tokens) if khmer_tokens else 0,\n",
    "            'top_non_khmer': Counter(non_khmer).most_common(10)\n",
    "        }\n",
    "        \n",
    "        self.results[label] = result\n",
    "        return result\n",
    "    \n",
    "    def generate_summary_report(self, output_dir):\n",
    "        \"\"\"Generate comprehensive summary report\"\"\"\n",
    "        print(f\"\\n\\n{'='*80}\")\n",
    "        print(f\"GENERATING COMPREHENSIVE SUMMARY REPORT\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        if not self.results:\n",
    "            print(\"❌ No results to summarize!\")\n",
    "            return\n",
    "        \n",
    "        # Create summary DataFrame\n",
    "        summary_data = []\n",
    "        for label, result in self.results.items():\n",
    "            summary_data.append({\n",
    "                'Corpus': result['label'],\n",
    "                'Documents': result['total_documents'],\n",
    "                'Total_Tokens': result['total_tokens'],\n",
    "                'Khmer_Tokens': result['khmer_tokens'],\n",
    "                'Khmer_%': result['khmer_percentage'],\n",
    "                'Latin_Tokens': result['latin_tokens'],\n",
    "                'Latin_%': result['latin_percentage'],\n",
    "                'Status': result['status']\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Display summary table\n",
    "        print(\"📊 SUMMARY TABLE\")\n",
    "        print(\"─\" * 120)\n",
    "        print(df.to_string(index=False))\n",
    "        print(\"─\" * 120)\n",
    "        \n",
    "        # Save to CSV\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        csv_file = os.path.join(output_dir, 'diagnostic_summary.csv')\n",
    "        df.to_csv(csv_file, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\n✓ Summary saved to: {csv_file}\")\n",
    "        \n",
    "        # Generate detailed report\n",
    "        report_file = os.path.join(output_dir, 'diagnostic_report.md')\n",
    "        with open(report_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"# Khmer Corpus Diagnostic Report\\n\\n\")\n",
    "            f.write(\"## Executive Summary\\n\\n\")\n",
    "            \n",
    "            total_files = len(self.results)\n",
    "            good_files = sum(1 for r in self.results.values() if r['khmer_percentage'] > 80)\n",
    "            problem_files = sum(1 for r in self.results.values() if r['khmer_percentage'] <= 80)\n",
    "            critical_files = sum(1 for r in self.results.values() if r['khmer_percentage'] < 50)\n",
    "            \n",
    "            f.write(f\"- **Total files analyzed**: {total_files}\\n\")\n",
    "            f.write(f\"- **Files with good quality (>80% Khmer)**: {good_files}\\n\")\n",
    "            f.write(f\"- **Files with issues (50-80% Khmer)**: {problem_files - critical_files}\\n\")\n",
    "            f.write(f\"- **Files with critical issues (<50% Khmer)**: {critical_files}\\n\\n\")\n",
    "            \n",
    "            f.write(\"## Detailed Results\\n\\n\")\n",
    "            f.write(df.to_markdown(index=False))\n",
    "            f.write(\"\\n\\n\")\n",
    "            \n",
    "            f.write(\"## Issues and Recommendations\\n\\n\")\n",
    "            \n",
    "            for label, result in self.results.items():\n",
    "                if result['khmer_percentage'] < 80:\n",
    "                    f.write(f\"### {label}\\n\\n\")\n",
    "                    f.write(f\"- **Khmer content**: {result['khmer_percentage']:.1f}%\\n\")\n",
    "                    f.write(f\"- **Top non-Khmer tokens**: {', '.join([t[0] for t in result['top_non_khmer'][:5]])}\\n\")\n",
    "                    \n",
    "                    if result['khmer_percentage'] < 50:\n",
    "                        f.write(f\"- **🚨 CRITICAL**: Less than 50% Khmer content\\n\")\n",
    "                        f.write(f\"- **Recommendation**: Review preprocessing pipeline completely\\n\")\n",
    "                    elif result['avg_khmer_length'] < 3:\n",
    "                        f.write(f\"- **⚠️  WARNING**: Very short Khmer tokens (avg {result['avg_khmer_length']:.1f} chars)\\n\")\n",
    "                        f.write(f\"- **Recommendation**: Check tokenization algorithm\\n\")\n",
    "                    \n",
    "                    f.write(\"\\n\")\n",
    "        \n",
    "        print(f\"✓ Detailed report saved to: {report_file}\")\n",
    "        \n",
    "        # Generate JSON for programmatic access\n",
    "        json_file = os.path.join(output_dir, 'diagnostic_results.json')\n",
    "        with open(json_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.results, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"✓ JSON results saved to: {json_file}\")\n",
    "        \n",
    "        # Final recommendations\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"🎯 RECOMMENDATIONS\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        if critical_files > 0:\n",
    "            print(\"❌ CRITICAL ISSUES FOUND:\")\n",
    "            print(\"   1. Check your preprocessing/tokenization code\")\n",
    "            print(\"   2. Verify source data is properly encoded (UTF-8)\")\n",
    "            print(\"   3. Review stopword removal - it may be too aggressive\")\n",
    "            print(\"   4. Examine raw source files for contamination\\n\")\n",
    "        elif problem_files > 0:\n",
    "            print(\"⚠️  SOME ISSUES FOUND:\")\n",
    "            print(\"   1. Review tokenization settings\")\n",
    "            print(\"   2. Check for metadata contamination in source\")\n",
    "            print(\"   3. Verify stopword lists are appropriate\\n\")\n",
    "        else:\n",
    "            print(\"✓ ALL FILES LOOK GOOD!\")\n",
    "            print(\"   Khmer content is well-preserved across all corpora.\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    base_dir = r'D:\\Year 5\\S1\\Information Retrieval\\StopwordProject\\khmer_stopword_project\\data\\processed'\n",
    "    output_dir = r'D:\\Year 5\\S1\\Information Retrieval\\StopwordProject\\khmer_stopword_project\\data\\diagnostics'\n",
    "    \n",
    "    # Files to analyze\n",
    "    files_to_analyze = [\n",
    "        ('Original', 'original_segmented_sentences.txt'),\n",
    "        ('No_All_Stopwords', 'no_all_stopwords.txt'),\n",
    "        ('No_Auxiliary_Verbs', 'no_Auxiliary_Verbs___Aspect_Markers.txt'),\n",
    "        ('No_Conjunctions', 'no_Conjunctions.txt'),\n",
    "        ('No_Determiners', 'no_Determiners_and_Quantifiers.txt'),\n",
    "        ('No_Function_Nouns', 'no_Function_Nouns.txt'),\n",
    "        ('No_Numbers', 'no_Numbers_and_Time_Expressions.txt'),\n",
    "        ('No_Particles', 'no_Particles_and_Discourse_Markers.txt'),\n",
    "        ('No_Politeness', 'no_Politeness_and_Honorifics.txt'),\n",
    "        ('No_Prepositions', 'no_Prepositions___Relational_Words.txt'),\n",
    "        ('No_Pronouns', 'no_Pronouns.txt'),\n",
    "        ('No_Questions', 'no_Question_and_Negation_Words.txt')\n",
    "    ]\n",
    "    \n",
    "    # Initialize diagnostic tool\n",
    "    print(\"=\"*80)\n",
    "    print(\" KHMER TEXT DIAGNOSTIC TOOL\")\n",
    "    print(\" Comprehensive Analysis of Processed Corpora\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    diagnostic = KhmerTextDiagnostic(base_dir)\n",
    "    \n",
    "    # Analyze each file\n",
    "    for label, filename in files_to_analyze:\n",
    "        filepath = os.path.join(base_dir, filename)\n",
    "        diagnostic.analyze_file(filepath, label, sample_size=1000)\n",
    "    \n",
    "    # Generate summary report\n",
    "    diagnostic.generate_summary_report(output_dir)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\" DIAGNOSTIC COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n📁 All results saved to: {output_dir}\")\n",
    "    print(\"\\nFiles created:\")\n",
    "    print(\"  • diagnostic_summary.csv - Quick overview table\")\n",
    "    print(\"  • diagnostic_report.md - Detailed findings and recommendations\")\n",
    "    print(\"  • diagnostic_results.json - Complete results in JSON format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66125eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      " KHMER TEXT SEGMENTATION DIAGNOSTIC TOOL\n",
      " Identifying Word Segmentation Issues\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ANALYZING: original_segmented_sentences.txt\n",
      "================================================================================\n",
      "\n",
      "Total lines: 58,409\n",
      "\n",
      "📄 RAW TEXT SAMPLES (first 10 lines):\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      " 1. សេចក្តីថ្លែងការណ៍ ពាក់ព័ន្ធ នឹង ព័ត៌មាន នេះ   បាន ឲ្យដឹង ថា   លោក   នឹង ទៅ បំពេញ ទស្សនកិច្ច នៅ ប្រទេស ឥណ្ឌា   តាម ការអញ្ជើញ របស់ នាយក រដ្ឋមន្ត្រី ឥណ្ឌា   លោក   ដោយ ចាត់ទុក ការធ្វើដំណើរ នេះ ថា   មាន សា...\n",
      " 2. ផ្តន្ទាទោស ដាក់ ពន្ធនាការ ដោយ ព្យួរ ទោស   រឺ មិន ព្យួរ ទោស ក្នុង រយ   ពេល ឆ្នាំ កន្លងទៅ ទេ   នោះ តុលាការ អាច សំរេច មិន ដាក់ទោស   ប៉ុន្តែ ត្រូវ\n",
      " 3. នៅ ឆ្នាំ   ភាគ កណ្ដាល នៃ ចក្រភព បាន បែកបាក់   បង្កើត បានជា រាជាណាចក្រ តូច ឯករាជ្យ ជាច្រើន\n",
      " 4. ក្នុង នោះ រួម មាន   បួរ ហ្គូញ លើ   បួរ ហ្គូញ ត្រង់ ហ្ស៊ូរ៉ាន   និង អ៊ីតាលី\n",
      " 5. ចំណែកឯ តំបន់ ឡូរ៉ែ ន   ត្រូវបាន បញ្ចូល ទៅ ភាគ ខាងកើត   ក្លាយជា រាជាណាចក្រ ចំណុះ\n",
      " 6. ស្តេច នៃ រាជាណាចក្រ ថ្មី ទាំង នេះ   បាន ឡើង កាន់អំណាច ដោយសារ ការគាំទ្រ ពី ពួក អភិជន ក្នុង តំបន់   ប្រឆាំង នឹង អ្នក ទាមទារ រាជ្យ ពី ខ្សែ រាជវង្ស ការ៉ូឡាំង ហ្ស៊ាំង\n",
      " 7. នៅ ភាគ ខាងកើត   ពួក អភិជន ក្នុង តំបន់ បាន ជ្រើសរើស តែងតាំង ពួក ឌុក   ឱ្យ កាន់អំណាច\n",
      " 8. ស្ថានភាព បាន ផ្លាស់ប្តូរ នៅ ឆ្នាំ   នៅ ពេល ដែល   ល្វីស   ឡង់ ហ្វង់   ដែល ជា ស្តេច ពី ខ្សែ រាជវង្ស ការ៉ូឡាំង ហ្ស៊ាំង ចុងក្រោយ បង្អស់ នៃ ហ្វ្រង់ស៊ី ខាងកើត   បាន សោយទិវង្គត\n",
      " 9. ប៉ុន្តែ ការបែកបាក់ នេះ ត្រូវបាន បញ្ចៀស   ដោយសារ ពួក មន្ត្រី ជាន់ ខ្ពស់ នៃ រាជាណាចក្រ បាន ជ្រើសរើស   កុងរ៉ាដ ទី   ឱ្យ ឡើងសោយរាជ្យ\n",
      "10. ទោះបីជា   កុងរ៉ាដ   មិន មែន ជា សមាជិក នៃ រាជវង្ស ការ៉ូឡាំង ហ្ស៊ាំង ក៏ដោយ   ទ្រង់ នៅតែ ជា ជនជាតិ ហ្វ្រង់   នៃ ខ្សែ ត្រកូល កុងរ៉ា ដាំង\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📊 TOKEN ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Token length distribution (first 1000 lines, 30,036 tokens):\n",
      "   1 chars:    198 (  0.7%) \n",
      "   2 chars:  3,567 ( 11.9%) ███████████\n",
      "   3 chars:  8,050 ( 26.8%) ██████████████████████████\n",
      "   4 chars:  3,376 ( 11.2%) ███████████\n",
      "   5 chars:  4,710 ( 15.7%) ███████████████\n",
      "   6 chars:  3,483 ( 11.6%) ███████████\n",
      "   7 chars:  2,550 (  8.5%) ████████\n",
      "   8 chars:  1,409 (  4.7%) ████\n",
      "   9 chars:    951 (  3.2%) ███\n",
      "  10 chars:    674 (  2.2%) ██\n",
      "  11 chars:    434 (  1.4%) █\n",
      "  12 chars:    223 (  0.7%) \n",
      "  13 chars:    195 (  0.6%) \n",
      "  14 chars:    105 (  0.3%) \n",
      "  15 chars:     48 (  0.2%) \n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📝 SAMPLE TOKENS BY LENGTH\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "1 character tokens:\n",
      "  ន | ថ | ឬ\n",
      "\n",
      "2 character tokens:\n",
      "  រឺ | រយ | ទេ | នៃ | ទៅ | នៅ | ថា\n",
      "\n",
      "3 character tokens:\n",
      "  ដោយ | នេះ | បាន | តាម | មាន | លោក | នឹង\n",
      "\n",
      "4 character tokens:\n",
      "  ទាំង | អ្នក | របស់ | ខ្សែ | ឱកាស | នាយក | អំពី | ដាក់ | ថ្មី\n",
      "\n",
      "5 character tokens:\n",
      "  ឆ្នាំ | ឥណ្ឌា | ព្យួរ | គ្រប់ | ក្នុង | បំពេញ | ផ្តល់\n",
      "\n",
      "6 character tokens:\n",
      "  ចក្រភព | ខាងកើត | ទាមទារ | ឲ្យដឹង | បញ្ចូល | ដោយសារ | ប្រទេស | ចំណែកឯ | កណ្ដាល | បង្កើត\n",
      "\n",
      "7 character tokens:\n",
      "  ដាក់ទោស | ចាត់ទុក | តុលាការ | រុស្ស៊ី | ប៉ុន្តែ | កន្លងទៅ | បែកបាក់ | ព័ត៌មាន | ទាំងមូល | ឯករាជ្យ\n",
      "\n",
      "8 character tokens:\n",
      "  ស្ថានភាព | ការបះបោរ | របៀបវារៈ | ការរៀបចំ | ត្រូវបាន | ជ្រើសរើស | ចុងក្រោយ\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📈 STATISTICAL SUMMARY\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Average token length: 4.90 characters\n",
      "\n",
      "Token breakdown:\n",
      "  1 char:       198 (  0.7%)\n",
      "  2 chars:    3,567 ( 11.9%)\n",
      "  3 chars:    8,050 ( 26.8%)\n",
      "  4+ chars:  18,221 ( 60.7%)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🔝 TOP 30 MOST COMMON TOKENS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      " 1. 'បាន' →    703 times ( 2.3%) [len=3]\n",
      " 2. 'និង' →    581 times ( 1.9%) [len=3]\n",
      " 3. 'ដែល' →    529 times ( 1.8%) [len=3]\n",
      " 4. 'មាន' →    527 times ( 1.8%) [len=3]\n",
      " 5. 'ថា' →    406 times ( 1.4%) [len=2]\n",
      " 6. 'ជា' →    404 times ( 1.3%) [len=2]\n",
      " 7. 'នៅ' →    383 times ( 1.3%) [len=2]\n",
      " 8. 'របស់' →    349 times ( 1.2%) [len=4]\n",
      " 9. 'ក្នុង' →    331 times ( 1.1%) [len=5]\n",
      "10. 'នេះ' →    261 times ( 0.9%) [len=3]\n",
      "11. 'ដោយ' →    231 times ( 0.8%) [len=3]\n",
      "12. 'នៃ' →    229 times ( 0.8%) [len=2]\n",
      "13. 'មិន' →    219 times ( 0.7%) [len=3]\n",
      "14. 'នោះ' →    204 times ( 0.7%) [len=3]\n",
      "15. 'ហើយ' →    195 times ( 0.6%) [len=3]\n",
      "16. 'ឆ្នាំ' →    190 times ( 0.6%) [len=5]\n",
      "17. 'ទៅ' →    185 times ( 0.6%) [len=2]\n",
      "18. 'លោក' →    172 times ( 0.6%) [len=3]\n",
      "19. 'ប្រទេស' →    160 times ( 0.5%) [len=6]\n",
      "20. 'ពី' →    158 times ( 0.5%) [len=2]\n",
      "21. 'កម្ពុជា' →    158 times ( 0.5%) [len=7]\n",
      "22. 'នឹង' →    140 times ( 0.5%) [len=3]\n",
      "23. 'ដល់' →    126 times ( 0.4%) [len=3]\n",
      "24. 'មួយ' →    125 times ( 0.4%) [len=3]\n",
      "25. 'ឱ្យ' →    117 times ( 0.4%) [len=3]\n",
      "26. 'ទី' →    117 times ( 0.4%) [len=2]\n",
      "27. 'នៅក្នុង' →    116 times ( 0.4%) [len=7]\n",
      "28. 'ក៏' →    116 times ( 0.4%) [len=2]\n",
      "29. 'ត្រូវ' →    115 times ( 0.4%) [len=5]\n",
      "30. 'ត្រូវបាន' →    110 times ( 0.4%) [len=8]\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🔍 DIAGNOSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Sample of longer tokens (5+ chars) - do these look like real words?\n",
      "  ចាត់ទុក, ឥណ្ឌា, ឲ្យដឹង, ការអញ្ជើញ, ទាំងមូល, ទស្សនកិច្ច, កិច្ចពិភាក្សា, ការធ្វើដំណើរ, ប្រទេស, បំពេញ\n",
      "\n",
      "✅ Tokens appear to be properly segmented!\n",
      "   Average length and distribution look reasonable for Khmer text.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ANALYZING: no_all_stopwords.txt\n",
      "================================================================================\n",
      "\n",
      "Total lines: 58,409\n",
      "\n",
      "📄 RAW TEXT SAMPLES (first 10 lines):\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      " 1. សេចក្តីថ្លែងការណ៍ ពាក់ព័ន្ធ ព័ត៌មាន ឲ្យដឹង ថា ទៅ បំពេញ ទស្សនកិច្ច នៅ ប្រទេស ឥណ្ឌា ការអញ្ជើញ នាយក រដ្ឋមន្ត្រី ឥណ្ឌា ដោយ ចាត់ទុក ការធ្វើដំណើរ ថា សារៈសំខាន់ ដោយ ផ្តល់ ឲ្យ កិច្ចពិភាក្សា ជ្រុងជ្រោយ របៀបវារ...\n",
      " 2. ផ្តន្ទាទោស ដាក់ ពន្ធនាការ ដោយ ព្យួរ ទោស រឺ ព្យួរ ទោស រយ ឆ្នាំ កន្លងទៅ តុលាការ សំរេច ដាក់ទោស ត្រូវ\n",
      " 3. នៅ ឆ្នាំ ភាគ កណ្ដាល ចក្រភព បែកបាក់ បង្កើត បានជា រាជាណាចក្រ តូច ឯករាជ្យ ជាច្រើន\n",
      " 4. រួម បួរ ហ្គូញ បួរ ហ្គូញ ត្រង់ ហ្ស៊ូរ៉ាន អ៊ីតាលី\n",
      " 5. តំបន់ ឡូរ៉ែ ន ទៅ ភាគ ខាងកើត ក្លាយជា រាជាណាចក្រ ចំណុះ\n",
      " 6. ស្តេច រាជាណាចក្រ ថ្មី កាន់អំណាច ការគាំទ្រ ពួក អភិជន តំបន់ ប្រឆាំង ទាមទារ រាជ្យ ខ្សែ រាជវង្ស ការ៉ូឡាំង ហ្ស៊ាំង\n",
      " 7. នៅ ភាគ ខាងកើត ពួក អភិជន តំបន់ ជ្រើសរើស តែងតាំង ពួក ឌុក ឱ្យ កាន់អំណាច\n",
      " 8. ស្ថានភាព ផ្លាស់ប្តូរ នៅ ឆ្នាំ នៅ ល្វីស ឡង់ ហ្វង់ ស្តេច ខ្សែ រាជវង្ស ការ៉ូឡាំង ហ្ស៊ាំង ចុងក្រោយ បង្អស់ ហ្វ្រង់ស៊ី ខាងកើត សោយទិវង្គត\n",
      " 9. ការបែកបាក់ បញ្ចៀស ពួក មន្ត្រី ជាន់ ខ្ពស់ រាជាណាចក្រ ជ្រើសរើស កុងរ៉ាដ ទី ឱ្យ ឡើងសោយរាជ្យ\n",
      "10. កុងរ៉ាដ សមាជិក រាជវង្ស ការ៉ូឡាំង ហ្ស៊ាំង ក៏ដោយ ទ្រង់ ជនជាតិ ហ្វ្រង់ ខ្សែ ត្រកូល កុងរ៉ា ដាំង\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📊 TOKEN ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Token length distribution (first 1000 lines, 21,334 tokens):\n",
      "   1 chars:    186 (  0.9%) \n",
      "   2 chars:  2,154 ( 10.1%) ██████████\n",
      "   3 chars:  3,384 ( 15.9%) ███████████████\n",
      "   4 chars:  2,558 ( 12.0%) ███████████\n",
      "   5 chars:  3,835 ( 18.0%) █████████████████\n",
      "   6 chars:  3,125 ( 14.6%) ██████████████\n",
      "   7 chars:  2,301 ( 10.8%) ██████████\n",
      "   8 chars:  1,173 (  5.5%) █████\n",
      "   9 chars:    902 (  4.2%) ████\n",
      "  10 chars:    651 (  3.1%) ███\n",
      "  11 chars:    432 (  2.0%) ██\n",
      "  12 chars:    222 (  1.0%) █\n",
      "  13 chars:    195 (  0.9%) \n",
      "  14 chars:    105 (  0.5%) \n",
      "  15 chars:     48 (  0.2%) \n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📝 SAMPLE TOKENS BY LENGTH\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "1 character tokens:\n",
      "  ន | ថ | ឬ\n",
      "\n",
      "2 character tokens:\n",
      "  រឺ | រយ | ទៅ | នៅ | ថា\n",
      "\n",
      "3 character tokens:\n",
      "  តូច | បួរ | ឲ្យ | ដោយ | រួម | ទោស | ភាគ\n",
      "\n",
      "4 character tokens:\n",
      "  សឺឡឺ | ដាំង | ខ្សែ | សាក់ | នាយក | ជាន់ | ដាក់ | ថ្មី\n",
      "\n",
      "5 character tokens:\n",
      "  ឆ្នាំ | ឥណ្ឌា | ព្យួរ | ត្រូវ | បំពេញ | ផ្តល់ | សំរេច\n",
      "\n",
      "6 character tokens:\n",
      "  បង្អស់ | ចក្រភព | ខាងកើត | ទាមទារ | ឲ្យដឹង | ប្រទេស | កណ្ដាល | បង្កើត\n",
      "\n",
      "7 character tokens:\n",
      "  ជាច្រើន | ដាក់ទោស | ចាត់ទុក | តុលាការ | រុស្ស៊ី | កន្លងទៅ | បែកបាក់ | ព័ត៌មាន | ទាំងមូល | ឯករាជ្យ\n",
      "\n",
      "8 character tokens:\n",
      "  ស្ថានភាព | ការបះបោរ | អាជ្ញាធរ | ការបរទេស | ជ័យជម្នះ | មាតុភូមិ | របៀបវារៈ | ជ្រើសរើស | ចុងក្រោយ\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📈 STATISTICAL SUMMARY\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Average token length: 5.45 characters\n",
      "\n",
      "Token breakdown:\n",
      "  1 char:       186 (  0.9%)\n",
      "  2 chars:    2,154 ( 10.1%)\n",
      "  3 chars:    3,384 ( 15.9%)\n",
      "  4+ chars:  15,610 ( 73.2%)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🔝 TOP 30 MOST COMMON TOKENS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      " 1. 'ថា' →    406 times ( 1.9%) [len=2]\n",
      " 2. 'នៅ' →    383 times ( 1.8%) [len=2]\n",
      " 3. 'ដោយ' →    231 times ( 1.1%) [len=3]\n",
      " 4. 'ឆ្នាំ' →    190 times ( 0.9%) [len=5]\n",
      " 5. 'ទៅ' →    185 times ( 0.9%) [len=2]\n",
      " 6. 'ប្រទេស' →    160 times ( 0.7%) [len=6]\n",
      " 7. 'កម្ពុជា' →    158 times ( 0.7%) [len=7]\n",
      " 8. 'ឱ្យ' →    117 times ( 0.5%) [len=3]\n",
      " 9. 'ទី' →    117 times ( 0.5%) [len=2]\n",
      "10. 'នៅក្នុង' →    116 times ( 0.5%) [len=7]\n",
      "11. 'ត្រូវ' →    115 times ( 0.5%) [len=5]\n",
      "12. 'គឺ' →    106 times ( 0.5%) [len=2]\n",
      "13. 'ឬ' →     97 times ( 0.5%) [len=1]\n",
      "14. 'មក' →     93 times ( 0.4%) [len=2]\n",
      "15. 'នាក់' →     88 times ( 0.4%) [len=4]\n",
      "16. 'ខេត្ត' →     83 times ( 0.4%) [len=5]\n",
      "17. 'មនុស្ស' →     82 times ( 0.4%) [len=6]\n",
      "18. 'តំបន់' →     80 times ( 0.4%) [len=5]\n",
      "19. 'ឲ្យ' →     73 times ( 0.3%) [len=3]\n",
      "20. 'ជាតិ' →     72 times ( 0.3%) [len=4]\n",
      "21. 'ទៀត' →     69 times ( 0.3%) [len=3]\n",
      "22. 'ធ្វើ' →     68 times ( 0.3%) [len=4]\n",
      "23. 'និយាយ' →     66 times ( 0.3%) [len=5]\n",
      "24. 'ឈ្មោះ' →     66 times ( 0.3%) [len=5]\n",
      "25. 'សម្ដេច' →     65 times ( 0.3%) [len=6]\n",
      "26. 'ស្រុក' →     65 times ( 0.3%) [len=5]\n",
      "27. 'ទីក្រុង' →     60 times ( 0.3%) [len=7]\n",
      "28. 'ទ្រង់' →     59 times ( 0.3%) [len=5]\n",
      "29. 'ជាមួយ' →     58 times ( 0.3%) [len=5]\n",
      "30. 'សម្តេច' →     58 times ( 0.3%) [len=6]\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🔍 DIAGNOSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Sample of longer tokens (5+ chars) - do these look like real words?\n",
      "  ចាត់ទុក, ឥណ្ឌា, ឲ្យដឹង, ការអញ្ជើញ, ទាំងមូល, ទំនាក់ទំនង, ទស្សនកិច្ច, កិច្ចពិភាក្សា, ការធ្វើដំណើរ, ប្រទេស\n",
      "\n",
      "✅ Tokens appear to be properly segmented!\n",
      "   Average length and distribution look reasonable for Khmer text.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ANALYZING: no_Auxiliary_Verbs___Aspect_Markers.txt\n",
      "================================================================================\n",
      "\n",
      "Total lines: 58,409\n",
      "\n",
      "📄 RAW TEXT SAMPLES (first 10 lines):\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      " 1. សេចក្តីថ្លែងការណ៍ ពាក់ព័ន្ធ ព័ត៌មាន នេះ ឲ្យដឹង ថា លោក ទៅ បំពេញ ទស្សនកិច្ច នៅ ប្រទេស ឥណ្ឌា តាម ការអញ្ជើញ របស់ នាយក រដ្ឋមន្ត្រី ឥណ្ឌា លោក ដោយ ចាត់ទុក ការធ្វើដំណើរ នេះ ថា មាន សារៈសំខាន់ ដោយ ផ្តល់ ឱកាស ឲ្...\n",
      " 2. ផ្តន្ទាទោស ដាក់ ពន្ធនាការ ដោយ ព្យួរ ទោស រឺ មិន ព្យួរ ទោស ក្នុង រយ ពេល ឆ្នាំ កន្លងទៅ ទេ នោះ តុលាការ សំរេច មិន ដាក់ទោស ប៉ុន្តែ ត្រូវ\n",
      " 3. នៅ ឆ្នាំ ភាគ កណ្ដាល នៃ ចក្រភព បែកបាក់ បង្កើត បានជា រាជាណាចក្រ តូច ឯករាជ្យ ជាច្រើន\n",
      " 4. ក្នុង នោះ រួម មាន បួរ ហ្គូញ លើ បួរ ហ្គូញ ត្រង់ ហ្ស៊ូរ៉ាន និង អ៊ីតាលី\n",
      " 5. ចំណែកឯ តំបន់ ឡូរ៉ែ ន បញ្ចូល ទៅ ភាគ ខាងកើត ក្លាយជា រាជាណាចក្រ ចំណុះ\n",
      " 6. ស្តេច នៃ រាជាណាចក្រ ថ្មី ទាំង នេះ ឡើង កាន់អំណាច ដោយសារ ការគាំទ្រ ពី ពួក អភិជន ក្នុង តំបន់ ប្រឆាំង អ្នក ទាមទារ រាជ្យ ពី ខ្សែ រាជវង្ស ការ៉ូឡាំង ហ្ស៊ាំង\n",
      " 7. នៅ ភាគ ខាងកើត ពួក អភិជន ក្នុង តំបន់ ជ្រើសរើស តែងតាំង ពួក ឌុក ឱ្យ កាន់អំណាច\n",
      " 8. ស្ថានភាព ផ្លាស់ប្តូរ នៅ ឆ្នាំ នៅ ពេល ដែល ល្វីស ឡង់ ហ្វង់ ដែល ជា ស្តេច ពី ខ្សែ រាជវង្ស ការ៉ូឡាំង ហ្ស៊ាំង ចុងក្រោយ បង្អស់ នៃ ហ្វ្រង់ស៊ី ខាងកើត សោយទិវង្គត\n",
      " 9. ប៉ុន្តែ ការបែកបាក់ នេះ បញ្ចៀស ដោយសារ ពួក មន្ត្រី ជាន់ ខ្ពស់ នៃ រាជាណាចក្រ ជ្រើសរើស កុងរ៉ាដ ទី ឱ្យ ឡើងសោយរាជ្យ\n",
      "10. ទោះបីជា កុងរ៉ាដ មិន មែន ជា សមាជិក នៃ រាជវង្ស ការ៉ូឡាំង ហ្ស៊ាំង ក៏ដោយ ទ្រង់ នៅតែ ជា ជនជាតិ ហ្វ្រង់ នៃ ខ្សែ ត្រកូល កុងរ៉ា ដាំង\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📊 TOKEN ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Token length distribution (first 1000 lines, 28,880 tokens):\n",
      "   1 chars:    198 (  0.7%) \n",
      "   2 chars:  3,567 ( 12.4%) ████████████\n",
      "   3 chars:  7,070 ( 24.5%) ████████████████████████\n",
      "   4 chars:  3,373 ( 11.7%) ███████████\n",
      "   5 chars:  4,673 ( 16.2%) ████████████████\n",
      "   6 chars:  3,476 ( 12.0%) ████████████\n",
      "   7 chars:  2,544 (  8.8%) ████████\n",
      "   8 chars:  1,291 (  4.5%) ████\n",
      "   9 chars:    946 (  3.3%) ███\n",
      "  10 chars:    674 (  2.3%) ██\n",
      "  11 chars:    434 (  1.5%) █\n",
      "  12 chars:    223 (  0.8%) \n",
      "  13 chars:    195 (  0.7%) \n",
      "  14 chars:    105 (  0.4%) \n",
      "  15 chars:     48 (  0.2%) \n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📝 SAMPLE TOKENS BY LENGTH\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "1 character tokens:\n",
      "  ន | ថ | ឬ\n",
      "\n",
      "2 character tokens:\n",
      "  រឺ | រយ | ទេ | នៃ | ទៅ | នៅ | ថា\n",
      "\n",
      "3 character tokens:\n",
      "  ឲ្យ | ដោយ | នេះ | តាម | មាន | លោក\n",
      "\n",
      "4 character tokens:\n",
      "  ទាំង | អ្នក | របស់ | ខ្សែ | ឱកាស | នាយក | អំពី | ដាក់ | ថ្មី\n",
      "\n",
      "5 character tokens:\n",
      "  ឆ្នាំ | ឥណ្ឌា | ព្យួរ | គ្រប់ | ក្នុង | បំពេញ | ផ្តល់\n",
      "\n",
      "6 character tokens:\n",
      "  ចក្រភព | ខាងកើត | ទាមទារ | ឲ្យដឹង | បញ្ចូល | ដោយសារ | ប្រទេស | ចំណែកឯ | កណ្ដាល | បង្កើត\n",
      "\n",
      "7 character tokens:\n",
      "  ដាក់ទោស | ចាត់ទុក | តុលាការ | រុស្ស៊ី | ប៉ុន្តែ | កន្លងទៅ | បែកបាក់ | ព័ត៌មាន | ទាំងមូល | ឯករាជ្យ\n",
      "\n",
      "8 character tokens:\n",
      "  ស្ថានភាព | ការបះបោរ | ពេញចិត្ត | ការបរទេស | ជ័យជម្នះ | របៀបវារៈ | ការរៀបចំ | ជ្រើសរើស | ចុងក្រោយ\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📈 STATISTICAL SUMMARY\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Average token length: 4.95 characters\n",
      "\n",
      "Token breakdown:\n",
      "  1 char:       198 (  0.7%)\n",
      "  2 chars:    3,567 ( 12.4%)\n",
      "  3 chars:    7,070 ( 24.5%)\n",
      "  4+ chars:  18,045 ( 62.5%)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🔝 TOP 30 MOST COMMON TOKENS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      " 1. 'និង' →    581 times ( 2.0%) [len=3]\n",
      " 2. 'ដែល' →    529 times ( 1.8%) [len=3]\n",
      " 3. 'មាន' →    527 times ( 1.8%) [len=3]\n",
      " 4. 'ថា' →    406 times ( 1.4%) [len=2]\n",
      " 5. 'ជា' →    404 times ( 1.4%) [len=2]\n",
      " 6. 'នៅ' →    383 times ( 1.3%) [len=2]\n",
      " 7. 'របស់' →    349 times ( 1.2%) [len=4]\n",
      " 8. 'ក្នុង' →    331 times ( 1.1%) [len=5]\n",
      " 9. 'នេះ' →    261 times ( 0.9%) [len=3]\n",
      "10. 'ដោយ' →    231 times ( 0.8%) [len=3]\n",
      "11. 'នៃ' →    229 times ( 0.8%) [len=2]\n",
      "12. 'មិន' →    219 times ( 0.8%) [len=3]\n",
      "13. 'នោះ' →    204 times ( 0.7%) [len=3]\n",
      "14. 'ហើយ' →    195 times ( 0.7%) [len=3]\n",
      "15. 'ឆ្នាំ' →    190 times ( 0.7%) [len=5]\n",
      "16. 'ទៅ' →    185 times ( 0.6%) [len=2]\n",
      "17. 'លោក' →    172 times ( 0.6%) [len=3]\n",
      "18. 'ប្រទេស' →    160 times ( 0.6%) [len=6]\n",
      "19. 'ពី' →    158 times ( 0.5%) [len=2]\n",
      "20. 'កម្ពុជា' →    158 times ( 0.5%) [len=7]\n",
      "21. 'ដល់' →    126 times ( 0.4%) [len=3]\n",
      "22. 'មួយ' →    125 times ( 0.4%) [len=3]\n",
      "23. 'ឱ្យ' →    117 times ( 0.4%) [len=3]\n",
      "24. 'ទី' →    117 times ( 0.4%) [len=2]\n",
      "25. 'នៅក្នុង' →    116 times ( 0.4%) [len=7]\n",
      "26. 'ក៏' →    116 times ( 0.4%) [len=2]\n",
      "27. 'ត្រូវ' →    115 times ( 0.4%) [len=5]\n",
      "28. 'តាម' →    108 times ( 0.4%) [len=3]\n",
      "29. 'គឺ' →    106 times ( 0.4%) [len=2]\n",
      "30. 'អ្នក' →    103 times ( 0.4%) [len=4]\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🔍 DIAGNOSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Sample of longer tokens (5+ chars) - do these look like real words?\n",
      "  ចាត់ទុក, ឥណ្ឌា, ឲ្យដឹង, ការអញ្ជើញ, ទាំងមូល, ទស្សនកិច្ច, កិច្ចពិភាក្សា, ការធ្វើដំណើរ, ប្រទេស, បំពេញ\n",
      "\n",
      "✅ Tokens appear to be properly segmented!\n",
      "   Average length and distribution look reasonable for Khmer text.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ANALYZING: no_Conjunctions.txt\n",
      "================================================================================\n",
      "\n",
      "Total lines: 58,409\n",
      "\n",
      "📄 RAW TEXT SAMPLES (first 10 lines):\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      " 1. សេចក្តីថ្លែងការណ៍ ពាក់ព័ន្ធ នឹង ព័ត៌មាន នេះ បាន ឲ្យដឹង ថា លោក នឹង ទៅ បំពេញ ទស្សនកិច្ច នៅ ប្រទេស ឥណ្ឌា តាម ការអញ្ជើញ របស់ នាយក រដ្ឋមន្ត្រី ឥណ្ឌា លោក ដោយ ចាត់ទុក ការធ្វើដំណើរ នេះ ថា មាន សារៈសំខាន់ ដោយ ផ...\n",
      " 2. ផ្តន្ទាទោស ដាក់ ពន្ធនាការ ដោយ ព្យួរ ទោស រឺ មិន ព្យួរ ទោស ក្នុង រយ ពេល ឆ្នាំ កន្លងទៅ ទេ នោះ តុលាការ អាច សំរេច មិន ដាក់ទោស ត្រូវ\n",
      " 3. នៅ ឆ្នាំ ភាគ កណ្ដាល នៃ ចក្រភព បាន បែកបាក់ បង្កើត បានជា រាជាណាចក្រ តូច ឯករាជ្យ ជាច្រើន\n",
      " 4. ក្នុង នោះ រួម មាន បួរ ហ្គូញ បួរ ហ្គូញ ត្រង់ ហ្ស៊ូរ៉ាន អ៊ីតាលី\n",
      " 5. តំបន់ ឡូរ៉ែ ន ត្រូវបាន ទៅ ភាគ ខាងកើត ក្លាយជា រាជាណាចក្រ ចំណុះ\n",
      " 6. ស្តេច នៃ រាជាណាចក្រ ថ្មី ទាំង នេះ បាន ឡើង កាន់អំណាច ការគាំទ្រ ពួក អភិជន ក្នុង តំបន់ ប្រឆាំង នឹង អ្នក ទាមទារ រាជ្យ ខ្សែ រាជវង្ស ការ៉ូឡាំង ហ្ស៊ាំង\n",
      " 7. នៅ ភាគ ខាងកើត ពួក អភិជន ក្នុង តំបន់ បាន ជ្រើសរើស តែងតាំង ពួក ឌុក ឱ្យ កាន់អំណាច\n",
      " 8. ស្ថានភាព បាន ផ្លាស់ប្តូរ នៅ ឆ្នាំ នៅ ពេល ល្វីស ឡង់ ហ្វង់ ជា ស្តេច ខ្សែ រាជវង្ស ការ៉ូឡាំង ហ្ស៊ាំង ចុងក្រោយ បង្អស់ នៃ ហ្វ្រង់ស៊ី ខាងកើត បាន សោយទិវង្គត\n",
      " 9. ការបែកបាក់ នេះ ត្រូវបាន បញ្ចៀស ពួក មន្ត្រី ជាន់ ខ្ពស់ នៃ រាជាណាចក្រ បាន ជ្រើសរើស កុងរ៉ាដ ទី ឱ្យ ឡើងសោយរាជ្យ\n",
      "10. កុងរ៉ាដ មិន មែន ជា សមាជិក នៃ រាជវង្ស ការ៉ូឡាំង ហ្ស៊ាំង ក៏ដោយ ទ្រង់ នៅតែ ជា ជនជាតិ ហ្វ្រង់ នៃ ខ្សែ ត្រកូល កុងរ៉ា ដាំង\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📊 TOKEN ANALYSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Token length distribution (first 1000 lines, 28,241 tokens):\n",
      "   1 chars:    198 (  0.7%) \n",
      "   2 chars:  3,250 ( 11.5%) ███████████\n",
      "   3 chars:  6,894 ( 24.4%) ████████████████████████\n",
      "   4 chars:  3,353 ( 11.9%) ███████████\n",
      "   5 chars:  4,689 ( 16.6%) ████████████████\n",
      "   6 chars:  3,320 ( 11.8%) ███████████\n",
      "   7 chars:  2,462 (  8.7%) ████████\n",
      "   8 chars:  1,391 (  4.9%) ████\n",
      "   9 chars:    947 (  3.4%) ███\n",
      "  10 chars:    670 (  2.4%) ██\n",
      "  11 chars:    434 (  1.5%) █\n",
      "  12 chars:    222 (  0.8%) \n",
      "  13 chars:    195 (  0.7%) \n",
      "  14 chars:    105 (  0.4%) \n",
      "  15 chars:     48 (  0.2%) \n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📝 SAMPLE TOKENS BY LENGTH\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "1 character tokens:\n",
      "  ន | ថ | ឬ\n",
      "\n",
      "2 character tokens:\n",
      "  រឺ | រយ | ទេ | នៃ | ទៅ | នៅ | ថា\n",
      "\n",
      "3 character tokens:\n",
      "  ដោយ | នេះ | បាន | តាម | មាន | លោក | នឹង\n",
      "\n",
      "4 character tokens:\n",
      "  ទាំង | អ្នក | របស់ | ខ្សែ | ឱកាស | នាយក | អំពី | ដាក់ | ថ្មី\n",
      "\n",
      "5 character tokens:\n",
      "  ឆ្នាំ | ឥណ្ឌា | ព្យួរ | គ្រប់ | ក្នុង | បំពេញ | ផ្តល់\n",
      "\n",
      "6 character tokens:\n",
      "  បង្អស់ | ចក្រភព | ខាងកើត | ទាមទារ | ឲ្យដឹង | ប្រទេស | កណ្ដាល | បង្កើត\n",
      "\n",
      "7 character tokens:\n",
      "  ជាច្រើន | ដាក់ទោស | ចាត់ទុក | តុលាការ | រុស្ស៊ី | កន្លងទៅ | បែកបាក់ | ព័ត៌មាន | ទាំងមូល | ឯករាជ្យ\n",
      "\n",
      "8 character tokens:\n",
      "  ស្ថានភាព | ការបះបោរ | របៀបវារៈ | ការរៀបចំ | ត្រូវបាន | ជ្រើសរើស | ចុងក្រោយ\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "📈 STATISTICAL SUMMARY\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Average token length: 4.99 characters\n",
      "\n",
      "Token breakdown:\n",
      "  1 char:       198 (  0.7%)\n",
      "  2 chars:    3,250 ( 11.5%)\n",
      "  3 chars:    6,894 ( 24.4%)\n",
      "  4+ chars:  17,899 ( 63.4%)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🔝 TOP 30 MOST COMMON TOKENS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      " 1. 'បាន' →    703 times ( 2.5%) [len=3]\n",
      " 2. 'មាន' →    527 times ( 1.9%) [len=3]\n",
      " 3. 'ថា' →    406 times ( 1.4%) [len=2]\n",
      " 4. 'ជា' →    404 times ( 1.4%) [len=2]\n",
      " 5. 'នៅ' →    383 times ( 1.4%) [len=2]\n",
      " 6. 'របស់' →    349 times ( 1.2%) [len=4]\n",
      " 7. 'ក្នុង' →    331 times ( 1.2%) [len=5]\n",
      " 8. 'នេះ' →    261 times ( 0.9%) [len=3]\n",
      " 9. 'ដោយ' →    231 times ( 0.8%) [len=3]\n",
      "10. 'នៃ' →    229 times ( 0.8%) [len=2]\n",
      "11. 'មិន' →    219 times ( 0.8%) [len=3]\n",
      "12. 'នោះ' →    204 times ( 0.7%) [len=3]\n",
      "13. 'ហើយ' →    195 times ( 0.7%) [len=3]\n",
      "14. 'ឆ្នាំ' →    190 times ( 0.7%) [len=5]\n",
      "15. 'ទៅ' →    185 times ( 0.7%) [len=2]\n",
      "16. 'លោក' →    172 times ( 0.6%) [len=3]\n",
      "17. 'ប្រទេស' →    160 times ( 0.6%) [len=6]\n",
      "18. 'កម្ពុជា' →    158 times ( 0.6%) [len=7]\n",
      "19. 'នឹង' →    140 times ( 0.5%) [len=3]\n",
      "20. 'ដល់' →    126 times ( 0.4%) [len=3]\n",
      "21. 'មួយ' →    125 times ( 0.4%) [len=3]\n",
      "22. 'ឱ្យ' →    117 times ( 0.4%) [len=3]\n",
      "23. 'ទី' →    117 times ( 0.4%) [len=2]\n",
      "24. 'នៅក្នុង' →    116 times ( 0.4%) [len=7]\n",
      "25. 'ក៏' →    116 times ( 0.4%) [len=2]\n",
      "26. 'ត្រូវ' →    115 times ( 0.4%) [len=5]\n",
      "27. 'ត្រូវបាន' →    110 times ( 0.4%) [len=8]\n",
      "28. 'តាម' →    108 times ( 0.4%) [len=3]\n",
      "29. 'គឺ' →    106 times ( 0.4%) [len=2]\n",
      "30. 'អ្នក' →    103 times ( 0.4%) [len=4]\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "🔍 DIAGNOSIS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Sample of longer tokens (5+ chars) - do these look like real words?\n",
      "  ចាត់ទុក, ឥណ្ឌា, ឲ្យដឹង, ការអញ្ជើញ, ទាំងមូល, ទស្សនកិច្ច, កិច្ចពិភាក្សា, ការធ្វើដំណើរ, ប្រទេស, បំពេញ\n",
      "\n",
      "✅ Tokens appear to be properly segmented!\n",
      "   Average length and distribution look reasonable for Khmer text.\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      " DIAGNOSTIC COMPLETE\n",
      "================================================================================\n",
      "\n",
      "📋 NEXT STEPS:\n",
      "1. Review the diagnosis for each file above\n",
      "2. If issues are found, check your preprocessing/segmentation code\n",
      "3. The problem is BEFORE stopword removal - in the tokenization stage\n",
      "4. You may need to re-segment your original text with a proper tool\n",
      "\n",
      "💡 TIP: If 'original_segmented_sentences.txt' has issues,\n",
      "   ALL downstream files will have the same problem!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# SOURCE FILE SEGMENTATION DIAGNOSTIC\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_source_segmentation(filepath, num_samples=10):\n",
    "    \"\"\"\n",
    "    Diagnose if source files have segmentation problems\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ANALYZING: {os.path.basename(filepath)}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"❌ File not found: {filepath}\")\n",
    "        return\n",
    "    \n",
    "    # Read file\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    if not lines:\n",
    "        print(\"⚠️  Empty file!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Total lines: {len(lines):,}\\n\")\n",
    "    \n",
    "    # Show raw samples\n",
    "    print(\"📄 RAW TEXT SAMPLES (first 10 lines):\")\n",
    "    print(\"─\" * 80)\n",
    "    for i, line in enumerate(lines[:num_samples], 1):\n",
    "        preview = line[:200] + '...' if len(line) > 200 else line\n",
    "        print(f\"{i:2d}. {preview}\")\n",
    "    \n",
    "    # Analyze token structure\n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(\"📊 TOKEN ANALYSIS\")\n",
    "    print(\"─\" * 80)\n",
    "    \n",
    "    all_tokens = []\n",
    "    for line in lines[:1000]:  # Sample first 1000 lines\n",
    "        all_tokens.extend(line.split())\n",
    "    \n",
    "    # Categorize by length\n",
    "    length_dist = Counter([len(t) for t in all_tokens])\n",
    "    \n",
    "    print(f\"\\nToken length distribution (first 1000 lines, {len(all_tokens):,} tokens):\")\n",
    "    for length in sorted(length_dist.keys())[:15]:  # Show first 15 lengths\n",
    "        count = length_dist[length]\n",
    "        pct = count / len(all_tokens) * 100\n",
    "        bar = '█' * int(pct)\n",
    "        print(f\"  {length:2d} chars: {count:6,} ({pct:5.1f}%) {bar}\")\n",
    "    \n",
    "    # Check for Khmer characters\n",
    "    khmer_pattern = re.compile(r'[\\u1780-\\u17FF]')\n",
    "    \n",
    "    # Sample tokens by length\n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(\"📝 SAMPLE TOKENS BY LENGTH\")\n",
    "    print(\"─\" * 80)\n",
    "    \n",
    "    tokens_by_length = {}\n",
    "    for token in all_tokens:\n",
    "        length = len(token)\n",
    "        if length not in tokens_by_length:\n",
    "            tokens_by_length[length] = []\n",
    "        if len(tokens_by_length[length]) < 10:  # Keep max 10 samples per length\n",
    "            tokens_by_length[length].append(token)\n",
    "    \n",
    "    for length in sorted(tokens_by_length.keys())[:8]:  # Show first 8 lengths\n",
    "        samples = list(set(tokens_by_length[length]))[:10]\n",
    "        print(f\"\\n{length} character tokens:\")\n",
    "        print(f\"  {' | '.join(samples)}\")\n",
    "    \n",
    "    # Statistical analysis\n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(\"📈 STATISTICAL SUMMARY\")\n",
    "    print(\"─\" * 80)\n",
    "    \n",
    "    avg_length = sum(len(t) for t in all_tokens) / len(all_tokens)\n",
    "    one_char = sum(1 for t in all_tokens if len(t) == 1)\n",
    "    two_char = sum(1 for t in all_tokens if len(t) == 2)\n",
    "    three_char = sum(1 for t in all_tokens if len(t) == 3)\n",
    "    four_plus = sum(1 for t in all_tokens if len(t) >= 4)\n",
    "    \n",
    "    print(f\"Average token length: {avg_length:.2f} characters\")\n",
    "    print(f\"\\nToken breakdown:\")\n",
    "    print(f\"  1 char:   {one_char:7,} ({one_char/len(all_tokens)*100:5.1f}%)\")\n",
    "    print(f\"  2 chars:  {two_char:7,} ({two_char/len(all_tokens)*100:5.1f}%)\")\n",
    "    print(f\"  3 chars:  {three_char:7,} ({three_char/len(all_tokens)*100:5.1f}%)\")\n",
    "    print(f\"  4+ chars: {four_plus:7,} ({four_plus/len(all_tokens)*100:5.1f}%)\")\n",
    "    \n",
    "    # Most common tokens\n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(\"🔝 TOP 30 MOST COMMON TOKENS\")\n",
    "    print(\"─\" * 80)\n",
    "    \n",
    "    token_freq = Counter(all_tokens)\n",
    "    for i, (token, count) in enumerate(token_freq.most_common(30), 1):\n",
    "        pct = count / len(all_tokens) * 100\n",
    "        print(f\"{i:2d}. '{token}' → {count:6,} times ({pct:4.1f}%) [len={len(token)}]\")\n",
    "    \n",
    "    # Diagnosis\n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(\"🔍 DIAGNOSIS\")\n",
    "    print(\"─\" * 80)\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    if avg_length < 3:\n",
    "        issues.append(f\"❌ CRITICAL: Average token length is {avg_length:.2f} chars\")\n",
    "        issues.append(f\"   → Tokens are TOO SHORT - likely over-segmented!\")\n",
    "        issues.append(f\"   → Normal Khmer words should be 3-10+ characters\")\n",
    "    \n",
    "    if one_char / len(all_tokens) > 0.2:\n",
    "        issues.append(f\"❌ CRITICAL: {one_char/len(all_tokens)*100:.1f}% are single characters\")\n",
    "        issues.append(f\"   → This indicates severe segmentation problems\")\n",
    "    \n",
    "    if two_char / len(all_tokens) > 0.3:\n",
    "        issues.append(f\"⚠️  WARNING: {two_char/len(all_tokens)*100:.1f}% are 2-character tokens\")\n",
    "        issues.append(f\"   → Many tokens appear to be word fragments\")\n",
    "    \n",
    "    if four_plus / len(all_tokens) < 0.3:\n",
    "        issues.append(f\"⚠️  WARNING: Only {four_plus/len(all_tokens)*100:.1f}% are 4+ characters\")\n",
    "        issues.append(f\"   → Too few complete words\")\n",
    "    \n",
    "    # Check if tokens look like real words\n",
    "    sample_tokens = [t for t in all_tokens if len(t) >= 5][:20]\n",
    "    if sample_tokens:\n",
    "        print(\"\\nSample of longer tokens (5+ chars) - do these look like real words?\")\n",
    "        print(f\"  {', '.join(list(set(sample_tokens))[:10])}\")\n",
    "    \n",
    "    if issues:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"⚠️  ISSUES DETECTED:\")\n",
    "        print(f\"{'='*80}\")\n",
    "        for issue in issues:\n",
    "            print(issue)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"💡 RECOMMENDED ACTIONS:\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(\"1. CHECK YOUR TOKENIZATION/SEGMENTATION CODE\")\n",
    "        print(\"   → The tool that created these files is breaking words incorrectly\")\n",
    "        print(\"2. VERIFY YOUR ORIGINAL RAW TEXT\")\n",
    "        print(\"   → Go back to the source before any processing\")\n",
    "        print(\"3. USE A PROPER KHMER WORD SEGMENTER\")\n",
    "        print(\"   → Libraries: khmer-nltk, khmernltk, or PyThaiNLP (supports Khmer)\")\n",
    "        print(\"4. IF USING CUSTOM SEGMENTATION\")\n",
    "        print(\"   → Review your algorithm - it's splitting at character level, not word level\")\n",
    "    else:\n",
    "        print(\"\\n✅ Tokens appear to be properly segmented!\")\n",
    "        print(\"   Average length and distribution look reasonable for Khmer text.\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = r'D:\\Year 5\\S1\\Information Retrieval\\StopwordProject\\khmer_stopword_project\\data\\processed'\n",
    "    \n",
    "    # Files to check\n",
    "    files_to_check = [\n",
    "        'original_segmented_sentences.txt',\n",
    "        'no_all_stopwords.txt',\n",
    "        'no_Auxiliary_Verbs___Aspect_Markers.txt',\n",
    "        'no_Conjunctions.txt'\n",
    "    ]\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\" KHMER TEXT SEGMENTATION DIAGNOSTIC TOOL\")\n",
    "    print(\" Identifying Word Segmentation Issues\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for filename in files_to_check:\n",
    "        filepath = os.path.join(base_dir, filename)\n",
    "        analyze_source_segmentation(filepath, num_samples=10)\n",
    "        print(\"\\n\\n\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\" DIAGNOSTIC COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n📋 NEXT STEPS:\")\n",
    "    print(\"1. Review the diagnosis for each file above\")\n",
    "    print(\"2. If issues are found, check your preprocessing/segmentation code\")\n",
    "    print(\"3. The problem is BEFORE stopword removal - in the tokenization stage\")\n",
    "    print(\"4. You may need to re-segment your original text with a proper tool\")\n",
    "    print(\"\\n💡 TIP: If 'original_segmented_sentences.txt' has issues,\")\n",
    "    print(\"   ALL downstream files will have the same problem!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0836674a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      " COMPREHENSIVE TF-IDF ANALYSIS WITH QUALITY FILTERING\n",
      " Enhanced version with Khmer-only token extraction\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      " COMPREHENSIVE TF-IDF ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Loading: Original\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ Loaded 58,409 documents\n",
      "  Sample kept tokens: ['សេចក្តីថ្លែងការណ៍', 'ពាក់ព័ន្ធ', 'នឹង', 'ព័ត៌មាន', 'នេះ', 'បាន', 'ឲ្យដឹង', 'ថា', 'លោក', 'នឹង']\n",
      "  Original tokens: 1,703,731\n",
      "  After cleaning: 1,691,824\n",
      "  Removed: 0.7% (non-Khmer content)\n",
      "  Documents retained: 58,398/58,409\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "TOKEN QUALITY DIAGNOSTIC: Original\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Token length distribution (first 100 docs):\n",
      "  1 character:       0 tokens (  0.0%)\n",
      "  2 characters:    314 tokens ( 11.3%)\n",
      "  3+ characters:  2,458 tokens ( 88.7%)\n",
      "\n",
      "  Sample of 3+ character tokens (will be kept):\n",
      "    យល់, ក្រហម, ព្រឹក, ចាត់ទុក, ក្តី, កាបាន, ភ្លៀង, ប្រាសចាក, ផឹក, ណាស់, គួរតែ, មើល, ខាងក្រោម, ទំនាក់ទំនង, បញ្ឆោត\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Calculating TF-IDF: Original\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  Documents after token filtering: 58,398\n",
      "✓ TF-IDF matrix shape: (58398, 5000)\n",
      "✓ Vocabulary size: 5,000 features\n",
      "\n",
      "  Sample vocabulary (first 20):\n",
      "     1. 'កកាយ' (len=4)\n",
      "     2. 'កកើត' (len=4)\n",
      "     3. 'កក់' (len=3)\n",
      "     4. 'កក់ក្តៅ' (len=7)\n",
      "     5. 'កក្កដា' (len=6)\n",
      "     6. 'កង' (len=2)\n",
      "     7. 'កងកម្លាំង' (len=9)\n",
      "     8. 'កងកម្លាំងប្រដាប់អាវុធ' (len=21)\n",
      "     9. 'កងទ័ព' (len=5)\n",
      "    10. 'កងពល' (len=4)\n",
      "    11. 'កងយោធពល' (len=7)\n",
      "    12. 'កងរាជអាវុធហត្ថ' (len=14)\n",
      "    13. 'កង់' (len=3)\n",
      "    14. 'កង្វល់' (len=6)\n",
      "    15. 'កង្វះ' (len=5)\n",
      "    16. 'កង្វះខាត' (len=8)\n",
      "    17. 'កញ្ចក់' (len=6)\n",
      "    18. 'កញ្ចប់' (len=6)\n",
      "    19. 'កញ្ញា' (len=5)\n",
      "    20. 'កឋិន' (len=4)\n",
      "\n",
      "  Vocabulary length distribution:\n",
      "    2 chars:   227 terms (  4.5%)\n",
      "    3 chars:   662 terms ( 13.2%)\n",
      "    4 chars:   522 terms ( 10.4%)\n",
      "    5 chars:   861 terms ( 17.2%)\n",
      "    6 chars:   749 terms ( 15.0%)\n",
      "    7 chars:   515 terms ( 10.3%)\n",
      "    8 chars:   446 terms (  8.9%)\n",
      "    9 chars:   344 terms (  6.9%)\n",
      "    10 chars:   259 terms (  5.2%)\n",
      "    11 chars:   161 terms (  3.2%)\n",
      "  Top 100 terms breakdown:\n",
      "    - Khmer (2+ chars): 100\n",
      "    - Non-Khmer: 0\n",
      "    - Single characters: 0\n",
      "✓ Saved top terms to: top_terms_Original.txt\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Loading: No_All_Stopwords\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ Loaded 58,409 documents\n",
      "  Sample kept tokens: ['សេចក្តីថ្លែងការណ៍', 'ពាក់ព័ន្ធ', 'ព័ត៌មាន', 'ឲ្យដឹង', 'ថា', 'ទៅ', 'បំពេញ', 'ទស្សនកិច្ច', 'នៅ', 'ប្រទេស']\n",
      "  Original tokens: 1,207,390\n",
      "  After cleaning: 1,196,010\n",
      "  Removed: 0.9% (non-Khmer content)\n",
      "  Documents retained: 58,394/58,409\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "TOKEN QUALITY DIAGNOSTIC: No_All_Stopwords\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Token length distribution (first 100 docs):\n",
      "  1 character:       0 tokens (  0.0%)\n",
      "  2 characters:    195 tokens (  9.5%)\n",
      "  3+ characters:  1,850 tokens ( 90.5%)\n",
      "\n",
      "  Sample of 3+ character tokens (will be kept):\n",
      "    យល់, ក្រហម, ព្រឹក, ចាត់ទុក, ក្តី, កាបាន, ភ្លៀង, ប្រាសចាក, ផឹក, មើល, ខាងក្រោម, ទំនាក់ទំនង, បញ្ឆោត, មធ្យម, ជាពិសេស\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Calculating TF-IDF: No_All_Stopwords\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  Documents after token filtering: 58,394\n",
      "✓ TF-IDF matrix shape: (58394, 5000)\n",
      "✓ Vocabulary size: 5,000 features\n",
      "\n",
      "  Sample vocabulary (first 20):\n",
      "     1. 'កកាយ' (len=4)\n",
      "     2. 'កកើត' (len=4)\n",
      "     3. 'កក់' (len=3)\n",
      "     4. 'កក់ក្ដៅ' (len=7)\n",
      "     5. 'កក់ក្តៅ' (len=7)\n",
      "     6. 'កក្កដា' (len=6)\n",
      "     7. 'កខ្វក់' (len=6)\n",
      "     8. 'កង' (len=2)\n",
      "     9. 'កងកម្លាំង' (len=9)\n",
      "    10. 'កងកម្លាំងប្រដាប់អាវុធ' (len=21)\n",
      "    11. 'កងទ័ព' (len=5)\n",
      "    12. 'កងពល' (len=4)\n",
      "    13. 'កងយោធពល' (len=7)\n",
      "    14. 'កងរាជអាវុធហត្ថ' (len=14)\n",
      "    15. 'កង់' (len=3)\n",
      "    16. 'កង្វល់' (len=6)\n",
      "    17. 'កង្វះ' (len=5)\n",
      "    18. 'កង្វះខាត' (len=8)\n",
      "    19. 'កញ្ចក់' (len=6)\n",
      "    20. 'កញ្ចប់' (len=6)\n",
      "\n",
      "  Vocabulary length distribution:\n",
      "    2 chars:   220 terms (  4.4%)\n",
      "    3 chars:   651 terms ( 13.0%)\n",
      "    4 chars:   522 terms ( 10.4%)\n",
      "    5 chars:   853 terms ( 17.1%)\n",
      "    6 chars:   747 terms ( 14.9%)\n",
      "    7 chars:   510 terms ( 10.2%)\n",
      "    8 chars:   451 terms (  9.0%)\n",
      "    9 chars:   344 terms (  6.9%)\n",
      "    10 chars:   263 terms (  5.3%)\n",
      "    11 chars:   165 terms (  3.3%)\n",
      "  Top 100 terms breakdown:\n",
      "    - Khmer (2+ chars): 100\n",
      "    - Non-Khmer: 0\n",
      "    - Single characters: 0\n",
      "✓ Saved top terms to: top_terms_No_All_Stopwords.txt\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Loading: No_Auxiliary_Verbs\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ Loaded 58,409 documents\n",
      "  Sample kept tokens: ['សេចក្តីថ្លែងការណ៍', 'ពាក់ព័ន្ធ', 'ព័ត៌មាន', 'នេះ', 'ឲ្យដឹង', 'ថា', 'លោក', 'ទៅ', 'បំពេញ', 'ទស្សនកិច្ច']\n",
      "  Original tokens: 1,636,650\n",
      "  After cleaning: 1,624,743\n",
      "  Removed: 0.7% (non-Khmer content)\n",
      "  Documents retained: 58,398/58,409\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "TOKEN QUALITY DIAGNOSTIC: No_Auxiliary_Verbs\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Token length distribution (first 100 docs):\n",
      "  1 character:       0 tokens (  0.0%)\n",
      "  2 characters:    314 tokens ( 11.8%)\n",
      "  3+ characters:  2,354 tokens ( 88.2%)\n",
      "\n",
      "  Sample of 3+ character tokens (will be kept):\n",
      "    យល់, ក្រហម, ព្រឹក, ចាត់ទុក, ក្តី, កាបាន, ភ្លៀង, ប្រាសចាក, ផឹក, ណាស់, មើល, ខាងក្រោម, ទំនាក់ទំនង, បញ្ឆោត, មធ្យម\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Calculating TF-IDF: No_Auxiliary_Verbs\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  Documents after token filtering: 58,398\n",
      "✓ TF-IDF matrix shape: (58398, 5000)\n",
      "✓ Vocabulary size: 5,000 features\n",
      "\n",
      "  Sample vocabulary (first 20):\n",
      "     1. 'កកាយ' (len=4)\n",
      "     2. 'កកើត' (len=4)\n",
      "     3. 'កក់' (len=3)\n",
      "     4. 'កក់ក្តៅ' (len=7)\n",
      "     5. 'កក្កដា' (len=6)\n",
      "     6. 'កង' (len=2)\n",
      "     7. 'កងកម្លាំង' (len=9)\n",
      "     8. 'កងកម្លាំងប្រដាប់អាវុធ' (len=21)\n",
      "     9. 'កងទ័ព' (len=5)\n",
      "    10. 'កងពល' (len=4)\n",
      "    11. 'កងយោធពល' (len=7)\n",
      "    12. 'កងរាជអាវុធហត្ថ' (len=14)\n",
      "    13. 'កង់' (len=3)\n",
      "    14. 'កង្វល់' (len=6)\n",
      "    15. 'កង្វះ' (len=5)\n",
      "    16. 'កញ្ចក់' (len=6)\n",
      "    17. 'កញ្ចប់' (len=6)\n",
      "    18. 'កញ្ញា' (len=5)\n",
      "    19. 'កឋិន' (len=4)\n",
      "    20. 'កណ្ដាល' (len=6)\n",
      "\n",
      "  Vocabulary length distribution:\n",
      "    2 chars:   228 terms (  4.6%)\n",
      "    3 chars:   658 terms ( 13.2%)\n",
      "    4 chars:   522 terms ( 10.4%)\n",
      "    5 chars:   864 terms ( 17.3%)\n",
      "    6 chars:   750 terms ( 15.0%)\n",
      "    7 chars:   515 terms ( 10.3%)\n",
      "    8 chars:   444 terms (  8.9%)\n",
      "    9 chars:   343 terms (  6.9%)\n",
      "    10 chars:   259 terms (  5.2%)\n",
      "    11 chars:   160 terms (  3.2%)\n",
      "  Top 100 terms breakdown:\n",
      "    - Khmer (2+ chars): 100\n",
      "    - Non-Khmer: 0\n",
      "    - Single characters: 0\n",
      "✓ Saved top terms to: top_terms_No_Auxiliary_Verbs.txt\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Loading: No_Conjunctions\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ Loaded 58,409 documents\n",
      "  Sample kept tokens: ['សេចក្តីថ្លែងការណ៍', 'ពាក់ព័ន្ធ', 'នឹង', 'ព័ត៌មាន', 'នេះ', 'បាន', 'ឲ្យដឹង', 'ថា', 'លោក', 'នឹង']\n",
      "  Original tokens: 1,597,449\n",
      "  After cleaning: 1,585,542\n",
      "  Removed: 0.7% (non-Khmer content)\n",
      "  Documents retained: 58,398/58,409\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "TOKEN QUALITY DIAGNOSTIC: No_Conjunctions\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Token length distribution (first 100 docs):\n",
      "  1 character:       0 tokens (  0.0%)\n",
      "  2 characters:    282 tokens ( 10.8%)\n",
      "  3+ characters:  2,329 tokens ( 89.2%)\n",
      "\n",
      "  Sample of 3+ character tokens (will be kept):\n",
      "    យល់, ក្រហម, ព្រឹក, ចាត់ទុក, ក្តី, កាបាន, ភ្លៀង, ប្រាសចាក, ផឹក, ណាស់, គួរតែ, មើល, ខាងក្រោម, ទំនាក់ទំនង, បញ្ឆោត\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Calculating TF-IDF: No_Conjunctions\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  Documents after token filtering: 58,398\n",
      "✓ TF-IDF matrix shape: (58398, 5000)\n",
      "✓ Vocabulary size: 5,000 features\n",
      "\n",
      "  Sample vocabulary (first 20):\n",
      "     1. 'កកាយ' (len=4)\n",
      "     2. 'កកើត' (len=4)\n",
      "     3. 'កក់' (len=3)\n",
      "     4. 'កក់ក្តៅ' (len=7)\n",
      "     5. 'កក្កដា' (len=6)\n",
      "     6. 'កង' (len=2)\n",
      "     7. 'កងកម្លាំង' (len=9)\n",
      "     8. 'កងកម្លាំងប្រដាប់អាវុធ' (len=21)\n",
      "     9. 'កងទ័ព' (len=5)\n",
      "    10. 'កងពល' (len=4)\n",
      "    11. 'កងយោធពល' (len=7)\n",
      "    12. 'កងរាជអាវុធហត្ថ' (len=14)\n",
      "    13. 'កង់' (len=3)\n",
      "    14. 'កង្វល់' (len=6)\n",
      "    15. 'កង្វះ' (len=5)\n",
      "    16. 'កង្វះខាត' (len=8)\n",
      "    17. 'កញ្ចក់' (len=6)\n",
      "    18. 'កញ្ចប់' (len=6)\n",
      "    19. 'កញ្ញា' (len=5)\n",
      "    20. 'កឋិន' (len=4)\n",
      "\n",
      "  Vocabulary length distribution:\n",
      "    2 chars:   225 terms (  4.5%)\n",
      "    3 chars:   665 terms ( 13.3%)\n",
      "    4 chars:   523 terms ( 10.5%)\n",
      "    5 chars:   862 terms ( 17.2%)\n",
      "    6 chars:   745 terms ( 14.9%)\n",
      "    7 chars:   511 terms ( 10.2%)\n",
      "    8 chars:   447 terms (  8.9%)\n",
      "    9 chars:   343 terms (  6.9%)\n",
      "    10 chars:   258 terms (  5.2%)\n",
      "    11 chars:   162 terms (  3.2%)\n",
      "  Top 100 terms breakdown:\n",
      "    - Khmer (2+ chars): 100\n",
      "    - Non-Khmer: 0\n",
      "    - Single characters: 0\n",
      "✓ Saved top terms to: top_terms_No_Conjunctions.txt\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Loading: No_Determiners\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ Loaded 58,409 documents\n",
      "  Sample kept tokens: ['សេចក្តីថ្លែងការណ៍', 'ពាក់ព័ន្ធ', 'នឹង', 'ព័ត៌មាន', 'បាន', 'ឲ្យដឹង', 'ថា', 'លោក', 'នឹង', 'ទៅ']\n",
      "  Original tokens: 1,658,928\n",
      "  After cleaning: 1,647,021\n",
      "  Removed: 0.7% (non-Khmer content)\n",
      "  Documents retained: 58,398/58,409\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "TOKEN QUALITY DIAGNOSTIC: No_Determiners\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Token length distribution (first 100 docs):\n",
      "  1 character:       0 tokens (  0.0%)\n",
      "  2 characters:    314 tokens ( 11.6%)\n",
      "  3+ characters:  2,395 tokens ( 88.4%)\n",
      "\n",
      "  Sample of 3+ character tokens (will be kept):\n",
      "    យល់, ក្រហម, ព្រឹក, ចាត់ទុក, ក្តី, កាបាន, ភ្លៀង, ប្រាសចាក, ផឹក, ណាស់, គួរតែ, មើល, ខាងក្រោម, ទំនាក់ទំនង, បញ្ឆោត\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Calculating TF-IDF: No_Determiners\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  Documents after token filtering: 58,398\n",
      "✓ TF-IDF matrix shape: (58398, 5000)\n",
      "✓ Vocabulary size: 5,000 features\n",
      "\n",
      "  Sample vocabulary (first 20):\n",
      "     1. 'កកាយ' (len=4)\n",
      "     2. 'កកើត' (len=4)\n",
      "     3. 'កក់' (len=3)\n",
      "     4. 'កក់ក្តៅ' (len=7)\n",
      "     5. 'កក្កដា' (len=6)\n",
      "     6. 'កង' (len=2)\n",
      "     7. 'កងកម្លាំង' (len=9)\n",
      "     8. 'កងកម្លាំងប្រដាប់អាវុធ' (len=21)\n",
      "     9. 'កងទ័ព' (len=5)\n",
      "    10. 'កងពល' (len=4)\n",
      "    11. 'កងយោធពល' (len=7)\n",
      "    12. 'កងរាជអាវុធហត្ថ' (len=14)\n",
      "    13. 'កង់' (len=3)\n",
      "    14. 'កង្វល់' (len=6)\n",
      "    15. 'កង្វះ' (len=5)\n",
      "    16. 'កង្វះខាត' (len=8)\n",
      "    17. 'កញ្ចក់' (len=6)\n",
      "    18. 'កញ្ចប់' (len=6)\n",
      "    19. 'កញ្ញា' (len=5)\n",
      "    20. 'កឋិន' (len=4)\n",
      "\n",
      "  Vocabulary length distribution:\n",
      "    2 chars:   227 terms (  4.5%)\n",
      "    3 chars:   663 terms ( 13.3%)\n",
      "    4 chars:   519 terms ( 10.4%)\n",
      "    5 chars:   861 terms ( 17.2%)\n",
      "    6 chars:   751 terms ( 15.0%)\n",
      "    7 chars:   513 terms ( 10.3%)\n",
      "    8 chars:   446 terms (  8.9%)\n",
      "    9 chars:   344 terms (  6.9%)\n",
      "    10 chars:   259 terms (  5.2%)\n",
      "    11 chars:   159 terms (  3.2%)\n",
      "  Top 100 terms breakdown:\n",
      "    - Khmer (2+ chars): 100\n",
      "    - Non-Khmer: 0\n",
      "    - Single characters: 0\n",
      "✓ Saved top terms to: top_terms_No_Determiners.txt\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Loading: No_Function_Nouns\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ Loaded 58,409 documents\n",
      "  Sample kept tokens: ['សេចក្តីថ្លែងការណ៍', 'ពាក់ព័ន្ធ', 'នឹង', 'ព័ត៌មាន', 'នេះ', 'បាន', 'ឲ្យដឹង', 'ថា', 'លោក', 'នឹង']\n",
      "  Original tokens: 1,655,447\n",
      "  After cleaning: 1,643,540\n",
      "  Removed: 0.7% (non-Khmer content)\n",
      "  Documents retained: 58,398/58,409\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "TOKEN QUALITY DIAGNOSTIC: No_Function_Nouns\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Token length distribution (first 100 docs):\n",
      "  1 character:       0 tokens (  0.0%)\n",
      "  2 characters:    314 tokens ( 11.6%)\n",
      "  3+ characters:  2,393 tokens ( 88.4%)\n",
      "\n",
      "  Sample of 3+ character tokens (will be kept):\n",
      "    យល់, ក្រហម, ព្រឹក, ចាត់ទុក, ក្តី, កាបាន, ភ្លៀង, ប្រាសចាក, ផឹក, ណាស់, គួរតែ, មើល, ខាងក្រោម, ទំនាក់ទំនង, បញ្ឆោត\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Calculating TF-IDF: No_Function_Nouns\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  Documents after token filtering: 58,398\n",
      "✓ TF-IDF matrix shape: (58398, 5000)\n",
      "✓ Vocabulary size: 5,000 features\n",
      "\n",
      "  Sample vocabulary (first 20):\n",
      "     1. 'កកាយ' (len=4)\n",
      "     2. 'កកើត' (len=4)\n",
      "     3. 'កក់' (len=3)\n",
      "     4. 'កក់ក្តៅ' (len=7)\n",
      "     5. 'កក្កដា' (len=6)\n",
      "     6. 'កខ្វក់' (len=6)\n",
      "     7. 'កង' (len=2)\n",
      "     8. 'កងកម្លាំង' (len=9)\n",
      "     9. 'កងកម្លាំងប្រដាប់អាវុធ' (len=21)\n",
      "    10. 'កងទ័ព' (len=5)\n",
      "    11. 'កងពល' (len=4)\n",
      "    12. 'កងយោធពល' (len=7)\n",
      "    13. 'កងរាជអាវុធហត្ថ' (len=14)\n",
      "    14. 'កង់' (len=3)\n",
      "    15. 'កង្វល់' (len=6)\n",
      "    16. 'កង្វះ' (len=5)\n",
      "    17. 'កង្វះខាត' (len=8)\n",
      "    18. 'កញ្ចក់' (len=6)\n",
      "    19. 'កញ្ចប់' (len=6)\n",
      "    20. 'កញ្ញា' (len=5)\n",
      "\n",
      "  Vocabulary length distribution:\n",
      "    2 chars:   228 terms (  4.6%)\n",
      "    3 chars:   661 terms ( 13.2%)\n",
      "    4 chars:   522 terms ( 10.4%)\n",
      "    5 chars:   861 terms ( 17.2%)\n",
      "    6 chars:   748 terms ( 15.0%)\n",
      "    7 chars:   516 terms ( 10.3%)\n",
      "    8 chars:   446 terms (  8.9%)\n",
      "    9 chars:   342 terms (  6.8%)\n",
      "    10 chars:   256 terms (  5.1%)\n",
      "    11 chars:   161 terms (  3.2%)\n",
      "  Top 100 terms breakdown:\n",
      "    - Khmer (2+ chars): 100\n",
      "    - Non-Khmer: 0\n",
      "    - Single characters: 0\n",
      "✓ Saved top terms to: top_terms_No_Function_Nouns.txt\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Loading: No_Numbers\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ Loaded 58,409 documents\n",
      "  Sample kept tokens: ['សេចក្តីថ្លែងការណ៍', 'ពាក់ព័ន្ធ', 'នឹង', 'ព័ត៌មាន', 'នេះ', 'បាន', 'ឲ្យដឹង', 'ថា', 'លោក', 'នឹង']\n",
      "  Original tokens: 1,685,022\n",
      "  After cleaning: 1,673,115\n",
      "  Removed: 0.7% (non-Khmer content)\n",
      "  Documents retained: 58,398/58,409\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "TOKEN QUALITY DIAGNOSTIC: No_Numbers\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Token length distribution (first 100 docs):\n",
      "  1 character:       0 tokens (  0.0%)\n",
      "  2 characters:    314 tokens ( 11.4%)\n",
      "  3+ characters:  2,430 tokens ( 88.6%)\n",
      "\n",
      "  Sample of 3+ character tokens (will be kept):\n",
      "    យល់, ក្រហម, ព្រឹក, ចាត់ទុក, ក្តី, កាបាន, ភ្លៀង, ប្រាសចាក, ផឹក, ណាស់, គួរតែ, មើល, ខាងក្រោម, ទំនាក់ទំនង, បញ្ឆោត\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Calculating TF-IDF: No_Numbers\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  Documents after token filtering: 58,398\n",
      "✓ TF-IDF matrix shape: (58398, 5000)\n",
      "✓ Vocabulary size: 5,000 features\n",
      "\n",
      "  Sample vocabulary (first 20):\n",
      "     1. 'កកើត' (len=4)\n",
      "     2. 'កក់' (len=3)\n",
      "     3. 'កក់ក្តៅ' (len=7)\n",
      "     4. 'កក្កដា' (len=6)\n",
      "     5. 'កង' (len=2)\n",
      "     6. 'កងកម្លាំង' (len=9)\n",
      "     7. 'កងកម្លាំងប្រដាប់អាវុធ' (len=21)\n",
      "     8. 'កងទ័ព' (len=5)\n",
      "     9. 'កងពល' (len=4)\n",
      "    10. 'កងយោធពល' (len=7)\n",
      "    11. 'កងរាជអាវុធហត្ថ' (len=14)\n",
      "    12. 'កង់' (len=3)\n",
      "    13. 'កង្វល់' (len=6)\n",
      "    14. 'កង្វះ' (len=5)\n",
      "    15. 'កញ្ចក់' (len=6)\n",
      "    16. 'កញ្ចប់' (len=6)\n",
      "    17. 'កញ្ញា' (len=5)\n",
      "    18. 'កឋិន' (len=4)\n",
      "    19. 'កណ្ដាល' (len=6)\n",
      "    20. 'កណ្តាល' (len=6)\n",
      "\n",
      "  Vocabulary length distribution:\n",
      "    2 chars:   227 terms (  4.5%)\n",
      "    3 chars:   660 terms ( 13.2%)\n",
      "    4 chars:   519 terms ( 10.4%)\n",
      "    5 chars:   865 terms ( 17.3%)\n",
      "    6 chars:   750 terms ( 15.0%)\n",
      "    7 chars:   515 terms ( 10.3%)\n",
      "    8 chars:   444 terms (  8.9%)\n",
      "    9 chars:   344 terms (  6.9%)\n",
      "    10 chars:   258 terms (  5.2%)\n",
      "    11 chars:   161 terms (  3.2%)\n",
      "  Top 100 terms breakdown:\n",
      "    - Khmer (2+ chars): 100\n",
      "    - Non-Khmer: 0\n",
      "    - Single characters: 0\n",
      "✓ Saved top terms to: top_terms_No_Numbers.txt\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Loading: No_Particles\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ Loaded 58,409 documents\n",
      "  Sample kept tokens: ['សេចក្តីថ្លែងការណ៍', 'ពាក់ព័ន្ធ', 'នឹង', 'ព័ត៌មាន', 'នេះ', 'បាន', 'ឲ្យដឹង', 'ថា', 'លោក', 'នឹង']\n",
      "  Original tokens: 1,660,710\n",
      "  After cleaning: 1,649,330\n",
      "  Removed: 0.7% (non-Khmer content)\n",
      "  Documents retained: 58,398/58,409\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "TOKEN QUALITY DIAGNOSTIC: No_Particles\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Token length distribution (first 100 docs):\n",
      "  1 character:       0 tokens (  0.0%)\n",
      "  2 characters:    298 tokens ( 11.0%)\n",
      "  3+ characters:  2,412 tokens ( 89.0%)\n",
      "\n",
      "  Sample of 3+ character tokens (will be kept):\n",
      "    យល់, ក្រហម, ព្រឹក, ចាត់ទុក, ក្តី, កាបាន, ភ្លៀង, ប្រាសចាក, ផឹក, គួរតែ, មើល, ខាងក្រោម, ទំនាក់ទំនង, បញ្ឆោត, មធ្យម\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Calculating TF-IDF: No_Particles\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  Documents after token filtering: 58,398\n",
      "✓ TF-IDF matrix shape: (58398, 5000)\n",
      "✓ Vocabulary size: 5,000 features\n",
      "\n",
      "  Sample vocabulary (first 20):\n",
      "     1. 'កកាយ' (len=4)\n",
      "     2. 'កកើត' (len=4)\n",
      "     3. 'កក់' (len=3)\n",
      "     4. 'កក់ក្តៅ' (len=7)\n",
      "     5. 'កក្កដា' (len=6)\n",
      "     6. 'កង' (len=2)\n",
      "     7. 'កងកម្លាំង' (len=9)\n",
      "     8. 'កងកម្លាំងប្រដាប់អាវុធ' (len=21)\n",
      "     9. 'កងទ័ព' (len=5)\n",
      "    10. 'កងពល' (len=4)\n",
      "    11. 'កងយោធពល' (len=7)\n",
      "    12. 'កងរាជអាវុធហត្ថ' (len=14)\n",
      "    13. 'កង់' (len=3)\n",
      "    14. 'កង្វល់' (len=6)\n",
      "    15. 'កង្វះ' (len=5)\n",
      "    16. 'កង្វះខាត' (len=8)\n",
      "    17. 'កញ្ចក់' (len=6)\n",
      "    18. 'កញ្ចប់' (len=6)\n",
      "    19. 'កញ្ញា' (len=5)\n",
      "    20. 'កឋិន' (len=4)\n",
      "\n",
      "  Vocabulary length distribution:\n",
      "    2 chars:   225 terms (  4.5%)\n",
      "    3 chars:   659 terms ( 13.2%)\n",
      "    4 chars:   519 terms ( 10.4%)\n",
      "    5 chars:   865 terms ( 17.3%)\n",
      "    6 chars:   753 terms ( 15.1%)\n",
      "    7 chars:   515 terms ( 10.3%)\n",
      "    8 chars:   444 terms (  8.9%)\n",
      "    9 chars:   344 terms (  6.9%)\n",
      "    10 chars:   259 terms (  5.2%)\n",
      "    11 chars:   160 terms (  3.2%)\n",
      "  Top 100 terms breakdown:\n",
      "    - Khmer (2+ chars): 100\n",
      "    - Non-Khmer: 0\n",
      "    - Single characters: 0\n",
      "✓ Saved top terms to: top_terms_No_Particles.txt\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Loading: No_Politeness\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ Loaded 58,409 documents\n",
      "  Sample kept tokens: ['សេចក្តីថ្លែងការណ៍', 'ពាក់ព័ន្ធ', 'នឹង', 'ព័ត៌មាន', 'នេះ', 'បាន', 'ឲ្យដឹង', 'ថា', 'នឹង', 'ទៅ']\n",
      "  Original tokens: 1,691,024\n",
      "  After cleaning: 1,679,117\n",
      "  Removed: 0.7% (non-Khmer content)\n",
      "  Documents retained: 58,398/58,409\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "TOKEN QUALITY DIAGNOSTIC: No_Politeness\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Token length distribution (first 100 docs):\n",
      "  1 character:       0 tokens (  0.0%)\n",
      "  2 characters:    314 tokens ( 11.4%)\n",
      "  3+ characters:  2,445 tokens ( 88.6%)\n",
      "\n",
      "  Sample of 3+ character tokens (will be kept):\n",
      "    យល់, ក្រហម, ព្រឹក, ចាត់ទុក, ក្តី, កាបាន, ភ្លៀង, ប្រាសចាក, ផឹក, ណាស់, គួរតែ, មើល, ខាងក្រោម, ទំនាក់ទំនង, បញ្ឆោត\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Calculating TF-IDF: No_Politeness\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  Documents after token filtering: 58,398\n",
      "✓ TF-IDF matrix shape: (58398, 5000)\n",
      "✓ Vocabulary size: 5,000 features\n",
      "\n",
      "  Sample vocabulary (first 20):\n",
      "     1. 'កកើត' (len=4)\n",
      "     2. 'កក់' (len=3)\n",
      "     3. 'កក់ក្តៅ' (len=7)\n",
      "     4. 'កក្កដា' (len=6)\n",
      "     5. 'កង' (len=2)\n",
      "     6. 'កងកម្លាំង' (len=9)\n",
      "     7. 'កងកម្លាំងប្រដាប់អាវុធ' (len=21)\n",
      "     8. 'កងទ័ព' (len=5)\n",
      "     9. 'កងពល' (len=4)\n",
      "    10. 'កងយោធពល' (len=7)\n",
      "    11. 'កងរាជអាវុធហត្ថ' (len=14)\n",
      "    12. 'កង់' (len=3)\n",
      "    13. 'កង្វល់' (len=6)\n",
      "    14. 'កង្វះ' (len=5)\n",
      "    15. 'កង្វះខាត' (len=8)\n",
      "    16. 'កញ្ចក់' (len=6)\n",
      "    17. 'កញ្ចប់' (len=6)\n",
      "    18. 'កញ្ញា' (len=5)\n",
      "    19. 'កឋិន' (len=4)\n",
      "    20. 'កណ្ដាល' (len=6)\n",
      "\n",
      "  Vocabulary length distribution:\n",
      "    2 chars:   227 terms (  4.5%)\n",
      "    3 chars:   661 terms ( 13.2%)\n",
      "    4 chars:   520 terms ( 10.4%)\n",
      "    5 chars:   863 terms ( 17.3%)\n",
      "    6 chars:   749 terms ( 15.0%)\n",
      "    7 chars:   516 terms ( 10.3%)\n",
      "    8 chars:   445 terms (  8.9%)\n",
      "    9 chars:   344 terms (  6.9%)\n",
      "    10 chars:   258 terms (  5.2%)\n",
      "    11 chars:   160 terms (  3.2%)\n",
      "  Top 100 terms breakdown:\n",
      "    - Khmer (2+ chars): 100\n",
      "    - Non-Khmer: 0\n",
      "    - Single characters: 0\n",
      "✓ Saved top terms to: top_terms_No_Politeness.txt\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Loading: No_Prepositions\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ Loaded 58,409 documents\n",
      "  Sample kept tokens: ['សេចក្តីថ្លែងការណ៍', 'ពាក់ព័ន្ធ', 'នឹង', 'ព័ត៌មាន', 'នេះ', 'បាន', 'ឲ្យដឹង', 'ថា', 'លោក', 'នឹង']\n",
      "  Original tokens: 1,588,604\n",
      "  After cleaning: 1,576,697\n",
      "  Removed: 0.7% (non-Khmer content)\n",
      "  Documents retained: 58,398/58,409\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "TOKEN QUALITY DIAGNOSTIC: No_Prepositions\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Token length distribution (first 100 docs):\n",
      "  1 character:       0 tokens (  0.0%)\n",
      "  2 characters:    256 tokens (  9.8%)\n",
      "  3+ characters:  2,345 tokens ( 90.2%)\n",
      "\n",
      "  Sample of 3+ character tokens (will be kept):\n",
      "    យល់, ក្រហម, ព្រឹក, ចាត់ទុក, ក្តី, កាបាន, ភ្លៀង, ប្រាសចាក, ផឹក, ណាស់, គួរតែ, មើល, ខាងក្រោម, ទំនាក់ទំនង, បញ្ឆោត\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Calculating TF-IDF: No_Prepositions\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  Documents after token filtering: 58,398\n",
      "✓ TF-IDF matrix shape: (58398, 5000)\n",
      "✓ Vocabulary size: 5,000 features\n",
      "\n",
      "  Sample vocabulary (first 20):\n",
      "     1. 'កកាយ' (len=4)\n",
      "     2. 'កកើត' (len=4)\n",
      "     3. 'កក់' (len=3)\n",
      "     4. 'កក់ក្តៅ' (len=7)\n",
      "     5. 'កក្កដា' (len=6)\n",
      "     6. 'កង' (len=2)\n",
      "     7. 'កងកម្លាំង' (len=9)\n",
      "     8. 'កងកម្លាំងប្រដាប់អាវុធ' (len=21)\n",
      "     9. 'កងទ័ព' (len=5)\n",
      "    10. 'កងពល' (len=4)\n",
      "    11. 'កងយោធពល' (len=7)\n",
      "    12. 'កងរាជអាវុធហត្ថ' (len=14)\n",
      "    13. 'កង់' (len=3)\n",
      "    14. 'កង្វល់' (len=6)\n",
      "    15. 'កង្វះ' (len=5)\n",
      "    16. 'កង្វះខាត' (len=8)\n",
      "    17. 'កញ្ចក់' (len=6)\n",
      "    18. 'កញ្ចប់' (len=6)\n",
      "    19. 'កញ្ញា' (len=5)\n",
      "    20. 'កឋិន' (len=4)\n",
      "\n",
      "  Vocabulary length distribution:\n",
      "    2 chars:   226 terms (  4.5%)\n",
      "    3 chars:   658 terms ( 13.2%)\n",
      "    4 chars:   520 terms ( 10.4%)\n",
      "    5 chars:   861 terms ( 17.2%)\n",
      "    6 chars:   750 terms ( 15.0%)\n",
      "    7 chars:   516 terms ( 10.3%)\n",
      "    8 chars:   447 terms (  8.9%)\n",
      "    9 chars:   344 terms (  6.9%)\n",
      "    10 chars:   259 terms (  5.2%)\n",
      "    11 chars:   160 terms (  3.2%)\n",
      "  Top 100 terms breakdown:\n",
      "    - Khmer (2+ chars): 100\n",
      "    - Non-Khmer: 0\n",
      "    - Single characters: 0\n",
      "✓ Saved top terms to: top_terms_No_Prepositions.txt\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Loading: No_Pronouns\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ Loaded 58,409 documents\n",
      "  Sample kept tokens: ['សេចក្តីថ្លែងការណ៍', 'ពាក់ព័ន្ធ', 'នឹង', 'ព័ត៌មាន', 'នេះ', 'បាន', 'ឲ្យដឹង', 'ថា', 'លោក', 'នឹង']\n",
      "  Original tokens: 1,678,950\n",
      "  After cleaning: 1,667,043\n",
      "  Removed: 0.7% (non-Khmer content)\n",
      "  Documents retained: 58,398/58,409\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "TOKEN QUALITY DIAGNOSTIC: No_Pronouns\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Token length distribution (first 100 docs):\n",
      "  1 character:       0 tokens (  0.0%)\n",
      "  2 characters:    308 tokens ( 11.2%)\n",
      "  3+ characters:  2,434 tokens ( 88.8%)\n",
      "\n",
      "  Sample of 3+ character tokens (will be kept):\n",
      "    យល់, ក្រហម, ព្រឹក, ចាត់ទុក, ក្តី, កាបាន, ភ្លៀង, ប្រាសចាក, ផឹក, ណាស់, គួរតែ, មើល, ខាងក្រោម, ទំនាក់ទំនង, បញ្ឆោត\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Calculating TF-IDF: No_Pronouns\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  Documents after token filtering: 58,398\n",
      "✓ TF-IDF matrix shape: (58398, 5000)\n",
      "✓ Vocabulary size: 5,000 features\n",
      "\n",
      "  Sample vocabulary (first 20):\n",
      "     1. 'កកើត' (len=4)\n",
      "     2. 'កក់' (len=3)\n",
      "     3. 'កក់ក្តៅ' (len=7)\n",
      "     4. 'កក្កដា' (len=6)\n",
      "     5. 'កង' (len=2)\n",
      "     6. 'កងកម្លាំង' (len=9)\n",
      "     7. 'កងកម្លាំងប្រដាប់អាវុធ' (len=21)\n",
      "     8. 'កងទ័ព' (len=5)\n",
      "     9. 'កងពល' (len=4)\n",
      "    10. 'កងយោធពល' (len=7)\n",
      "    11. 'កងរាជអាវុធហត្ថ' (len=14)\n",
      "    12. 'កង់' (len=3)\n",
      "    13. 'កង្វល់' (len=6)\n",
      "    14. 'កង្វះ' (len=5)\n",
      "    15. 'កង្វះខាត' (len=8)\n",
      "    16. 'កញ្ចក់' (len=6)\n",
      "    17. 'កញ្ចប់' (len=6)\n",
      "    18. 'កញ្ញា' (len=5)\n",
      "    19. 'កឋិន' (len=4)\n",
      "    20. 'កណ្ដាល' (len=6)\n",
      "\n",
      "  Vocabulary length distribution:\n",
      "    2 chars:   224 terms (  4.5%)\n",
      "    3 chars:   662 terms ( 13.2%)\n",
      "    4 chars:   518 terms ( 10.4%)\n",
      "    5 chars:   864 terms ( 17.3%)\n",
      "    6 chars:   750 terms ( 15.0%)\n",
      "    7 chars:   516 terms ( 10.3%)\n",
      "    8 chars:   446 terms (  8.9%)\n",
      "    9 chars:   345 terms (  6.9%)\n",
      "    10 chars:   257 terms (  5.1%)\n",
      "    11 chars:   160 terms (  3.2%)\n",
      "  Top 100 terms breakdown:\n",
      "    - Khmer (2+ chars): 100\n",
      "    - Non-Khmer: 0\n",
      "    - Single characters: 0\n",
      "✓ Saved top terms to: top_terms_No_Pronouns.txt\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Loading: No_Questions\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ Loaded 58,409 documents\n",
      "  Sample kept tokens: ['សេចក្តីថ្លែងការណ៍', 'ពាក់ព័ន្ធ', 'នឹង', 'ព័ត៌មាន', 'នេះ', 'បាន', 'ឲ្យដឹង', 'ថា', 'លោក', 'នឹង']\n",
      "  Original tokens: 1,688,185\n",
      "  After cleaning: 1,676,278\n",
      "  Removed: 0.7% (non-Khmer content)\n",
      "  Documents retained: 58,398/58,409\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "TOKEN QUALITY DIAGNOSTIC: No_Questions\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Token length distribution (first 100 docs):\n",
      "  1 character:       0 tokens (  0.0%)\n",
      "  2 characters:    307 tokens ( 11.2%)\n",
      "  3+ characters:  2,435 tokens ( 88.8%)\n",
      "\n",
      "  Sample of 3+ character tokens (will be kept):\n",
      "    យល់, ក្រហម, ព្រឹក, ចាត់ទុក, ក្តី, កាបាន, ភ្លៀង, ប្រាសចាក, ផឹក, ណាស់, គួរតែ, មើល, ខាងក្រោម, ទំនាក់ទំនង, បញ្ឆោត\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Calculating TF-IDF: No_Questions\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  Documents after token filtering: 58,398\n",
      "✓ TF-IDF matrix shape: (58398, 5000)\n",
      "✓ Vocabulary size: 5,000 features\n",
      "\n",
      "  Sample vocabulary (first 20):\n",
      "     1. 'កកាយ' (len=4)\n",
      "     2. 'កកើត' (len=4)\n",
      "     3. 'កក់' (len=3)\n",
      "     4. 'កក់ក្តៅ' (len=7)\n",
      "     5. 'កក្កដា' (len=6)\n",
      "     6. 'កង' (len=2)\n",
      "     7. 'កងកម្លាំង' (len=9)\n",
      "     8. 'កងកម្លាំងប្រដាប់អាវុធ' (len=21)\n",
      "     9. 'កងទ័ព' (len=5)\n",
      "    10. 'កងពល' (len=4)\n",
      "    11. 'កងយោធពល' (len=7)\n",
      "    12. 'កងរាជអាវុធហត្ថ' (len=14)\n",
      "    13. 'កង់' (len=3)\n",
      "    14. 'កង្វល់' (len=6)\n",
      "    15. 'កង្វះ' (len=5)\n",
      "    16. 'កង្វះខាត' (len=8)\n",
      "    17. 'កញ្ចក់' (len=6)\n",
      "    18. 'កញ្ចប់' (len=6)\n",
      "    19. 'កញ្ញា' (len=5)\n",
      "    20. 'កឋិន' (len=4)\n",
      "\n",
      "  Vocabulary length distribution:\n",
      "    2 chars:   225 terms (  4.5%)\n",
      "    3 chars:   663 terms ( 13.3%)\n",
      "    4 chars:   522 terms ( 10.4%)\n",
      "    5 chars:   866 terms ( 17.3%)\n",
      "    6 chars:   747 terms ( 14.9%)\n",
      "    7 chars:   515 terms ( 10.3%)\n",
      "    8 chars:   442 terms (  8.8%)\n",
      "    9 chars:   343 terms (  6.9%)\n",
      "    10 chars:   259 terms (  5.2%)\n",
      "    11 chars:   160 terms (  3.2%)\n",
      "  Top 100 terms breakdown:\n",
      "    - Khmer (2+ chars): 100\n",
      "    - Non-Khmer: 0\n",
      "    - Single characters: 0\n",
      "✓ Saved top terms to: top_terms_No_Questions.txt\n",
      "\n",
      "================================================================================\n",
      " COMPARATIVE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "📊 SUMMARY STATISTICS\n",
      "\n",
      "            Corpus  Documents  Total_Terms  Unique_Terms  Vocab_Size  Avg_Doc_Length  Avg_TF-IDF  Khmer_Top_Terms  Non-Khmer_Top_Terms\n",
      "          Original      58398      1691824         45456        5000       28.970581    0.000841              100                    0\n",
      "  No_All_Stopwords      58394      1196010         45258        5000       20.481728    0.000722              100                    0\n",
      "No_Auxiliary_Verbs      58398      1624743         45438        5000       27.821895    0.000828              100                    0\n",
      "   No_Conjunctions      58398      1585542         45418        5000       27.150622    0.000820              100                    0\n",
      "    No_Determiners      58398      1647021         45441        5000       28.203380    0.000831              100                    0\n",
      " No_Function_Nouns      58398      1643540         45418        5000       28.143772    0.000830              100                    0\n",
      "        No_Numbers      58398      1673115         45445        5000       28.650211    0.000836              100                    0\n",
      "      No_Particles      58398      1649330         45429        5000       28.242919    0.000830              100                    0\n",
      "     No_Politeness      58398      1679117         45452        5000       28.752988    0.000839              100                    0\n",
      "   No_Prepositions      58398      1576697         45427        5000       26.999161    0.000815              100                    0\n",
      "       No_Pronouns      58398      1667043         45444        5000       28.546234    0.000835              100                    0\n",
      "      No_Questions      58398      1676278         45450        5000       28.704373    0.000837              100                    0\n",
      "\n",
      "✓ Summary saved to: tfidf_summary.csv\n",
      "\n",
      "📊 VOCABULARY OVERLAP ANALYSIS\n",
      "✓ Vocabulary overlap saved to: vocabulary_overlap.csv\n",
      "\n",
      "📊 TOP TERMS COMPARISON\n",
      "✓ Top terms comparison saved to: top_terms_comparison.txt\n",
      "\n",
      "📊 IMPACT OF STOPWORD REMOVAL\n",
      "\n",
      " Stopword_Group  Term_Reduction_%  Unique_Term_Reduction_%  Vocab_Reduction_%  Remaining_Terms  Quality_Score\n",
      "  All_Stopwords         29.306476                 0.435586                0.0          1196010          100.0\n",
      "   Prepositions          6.804904                 0.063798                0.0          1576697          100.0\n",
      "   Conjunctions          6.282096                 0.083597                0.0          1585542          100.0\n",
      "Auxiliary_Verbs          3.965011                 0.039599                0.0          1624743          100.0\n",
      " Function_Nouns          2.853961                 0.083597                0.0          1643540          100.0\n",
      "    Determiners          2.648207                 0.032999                0.0          1647021          100.0\n",
      "      Particles          2.511727                 0.059398                0.0          1649330          100.0\n",
      "       Pronouns          1.464750                 0.026399                0.0          1667043          100.0\n",
      "        Numbers          1.105848                 0.024199                0.0          1673115          100.0\n",
      "      Questions          0.918890                 0.013200                0.0          1676278          100.0\n",
      "     Politeness          0.751083                 0.008800                0.0          1679117          100.0\n",
      "\n",
      "✓ Impact analysis saved to: stopword_impact.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "# ============================================================================\n",
    "# ENHANCED TF-IDF ANALYZER WITH QUALITY FILTERING\n",
    "# ============================================================================\n",
    "\n",
    "class KhmerTfidfAnalyzer:\n",
    "    def __init__(self, processed_dir, output_dir):\n",
    "        self.processed_dir = processed_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.khmer_pattern = re.compile(r'[\\u1780-\\u17FF]')\n",
    "        self.results = {}\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    def diagnose_token_quality(self, documents, label):\n",
    "        \"\"\"Diagnose token quality before TF-IDF\"\"\"\n",
    "        print(f\"\\n{'─'*80}\")\n",
    "        print(f\"TOKEN QUALITY DIAGNOSTIC: {label}\")\n",
    "        print(f\"{'─'*80}\")\n",
    "        \n",
    "        # Sample first 100 documents\n",
    "        sample_docs = documents[:100]\n",
    "        all_tokens = []\n",
    "        for doc in sample_docs:\n",
    "            all_tokens.extend(doc.split())\n",
    "        \n",
    "        # Categorize tokens\n",
    "        single_char = [t for t in all_tokens if len(t) == 1]\n",
    "        two_char = [t for t in all_tokens if len(t) == 2]\n",
    "        three_plus = [t for t in all_tokens if len(t) >= 3]\n",
    "        \n",
    "        print(f\"Token length distribution (first 100 docs):\")\n",
    "        print(f\"  1 character:  {len(single_char):6,} tokens ({len(single_char)/len(all_tokens)*100:5.1f}%)\")\n",
    "        print(f\"  2 characters: {len(two_char):6,} tokens ({len(two_char)/len(all_tokens)*100:5.1f}%)\")\n",
    "        print(f\"  3+ characters: {len(three_plus):6,} tokens ({len(three_plus)/len(all_tokens)*100:5.1f}%)\")\n",
    "        \n",
    "        # Show most common single characters\n",
    "        if single_char:\n",
    "            print(f\"\\n  Most common single-char tokens (will be filtered):\")\n",
    "            for token, count in Counter(single_char).most_common(20):\n",
    "                print(f\"    '{token}' → {count:,} times\")\n",
    "        \n",
    "        # Show sample of good tokens\n",
    "        if three_plus:\n",
    "            print(f\"\\n  Sample of 3+ character tokens (will be kept):\")\n",
    "            unique_good = list(set(three_plus))[:15]\n",
    "            print(f\"    {', '.join(unique_good)}\")\n",
    "        \n",
    "        return {\n",
    "            'single_char_count': len(single_char),\n",
    "            'two_char_count': len(two_char),\n",
    "            'three_plus_count': len(three_plus)\n",
    "        }\n",
    "    \n",
    "    def clean_tokens(self, text, keep_khmer_only=True, min_length=2):\n",
    "        \"\"\"Clean text to keep only meaningful tokens\"\"\"\n",
    "        tokens = text.split()\n",
    "        \n",
    "        if keep_khmer_only:\n",
    "            # Filter criteria:\n",
    "            # 1. Must contain Khmer script\n",
    "            # 2. Must be at least min_length characters (default 2)\n",
    "            # 3. Should not be purely non-linguistic\n",
    "            cleaned_tokens = []\n",
    "            \n",
    "            for token in tokens:\n",
    "                # Must contain Khmer\n",
    "                if not self.khmer_pattern.search(token):\n",
    "                    continue\n",
    "                \n",
    "                # Must be long enough (filter out single characters)\n",
    "                if len(token) < min_length:\n",
    "                    continue\n",
    "                \n",
    "                # Check if it's a meaningful Khmer token (not just punctuation/symbols)\n",
    "                # Count actual Khmer characters (exclude combining marks, numbers, etc.)\n",
    "                khmer_chars = len([c for c in token if '\\u1780' <= c <= '\\u17DD'])\n",
    "                \n",
    "                # Must have at least min_length actual Khmer base characters\n",
    "                if khmer_chars < min_length:\n",
    "                    continue\n",
    "                \n",
    "                cleaned_tokens.append(token)\n",
    "            \n",
    "            return ' '.join(cleaned_tokens)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def load_and_clean_corpus(self, filepath, label):\n",
    "        \"\"\"Load corpus and clean it\"\"\"\n",
    "        print(f\"\\n{'─'*80}\")\n",
    "        print(f\"Loading: {label}\")\n",
    "        print(f\"{'─'*80}\")\n",
    "        \n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"❌ File not found: {filepath}\")\n",
    "            return None, None\n",
    "        \n",
    "        # Read documents\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            documents = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "        if not documents:\n",
    "            print(f\"⚠️  Empty file!\")\n",
    "            return None, None\n",
    "        \n",
    "        print(f\"✓ Loaded {len(documents):,} documents\")\n",
    "        \n",
    "        # Clean documents (keep only Khmer, min 2 characters)\n",
    "        cleaned_docs = [self.clean_tokens(doc, keep_khmer_only=True, min_length=2) for doc in documents]\n",
    "        cleaned_docs = [doc for doc in cleaned_docs if doc]  # Remove empty\n",
    "        \n",
    "        # Additional quality check - show sample of removed vs kept tokens\n",
    "        if documents:\n",
    "            sample_original = documents[0].split()[:20]\n",
    "            sample_cleaned = cleaned_docs[0].split()[:20] if cleaned_docs else []\n",
    "            \n",
    "            removed_tokens = [t for t in sample_original if t not in ' '.join(cleaned_docs[0:5]).split()]\n",
    "            \n",
    "            if removed_tokens:\n",
    "                print(f\"  Sample removed tokens: {removed_tokens[:10]}\")\n",
    "            if sample_cleaned:\n",
    "                print(f\"  Sample kept tokens: {sample_cleaned[:10]}\")\n",
    "        \n",
    "        # Calculate cleaning statistics\n",
    "        original_tokens = sum(len(doc.split()) for doc in documents)\n",
    "        cleaned_tokens = sum(len(doc.split()) for doc in cleaned_docs)\n",
    "        removed_pct = ((original_tokens - cleaned_tokens) / original_tokens * 100) if original_tokens > 0 else 0\n",
    "        \n",
    "        print(f\"  Original tokens: {original_tokens:,}\")\n",
    "        print(f\"  After cleaning: {cleaned_tokens:,}\")\n",
    "        print(f\"  Removed: {removed_pct:.1f}% (non-Khmer content)\")\n",
    "        print(f\"  Documents retained: {len(cleaned_docs):,}/{len(documents):,}\")\n",
    "        \n",
    "        if len(cleaned_docs) == 0:\n",
    "            print(f\"❌ No documents left after cleaning!\")\n",
    "            return None, None\n",
    "        \n",
    "        return documents, cleaned_docs\n",
    "    \n",
    "    def calculate_tfidf(self, documents, label, max_features=5000, min_token_length=2):\n",
    "        \"\"\"Calculate TF-IDF with quality checks\"\"\"\n",
    "        print(f\"\\n{'─'*80}\")\n",
    "        print(f\"Calculating TF-IDF: {label}\")\n",
    "        print(f\"{'─'*80}\")\n",
    "        \n",
    "        if not documents or len(documents) == 0:\n",
    "            print(\"❌ No documents to analyze!\")\n",
    "            return None\n",
    "        \n",
    "        # Additional pre-filtering: remove very short tokens from documents\n",
    "        filtered_docs = []\n",
    "        for doc in documents:\n",
    "            tokens = doc.split()\n",
    "            # Keep only tokens with at least min_token_length characters\n",
    "            valid_tokens = [t for t in tokens if len(t) >= min_token_length]\n",
    "            if valid_tokens:\n",
    "                filtered_docs.append(' '.join(valid_tokens))\n",
    "        \n",
    "        if not filtered_docs:\n",
    "            print(\"❌ No valid tokens after filtering!\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"  Documents after token filtering: {len(filtered_docs):,}\")\n",
    "        \n",
    "        # Create vectorizer with custom token pattern\n",
    "        # This pattern ensures we only get tokens that are actual words (2+ chars)\n",
    "        # CRITICAL: Use custom analyzer to prevent n-gram extraction\n",
    "        def khmer_tokenizer(text):\n",
    "            \"\"\"Custom tokenizer that only splits on whitespace - no n-grams\"\"\"\n",
    "            return text.split()\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            min_df=2,  # Term must appear in at least 2 documents\n",
    "            max_df=0.95,  # Ignore terms in >95% of documents\n",
    "            tokenizer=khmer_tokenizer,  # Use our custom tokenizer\n",
    "            lowercase=False,  # Don't lowercase (preserve Khmer)\n",
    "            token_pattern=None  # Disable default pattern\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Fit and transform\n",
    "            tfidf_matrix = vectorizer.fit_transform(filtered_docs)\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            \n",
    "            # Additional filtering: remove single-character terms that might have slipped through\n",
    "            valid_features = []\n",
    "            valid_indices = []\n",
    "            for idx, term in enumerate(feature_names):\n",
    "                if len(term) >= min_token_length:\n",
    "                    valid_features.append(term)\n",
    "                    valid_indices.append(idx)\n",
    "            \n",
    "            if len(valid_indices) < len(feature_names):\n",
    "                print(f\"  Filtered out {len(feature_names) - len(valid_indices)} single-character terms\")\n",
    "                # Rebuild matrix with only valid features\n",
    "                tfidf_matrix = tfidf_matrix[:, valid_indices]\n",
    "                feature_names = np.array(valid_features)\n",
    "            \n",
    "            print(f\"✓ TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "            print(f\"✓ Vocabulary size: {len(feature_names):,} features\")\n",
    "            \n",
    "            # Show sample of vocabulary with length analysis\n",
    "            print(f\"\\n  Sample vocabulary (first 20):\")\n",
    "            for i, term in enumerate(feature_names[:20], 1):\n",
    "                print(f\"    {i:2d}. '{term}' (len={len(term)})\")\n",
    "            \n",
    "            # Analyze vocabulary by length\n",
    "            vocab_lengths = Counter([len(term) for term in feature_names])\n",
    "            print(f\"\\n  Vocabulary length distribution:\")\n",
    "            for length in sorted(vocab_lengths.keys())[:10]:\n",
    "                count = vocab_lengths[length]\n",
    "                pct = count / len(feature_names) * 100\n",
    "                print(f\"    {length} chars: {count:5,} terms ({pct:5.1f}%)\")\n",
    "            \n",
    "            # Calculate statistics\n",
    "            avg_tfidf_scores = np.asarray(tfidf_matrix.mean(axis=0)).flatten()\n",
    "            \n",
    "            # Get top terms\n",
    "            top_indices = avg_tfidf_scores.argsort()[::-1][:100]\n",
    "            top_terms = [(feature_names[i], avg_tfidf_scores[i]) for i in top_indices]\n",
    "            \n",
    "            # Verify top terms are meaningful (not single chars)\n",
    "            khmer_top_terms = [(term, score) for term, score in top_terms \n",
    "                              if self.khmer_pattern.search(term) and len(term) >= 2]\n",
    "            non_khmer_top_terms = [(term, score) for term, score in top_terms \n",
    "                                  if not self.khmer_pattern.search(term)]\n",
    "            single_char_terms = [(term, score) for term, score in top_terms\n",
    "                                if len(term) < 2]\n",
    "            \n",
    "            print(f\"  Top 100 terms breakdown:\")\n",
    "            print(f\"    - Khmer (2+ chars): {len(khmer_top_terms)}\")\n",
    "            print(f\"    - Non-Khmer: {len(non_khmer_top_terms)}\")\n",
    "            print(f\"    - Single characters: {len(single_char_terms)}\")\n",
    "            \n",
    "            if single_char_terms:\n",
    "                print(f\"  ⚠️  Warning: {len(single_char_terms)} single-character terms in top 100\")\n",
    "                print(f\"     These may indicate segmentation issues: {[t[0] for t in single_char_terms[:5]]}\")\n",
    "            \n",
    "            if non_khmer_top_terms:\n",
    "                print(f\"  ⚠️  Non-Khmer terms in top 100: {[t[0] for t in non_khmer_top_terms[:5]]}\")\n",
    "            \n",
    "            # Calculate term frequencies (from filtered docs)\n",
    "            all_terms = []\n",
    "            for doc in filtered_docs:\n",
    "                all_terms.extend(doc.split())\n",
    "            term_freq = Counter(all_terms)\n",
    "            \n",
    "            # Document statistics\n",
    "            doc_lengths = [len(doc.split()) for doc in filtered_docs]\n",
    "            \n",
    "            result = {\n",
    "                'label': label,\n",
    "                'vectorizer': vectorizer,\n",
    "                'tfidf_matrix': tfidf_matrix,\n",
    "                'feature_names': feature_names,\n",
    "                'avg_tfidf_scores': avg_tfidf_scores,\n",
    "                'top_terms': top_terms,\n",
    "                'khmer_top_terms': khmer_top_terms,\n",
    "                'non_khmer_top_terms': non_khmer_top_terms,\n",
    "                'single_char_terms': single_char_terms if 'single_char_terms' in locals() else [],\n",
    "                'term_freq': term_freq,\n",
    "                'doc_lengths': doc_lengths,\n",
    "                'total_docs': len(filtered_docs),\n",
    "                'total_terms': sum(doc_lengths),\n",
    "                'unique_terms': len(term_freq),\n",
    "                'vocab_size': len(feature_names),\n",
    "                'avg_doc_length': np.mean(doc_lengths),\n",
    "                'avg_tfidf': np.mean(avg_tfidf_scores),\n",
    "                'std_tfidf': np.std(avg_tfidf_scores)\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error calculating TF-IDF: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def save_top_terms(self, result):\n",
    "        \"\"\"Save top terms to file\"\"\"\n",
    "        label = result['label']\n",
    "        output_file = os.path.join(self.output_dir, f\"top_terms_{label}.txt\")\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"TOP 100 TERMS BY TF-IDF - {label}\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(f\"Documents: {result['total_docs']:,}\\n\")\n",
    "            f.write(f\"Total terms: {result['total_terms']:,}\\n\")\n",
    "            f.write(f\"Unique terms: {result['unique_terms']:,}\\n\")\n",
    "            f.write(f\"Vocabulary size: {result['vocab_size']:,}\\n\")\n",
    "            f.write(f\"Avg TF-IDF score: {result['avg_tfidf']:.6f}\\n\\n\")\n",
    "            \n",
    "            # Separate Khmer and non-Khmer terms\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(\"KHMER TERMS (Pure Khmer script only)\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(f\"{'Rank':<6} | {'Term':<20} | {'Avg TF-IDF Score':<18}\\n\")\n",
    "            f.write(\"-\"*80 + \"\\n\")\n",
    "            \n",
    "            for i, (term, score) in enumerate(result['khmer_top_terms'][:100], 1):\n",
    "                f.write(f\"{i:<6} | {term:<20} | {score:<18.6f}\\n\")\n",
    "            \n",
    "            if result['non_khmer_top_terms']:\n",
    "                f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "                f.write(\"NON-KHMER TERMS (Suspicious - may indicate data quality issues)\\n\")\n",
    "                f.write(\"=\"*80 + \"\\n\")\n",
    "                f.write(f\"{'Rank':<6} | {'Term':<20} | {'Avg TF-IDF Score':<18}\\n\")\n",
    "                f.write(\"-\"*80 + \"\\n\")\n",
    "                \n",
    "                for i, (term, score) in enumerate(result['non_khmer_top_terms'][:50], 1):\n",
    "                    f.write(f\"{i:<6} | {term:<20} | {score:<18.6f}\\n\")\n",
    "        \n",
    "        print(f\"✓ Saved top terms to: {os.path.basename(output_file)}\")\n",
    "    \n",
    "    def analyze_all_files(self, files_to_analyze):\n",
    "        \"\"\"Analyze all corpus files\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\" COMPREHENSIVE TF-IDF ANALYSIS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for label, filename in files_to_analyze:\n",
    "            filepath = os.path.join(self.processed_dir, filename)\n",
    "            \n",
    "            # Load and clean\n",
    "            original_docs, cleaned_docs = self.load_and_clean_corpus(filepath, label)\n",
    "            \n",
    "            if cleaned_docs is None:\n",
    "                continue\n",
    "            \n",
    "            # Diagnose token quality\n",
    "            self.diagnose_token_quality(cleaned_docs, label)\n",
    "            \n",
    "            # Calculate TF-IDF\n",
    "            result = self.calculate_tfidf(cleaned_docs, label)\n",
    "            \n",
    "            if result is None:\n",
    "                continue\n",
    "            \n",
    "            # Save results\n",
    "            self.results[label] = result\n",
    "            self.save_top_terms(result)\n",
    "            \n",
    "            # Save TF-IDF matrix (sparse format)\n",
    "            matrix_file = os.path.join(self.output_dir, f\"tfidf_matrix_{label}.npz\")\n",
    "            np.savez(matrix_file,\n",
    "                    data=result['tfidf_matrix'].data,\n",
    "                    indices=result['tfidf_matrix'].indices,\n",
    "                    indptr=result['tfidf_matrix'].indptr,\n",
    "                    shape=result['tfidf_matrix'].shape)\n",
    "            \n",
    "            # Save feature names\n",
    "            features_file = os.path.join(self.output_dir, f\"features_{label}.txt\")\n",
    "            with open(features_file, 'w', encoding='utf-8') as f:\n",
    "                for feature in result['feature_names']:\n",
    "                    f.write(feature + '\\n')\n",
    "    \n",
    "    def generate_comparative_analysis(self):\n",
    "        \"\"\"Generate comparative analysis across all corpora\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\" COMPARATIVE ANALYSIS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        if len(self.results) < 2:\n",
    "            print(\"⚠️  Need at least 2 corpora for comparison\")\n",
    "            return\n",
    "        \n",
    "        # 1. Summary statistics\n",
    "        print(\"\\n📊 SUMMARY STATISTICS\")\n",
    "        summary_data = []\n",
    "        for label, result in self.results.items():\n",
    "            summary_data.append({\n",
    "                'Corpus': label,\n",
    "                'Documents': result['total_docs'],\n",
    "                'Total_Terms': result['total_terms'],\n",
    "                'Unique_Terms': result['unique_terms'],\n",
    "                'Vocab_Size': result['vocab_size'],\n",
    "                'Avg_Doc_Length': result['avg_doc_length'],\n",
    "                'Avg_TF-IDF': result['avg_tfidf'],\n",
    "                'Khmer_Top_Terms': len(result['khmer_top_terms']),\n",
    "                'Non-Khmer_Top_Terms': len(result['non_khmer_top_terms'])\n",
    "            })\n",
    "        \n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        print(\"\\n\" + summary_df.to_string(index=False))\n",
    "        \n",
    "        # Save summary\n",
    "        summary_file = os.path.join(self.output_dir, 'tfidf_summary.csv')\n",
    "        summary_df.to_csv(summary_file, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\n✓ Summary saved to: {os.path.basename(summary_file)}\")\n",
    "        \n",
    "        # 2. Vocabulary overlap analysis\n",
    "        print(\"\\n📊 VOCABULARY OVERLAP ANALYSIS\")\n",
    "        vocabularies = {label: set(result['feature_names']) \n",
    "                       for label, result in self.results.items()}\n",
    "        \n",
    "        labels = list(vocabularies.keys())\n",
    "        overlap_matrix = np.zeros((len(labels), len(labels)))\n",
    "        \n",
    "        for i, label1 in enumerate(labels):\n",
    "            for j, label2 in enumerate(labels):\n",
    "                if i == j:\n",
    "                    overlap_matrix[i, j] = 1.0\n",
    "                else:\n",
    "                    intersection = len(vocabularies[label1] & vocabularies[label2])\n",
    "                    union = len(vocabularies[label1] | vocabularies[label2])\n",
    "                    overlap_matrix[i, j] = intersection / union if union > 0 else 0\n",
    "        \n",
    "        overlap_df = pd.DataFrame(overlap_matrix, index=labels, columns=labels)\n",
    "        overlap_file = os.path.join(self.output_dir, 'vocabulary_overlap.csv')\n",
    "        overlap_df.to_csv(overlap_file, encoding='utf-8-sig')\n",
    "        print(f\"✓ Vocabulary overlap saved to: {os.path.basename(overlap_file)}\")\n",
    "        \n",
    "        # 3. Top terms comparison\n",
    "        print(\"\\n📊 TOP TERMS COMPARISON\")\n",
    "        comparison_file = os.path.join(self.output_dir, 'top_terms_comparison.txt')\n",
    "        \n",
    "        with open(comparison_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"TOP 20 KHMER TERMS COMPARISON\\n\")\n",
    "            f.write(\"=\"*120 + \"\\n\\n\")\n",
    "            \n",
    "            # Get top 20 Khmer terms from each corpus\n",
    "            top_terms_dict = {}\n",
    "            for label, result in self.results.items():\n",
    "                top_terms_dict[label] = [term for term, _ in result['khmer_top_terms'][:20]]\n",
    "            \n",
    "            # Create comparison table\n",
    "            f.write(f\"{'Rank':<6}\")\n",
    "            for label in top_terms_dict.keys():\n",
    "                f.write(f\" | {label:<25}\")\n",
    "            f.write(\"\\n\" + \"-\"*120 + \"\\n\")\n",
    "            \n",
    "            for i in range(20):\n",
    "                f.write(f\"{i+1:<6}\")\n",
    "                for label in top_terms_dict.keys():\n",
    "                    term = top_terms_dict[label][i] if i < len(top_terms_dict[label]) else \"\"\n",
    "                    f.write(f\" | {term:<25}\")\n",
    "                f.write(\"\\n\")\n",
    "        \n",
    "        print(f\"✓ Top terms comparison saved to: {os.path.basename(comparison_file)}\")\n",
    "        \n",
    "        # 4. Impact analysis (compared to Original)\n",
    "        if 'Original' in self.results:\n",
    "            print(\"\\n📊 IMPACT OF STOPWORD REMOVAL\")\n",
    "            original = self.results['Original']\n",
    "            \n",
    "            impact_data = []\n",
    "            for label, result in self.results.items():\n",
    "                if label == 'Original':\n",
    "                    continue\n",
    "                \n",
    "                term_reduction = (1 - result['total_terms'] / original['total_terms']) * 100\n",
    "                unique_reduction = (1 - result['unique_terms'] / original['unique_terms']) * 100\n",
    "                vocab_reduction = (1 - result['vocab_size'] / original['vocab_size']) * 100\n",
    "                \n",
    "                impact_data.append({\n",
    "                    'Stopword_Group': label.replace('No_', ''),\n",
    "                    'Term_Reduction_%': term_reduction,\n",
    "                    'Unique_Term_Reduction_%': unique_reduction,\n",
    "                    'Vocab_Reduction_%': vocab_reduction,\n",
    "                    'Remaining_Terms': result['total_terms'],\n",
    "                    'Quality_Score': len(result['khmer_top_terms']) / 100 * 100  # % Khmer in top 100\n",
    "                })\n",
    "            \n",
    "            impact_df = pd.DataFrame(impact_data)\n",
    "            impact_df = impact_df.sort_values('Term_Reduction_%', ascending=False)\n",
    "            \n",
    "            print(\"\\n\" + impact_df.to_string(index=False))\n",
    "            \n",
    "            impact_file = os.path.join(self.output_dir, 'stopword_impact.csv')\n",
    "            impact_df.to_csv(impact_file, index=False, encoding='utf-8-sig')\n",
    "            print(f\"\\n✓ Impact analysis saved to: {os.path.basename(impact_file)}\")\n",
    "    \n",
    "    def create_visualizations(self):\n",
    "        \"\"\"Create comprehensive visualizations\"\"\"\n",
    "        print(\"\\n📊 CREATING VISUALIZATIONS\")\n",
    "        \n",
    "        plt.style.use('seaborn-v0_8-darkgrid')\n",
    "        \n",
    "        # Figure 1: Corpus statistics\n",
    "        fig1, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        fig1.suptitle('Corpus Statistics Comparison', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        labels = list(self.results.keys())\n",
    "        total_terms = [self.results[l]['total_terms'] for l in labels]\n",
    "        unique_terms = [self.results[l]['unique_terms'] for l in labels]\n",
    "        vocab_sizes = [self.results[l]['vocab_size'] for l in labels]\n",
    "        avg_lengths = [self.results[l]['avg_doc_length'] for l in labels]\n",
    "        \n",
    "        # Plot 1: Total terms\n",
    "        axes[0, 0].bar(range(len(labels)), total_terms, color='skyblue', edgecolor='black')\n",
    "        axes[0, 0].set_xticks(range(len(labels)))\n",
    "        axes[0, 0].set_xticklabels(labels, rotation=45, ha='right')\n",
    "        axes[0, 0].set_ylabel('Number of Terms', fontsize=12)\n",
    "        axes[0, 0].set_title('Total Terms per Corpus', fontsize=14, fontweight='bold')\n",
    "        axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Unique terms\n",
    "        axes[0, 1].bar(range(len(labels)), unique_terms, color='lightgreen', edgecolor='black')\n",
    "        axes[0, 1].set_xticks(range(len(labels)))\n",
    "        axes[0, 1].set_xticklabels(labels, rotation=45, ha='right')\n",
    "        axes[0, 1].set_ylabel('Number of Unique Terms', fontsize=12)\n",
    "        axes[0, 1].set_title('Unique Terms per Corpus', fontsize=14, fontweight='bold')\n",
    "        axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Vocabulary size\n",
    "        axes[1, 0].bar(range(len(labels)), vocab_sizes, color='salmon', edgecolor='black')\n",
    "        axes[1, 0].set_xticks(range(len(labels)))\n",
    "        axes[1, 0].set_xticklabels(labels, rotation=45, ha='right')\n",
    "        axes[1, 0].set_ylabel('Vocabulary Size', fontsize=12)\n",
    "        axes[1, 0].set_title('TF-IDF Vocabulary Size', fontsize=14, fontweight='bold')\n",
    "        axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Average document length\n",
    "        axes[1, 1].bar(range(len(labels)), avg_lengths, color='gold', edgecolor='black')\n",
    "        axes[1, 1].set_xticks(range(len(labels)))\n",
    "        axes[1, 1].set_xticklabels(labels, rotation=45, ha='right')\n",
    "        axes[1, 1].set_ylabel('Average Terms per Document', fontsize=12)\n",
    "        axes[1, 1].set_title('Average Document Length', fontsize=14, fontweight='bold')\n",
    "        axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        fig1.savefig(os.path.join(self.output_dir, 'corpus_statistics.png'), \n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        print(\"✓ Saved: corpus_statistics.png\")\n",
    "        \n",
    "        # Figure 2: TF-IDF distributions\n",
    "        n_corpora = len(self.results)\n",
    "        n_cols = 4\n",
    "        n_rows = (n_corpora + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig2, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))\n",
    "        fig2.suptitle('TF-IDF Score Distributions', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        if n_rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for idx, (label, result) in enumerate(self.results.items()):\n",
    "            row, col = divmod(idx, n_cols)\n",
    "            ax = axes[row, col]\n",
    "            \n",
    "            scores = result['avg_tfidf_scores']\n",
    "            ax.hist(scores, bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "            ax.axvline(np.mean(scores), color='red', linestyle='--', linewidth=2,\n",
    "                      label=f'Mean: {np.mean(scores):.4f}')\n",
    "            ax.set_xlabel('TF-IDF Score', fontsize=10)\n",
    "            ax.set_ylabel('Frequency', fontsize=10)\n",
    "            ax.set_title(label, fontsize=12, fontweight='bold')\n",
    "            ax.legend()\n",
    "            ax.grid(alpha=0.3)\n",
    "        \n",
    "        # Remove empty subplots\n",
    "        for idx in range(n_corpora, n_rows * n_cols):\n",
    "            row, col = divmod(idx, n_cols)\n",
    "            fig2.delaxes(axes[row, col])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        fig2.savefig(os.path.join(self.output_dir, 'tfidf_distributions.png'),\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        print(\"✓ Saved: tfidf_distributions.png\")\n",
    "        \n",
    "        # Figure 3: Quality scores (Khmer vs Non-Khmer in top 100)\n",
    "        fig3, ax = plt.subplots(figsize=(14, 8))\n",
    "        \n",
    "        khmer_counts = [len(self.results[l]['khmer_top_terms']) for l in labels]\n",
    "        non_khmer_counts = [len(self.results[l]['non_khmer_top_terms']) for l in labels]\n",
    "        \n",
    "        x = np.arange(len(labels))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax.bar(x - width/2, khmer_counts, width, label='Khmer Terms', \n",
    "               color='green', alpha=0.7, edgecolor='black')\n",
    "        ax.bar(x + width/2, non_khmer_counts, width, label='Non-Khmer Terms',\n",
    "               color='red', alpha=0.7, edgecolor='black')\n",
    "        \n",
    "        ax.set_xlabel('Corpus', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Count in Top 100', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('Quality Check: Khmer vs Non-Khmer Terms in Top 100',\n",
    "                    fontsize=14, fontweight='bold')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        ax.axhline(y=80, color='orange', linestyle='--', label='Target: 80+ Khmer terms')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        fig3.savefig(os.path.join(self.output_dir, 'quality_check.png'),\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        print(\"✓ Saved: quality_check.png\")\n",
    "        \n",
    "        plt.close('all')\n",
    "    \n",
    "    def generate_final_report(self):\n",
    "        \"\"\"Generate comprehensive final report\"\"\"\n",
    "        print(\"\\n📄 GENERATING FINAL REPORT\")\n",
    "        \n",
    "        report_file = os.path.join(self.output_dir, 'tfidf_analysis_report.md')\n",
    "        \n",
    "        with open(report_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"# Comprehensive TF-IDF Analysis Report\\n\\n\")\n",
    "            f.write(\"## Executive Summary\\n\\n\")\n",
    "            \n",
    "            total_corpora = len(self.results)\n",
    "            f.write(f\"- **Total corpora analyzed**: {total_corpora}\\n\")\n",
    "            \n",
    "            if 'Original' in self.results:\n",
    "                orig = self.results['Original']\n",
    "                f.write(f\"- **Original corpus**: {orig['total_docs']:,} documents, \"\n",
    "                       f\"{orig['total_terms']:,} terms\\n\")\n",
    "            \n",
    "            # Quality assessment\n",
    "            high_quality = sum(1 for r in self.results.values() \n",
    "                             if len(r['khmer_top_terms']) >= 80)\n",
    "            f.write(f\"- **High quality corpora** (≥80 Khmer terms in top 100): {high_quality}/{total_corpora}\\n\\n\")\n",
    "            \n",
    "            f.write(\"## Key Findings\\n\\n\")\n",
    "            \n",
    "            # Find most effective stopword removal\n",
    "            if 'Original' in self.results:\n",
    "                original = self.results['Original']\n",
    "                reductions = []\n",
    "                for label, result in self.results.items():\n",
    "                    if label != 'Original':\n",
    "                        reduction = (1 - result['total_terms'] / original['total_terms']) * 100\n",
    "                        quality = len(result['khmer_top_terms'])\n",
    "                        reductions.append((label, reduction, quality))\n",
    "                \n",
    "                reductions.sort(key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                f.write(\"### Most Effective Stopword Groups\\n\\n\")\n",
    "                f.write(\"| Rank | Group | Term Reduction | Quality (Khmer/100) |\\n\")\n",
    "                f.write(\"|------|-------|----------------|---------------------|\\n\")\n",
    "                for i, (label, reduction, quality) in enumerate(reductions[:5], 1):\n",
    "                    f.write(f\"| {i} | {label.replace('No_', '')} | {reduction:.1f}% | {quality}/100 |\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            f.write(\"## Quality Assessment\\n\\n\")\n",
    "            for label, result in self.results.items():\n",
    "                khmer_pct = len(result['khmer_top_terms']) / 100 * 100\n",
    "                status = \"✅ Excellent\" if khmer_pct >= 90 else \"✓ Good\" if khmer_pct >= 80 else \"⚠️ Needs review\"\n",
    "                \n",
    "                f.write(f\"### {label}\\n\")\n",
    "                f.write(f\"- **Status**: {status}\\n\")\n",
    "                f.write(f\"- **Khmer terms in top 100**: {len(result['khmer_top_terms'])}/100\\n\")\n",
    "                f.write(f\"- **Total terms**: {result['total_terms']:,}\\n\")\n",
    "                f.write(f\"- **Vocabulary size**: {result['vocab_size']:,}\\n\")\n",
    "                \n",
    "                if result['non_khmer_top_terms']:\n",
    "                    f.write(f\"- **⚠️ Non-Khmer terms detected**: {[t[0] for t in result['non_khmer_top_terms'][:5]]}\\n\")\n",
    "                \n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            f.write(\"## Recommendations\\n\\n\")\n",
    "            f.write(\"1. **For Information Retrieval**: Use corpora with highest Khmer term retention\\n\")\n",
    "            f.write(\"2. **For Efficiency**: Balance term reduction with quality preservation\\n\")\n",
    "            f.write(\"3. **Data Quality**: Review corpora with <80 Khmer terms in top 100\\n\")\n",
    "            f.write(\"4. **Further Analysis**: Examine top_terms_*.txt files for domain relevance\\n\\n\")\n",
    "            \n",
    "            f.write(\"## Files Generated\\n\\n\")\n",
    "            f.write(\"- `tfidf_summary.csv` - Overall statistics\\n\")\n",
    "            f.write(\"- `vocabulary_overlap.csv` - Term overlap between corpora\\n\")\n",
    "            f.write(\"- `stopword_impact.csv` - Impact analysis of each stopword group\\n\")\n",
    "            f.write(\"- `top_terms_*.txt` - Top 100 terms for each corpus\\n\")\n",
    "            f.write(\"- `corpus_statistics.png` - Visual comparison charts\\n\")\n",
    "            f.write(\"- `tfidf_distributions.png` - Score distribution histograms\\n\")\n",
    "            f.write(\"- `quality_check.png` - Khmer vs non-Khmer term quality\\n\")\n",
    "        \n",
    "        print(f\"✓ Final report saved to: {os.path.basename(report_file)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    processed_dir = r'D:\\Year 5\\S1\\Information Retrieval\\StopwordProject\\khmer_stopword_project\\data\\processed'\n",
    "    output_dir = r'D:\\Year 5\\S1\\Information Retrieval\\StopwordProject\\khmer_stopword_project\\data\\tfidf_results_clean'\n",
    "    \n",
    "    # Files to analyze\n",
    "    files_to_analyze = [\n",
    "        ('Original', 'original_segmented_sentences.txt'),\n",
    "        ('No_All_Stopwords', 'no_all_stopwords.txt'),\n",
    "        ('No_Auxiliary_Verbs', 'no_Auxiliary_Verbs___Aspect_Markers.txt'),\n",
    "        ('No_Conjunctions', 'no_Conjunctions.txt'),\n",
    "        ('No_Determiners', 'no_Determiners_and_Quantifiers.txt'),\n",
    "        ('No_Function_Nouns', 'no_Function_Nouns.txt'),\n",
    "        ('No_Numbers', 'no_Numbers_and_Time_Expressions.txt'),\n",
    "        ('No_Particles', 'no_Particles_and_Discourse_Markers.txt'),\n",
    "        ('No_Politeness', 'no_Politeness_and_Honorifics.txt'),\n",
    "        ('No_Prepositions', 'no_Prepositions___Relational_Words.txt'),\n",
    "        ('No_Pronouns', 'no_Pronouns.txt'),\n",
    "        ('No_Questions', 'no_Question_and_Negation_Words.txt')\n",
    "    ]\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    print(\"=\"*80)\n",
    "    print(\" COMPREHENSIVE TF-IDF ANALYSIS WITH QUALITY FILTERING\")\n",
    "    print(\" Enhanced version with Khmer-only token extraction\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    analyzer = KhmerTfidfAnalyzer(processed_dir, output_dir)\n",
    "    \n",
    "    # Run analysis\n",
    "    analyzer.analyze_all_files(files_to_analyze)\n",
    "    analyzer.generate_comparative_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fce35c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
