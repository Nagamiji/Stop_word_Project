{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Evaluation: Impact of Stopword Removal\n",
    "\n",
    "This notebook implements the Information Retrieval (IR) evaluation as part of the Stop Word Project.\n",
    "We compare the retrieval performance of a Search Engine using two datasets:\n",
    "1. **Full Text**: Documents with all words (Stopwords included).\n",
    "2. **Cleaned Text**: Documents with Khmer stopwords removed.\n",
    "\n",
    "## Objectives:\n",
    "- Prepare the two datasets (segmentation and filtering).\n",
    "- Implement a TF-IDF based Vector Space Model.\n",
    "- Evaluate retrieval quality using **Known-Item Retrieval** task (simulating search ranking).\n",
    "- Metrics: Mean Rank, Recall@K.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries if not present\n",
    "!pip install khmer-nltk scikit-learn pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "try:\n",
    "    from khmernltk import word_tokenize\n",
    "    print(\"Khmer NLTK loaded successfully.\")\n",
    "except ImportError:\n",
    "    print(\"Khmer NLTK not found. Please run the installation cell above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Resources and Data\n",
    "We load the comprehensive Stopword list and the raw text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_custom_stopwords(csv_path):\n",
    "    stopwords = set()\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"Warning: Stopword file not found at {csv_path}\")\n",
    "        return stopwords\n",
    "        \n",
    "    with open(csv_path, encoding=\"utf-8-sig\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            # Filter based on Linguistic Group (Keep 'Content Words', Remove functional ones)\n",
    "            # If the group implies function word, add to stopwords.\n",
    "            # Based on previous notebook logic, we keep only 'Content Words' and remove others.\n",
    "            if \"content word\" not in row.get(\"linguistic_group\", \"\").lower():\n",
    "                stopwords.add(row[\"term\"].strip())\n",
    "    return stopwords\n",
    "\n",
    "STOPWORDS_PATH = \"../stopwords/FIle_Stopwords.csv\"\n",
    "KHMER_STOPWORDS = load_custom_stopwords(STOPWORDS_PATH)\n",
    "print(f\"Loaded {len(KHMER_STOPWORDS)} Khmer stopwords.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_corpus(filepath, limit=5000):\n",
    "    \"\"\"\n",
    "    Reads the raw file, tokenizes it, and creates two versions:\n",
    "    1. segmented_text (with stopwords)\n",
    "    2. filtered_text (without stopwords)\n",
    "    \"\"\"\n",
    "    raw_docs = []\n",
    "    corpus_sw = []\n",
    "    corpus_no_sw = []\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: Data file not found at {filepath}\")\n",
    "        return [], [], []\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line: \n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                tokens = word_tokenize(line)\n",
    "                if not tokens: continue\n",
    "                \n",
    "                # Join for TF-IDF (space separated)\n",
    "                text_sw = \" \".join(tokens)\n",
    "                \n",
    "                # Remove stopwords\n",
    "                tokens_filtered = [t for t in tokens if t not in KHMER_STOPWORDS]\n",
    "                text_no_sw = \" \".join(tokens_filtered)\n",
    "                \n",
    "                raw_docs.append(line)\n",
    "                corpus_sw.append(text_sw)\n",
    "                corpus_no_sw.append(text_no_sw)\n",
    "                \n",
    "                count += 1\n",
    "                if count >= limit:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                continue\n",
    "                \n",
    "    print(f\"Processed {len(corpus_sw)} documents.\")\n",
    "    return raw_docs, corpus_sw, corpus_no_sw\n",
    "\n",
    "# Load a sample of 3000 documents for evaluation speed\n",
    "DATA_PATH = \"../data/raw/news_text_file_150k.txt\"\n",
    "raw_docs, docs_with_sw, docs_without_sw = load_and_process_corpus(DATA_PATH, limit=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. IR System Implementation\n",
    "We use TF-IDF weighting and Cosine Similarity.\n",
    "We define a function `evaluate_ir` that takes a corpus and a set of query documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(corpus, query_indices, top_k=10):\n",
    "    \"\"\"\n",
    "    Evaluates retrieval performance using Known-Item Retrieval.\n",
    "    For each document in query_indices, we try to retrieve it from the corpus.\n",
    "    Ideally, it should be Rank 1.\n",
    "    \"\"\"\n",
    "    # 1. Build Index\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_corpus = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    # 2. Process Queries\n",
    "    # The queries are the documents themselves\n",
    "    queries = [corpus[i] for i in query_indices]\n",
    "    X_queries = vectorizer.transform(queries)\n",
    "    \n",
    "    # 3. Compute Similarity\n",
    "    # Shape: (n_queries, n_corpus)\n",
    "    sim_matrix = cosine_similarity(X_queries, X_corpus)\n",
    "    \n",
    "    ranks = []\n",
    "    hits_at_k = 0\n",
    "    \n",
    "    for i, true_doc_idx in enumerate(query_indices):\n",
    "        scores = sim_matrix[i]\n",
    "        \n",
    "        # Sort indices by score descending\n",
    "        sorted_indices = np.argsort(scores)[::-1]\n",
    "        \n",
    "        # Find where the true document is in the ranked list\n",
    "        # np.where returns a tuple, [0][0] gets the index\n",
    "        rank_positions = np.where(sorted_indices == true_doc_idx)[0]\n",
    "        \n",
    "        if len(rank_positions) > 0:\n",
    "            rank = rank_positions[0] + 1 # 1-based rank\n",
    "        else:\n",
    "            rank = len(corpus) # Should not happen if query is in corpus\n",
    "            \n",
    "        ranks.append(rank)\n",
    "        if rank <= top_k:\n",
    "            hits_at_k += 1\n",
    "            \n",
    "    mean_rank = np.mean(ranks)\n",
    "    recall_at_k = hits_at_k / len(query_indices)\n",
    "    \n",
    "    return mean_rank, recall_at_k, ranks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Experiments\n",
    "We select 100 random documents as \"queries\" and test retrieval on both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "NUM_QUERIES = 50\n",
    "if len(docs_with_sw) > NUM_QUERIES:\n",
    "    query_indices = random.sample(range(len(docs_with_sw)), NUM_QUERIES)\n",
    "else:\n",
    "    query_indices = list(range(len(docs_with_sw)))\n",
    "\n",
    "print(f\"Selected {len(query_indices)} random documents as queries.\")\n",
    "\n",
    "# Experiment 1: With Stopwords\n",
    "print(\"\\n--- Evaluating WITH Stopwords ---\")\n",
    "mr_sw, r_k_sw, ranks_sw = evaluate_retrieval(docs_with_sw, query_indices)\n",
    "print(f\"Mean Rank: {mr_sw:.2f}\")\n",
    "print(f\"Recall@10: {r_k_sw:.2f}\")\n",
    "\n",
    "# Experiment 2: Without Stopwords\n",
    "print(\"\\n--- Evaluating WITHOUT Stopwords ---\")\n",
    "mr_now, r_k_now, ranks_now = evaluate_retrieval(docs_without_sw, query_indices)\n",
    "print(f\"Mean Rank: {mr_now:.2f}\")\n",
    "print(f\"Recall@10: {r_k_now:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis and Visualization\n",
    "We compare the rank distribution to see if removing stopwords helps the correct document appear higher (closer to rank 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(ranks_sw, alpha=0.5, label='With Stopwords', bins=20)\n",
    "plt.hist(ranks_now, alpha=0.5, label='Without Stopwords', bins=20)\n",
    "plt.xlabel('Rank of Relevant Document')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Ranks (Lower is Better)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Diff in Mean Rank:\", mr_sw - mr_now)\n",
    "if mr_now < mr_sw:\n",
    "    print(\"Conclusion: Removing stopwords IMPROVED retrieval performance.\")\n",
    "else:\n",
    "    print(\"Conclusion: Removing stopwords DID NOT improve retrieval performance (or slight degradation).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "- **Mean Rank**: The average position of the correct document. Lower is better (1.0 is perfect).\n",
    "- **Recall@10**: Percentage of times the correct document appeared in the top 10 results.\n",
    "\n",
    "In highly specific retrieval (like this known-item task), stopwords can sometimes help by providing phrase specificity, but in general topic retrieval, they add noise. If the Mean Rank decreases after removal, our customized stopword list is effective."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
