{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Evaluation: Impact of Stopword Removal\n",
    "\n",
    "This notebook implements the Information Retrieval (IR) evaluation as part of the Stop Word Project.\n",
    "We compare the retrieval performance of a Search Engine using two datasets:\n",
    "1. **Full Text**: Documents with all words (Stopwords included).\n",
    "2. **Cleaned Text**: Documents with Khmer stopwords removed.\n",
    "\n",
    "## Objectives:\n",
    "- Prepare the two datasets (segmentation and filtering).\n",
    "- Implement a TF-IDF based Vector Space Model.\n",
    "- Evaluate retrieval quality using **Known-Item Retrieval** task (simulating search ranking).\n",
    "- Metrics: Mean Rank, Recall@K.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries if not present\n",
    "!pip install khmer-nltk scikit-learn pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "try:\n",
    "    from khmernltk import word_tokenize\n",
    "    print(\"Khmer NLTK loaded successfully.\")\n",
    "except ImportError:\n",
    "    print(\"Khmer NLTK not found. Please run the installation cell above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Resources and Data\n",
    "We load the comprehensive Stopword list and the raw text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_custom_stopwords(csv_path):\n",
    "    stopwords = set()\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"Warning: Stopword file not found at {csv_path}\")\n",
    "        return stopwords\n",
    "        \n",
    "    with open(csv_path, encoding=\"utf-8-sig\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            # Filter based on Linguistic Group (Keep 'Content Words', Remove functional ones)\n",
    "            # If the group implies function word, add to stopwords.\n",
    "            # Based on previous notebook logic, we keep only 'Content Words' and remove others.\n",
    "            if \"content word\" not in row.get(\"linguistic_group\", \"\").lower():\n",
    "                stopwords.add(row[\"term\"].strip())\n",
    "    return stopwords\n",
    "\n",
    "STOPWORDS_PATH = \"../stopwords/FIle_Stopwords.csv\"\n",
    "KHMER_STOPWORDS = load_custom_stopwords(STOPWORDS_PATH)\n",
    "print(f\"Loaded {len(KHMER_STOPWORDS)} Khmer stopwords.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_corpus(filepath, limit=5000):\n",
    "    \"\"\"\n",
    "    Reads the raw file, tokenizes it, and creates two versions:\n",
    "    1. segmented_text (with stopwords)\n",
    "    2. filtered_text (without stopwords)\n",
    "    \"\"\"\n",
    "    raw_docs = []\n",
    "    corpus_sw = []\n",
    "    corpus_no_sw = []\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: Data file not found at {filepath}\")\n",
    "        return [], [], []\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line: \n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                tokens = word_tokenize(line)\n",
    "                if not tokens: continue\n",
    "                \n",
    "                # Join for TF-IDF (space separated)\n",
    "                text_sw = \" \".join(tokens)\n",
    "                \n",
    "                # Remove stopwords\n",
    "                tokens_filtered = [t for t in tokens if t not in KHMER_STOPWORDS]\n",
    "                text_no_sw = \" \".join(tokens_filtered)\n",
    "                \n",
    "                raw_docs.append(line)\n",
    "                corpus_sw.append(text_sw)\n",
    "                corpus_no_sw.append(text_no_sw)\n",
    "                \n",
    "                count += 1\n",
    "                if count >= limit:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                continue\n",
    "                \n",
    "    print(f\"Processed {len(corpus_sw)} documents.\")\n",
    "    return raw_docs, corpus_sw, corpus_no_sw\n",
    "\n",
    "# Load a sample of 3000 documents for evaluation speed\n",
    "DATA_PATH = \"../data/raw/news_text_file_150k.txt\"\n",
    "raw_docs, docs_with_sw, docs_without_sw = load_and_process_corpus(DATA_PATH, limit=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. IR System Implementation\n",
    "We use TF-IDF weighting and Cosine Similarity.\n",
    "We define a function `evaluate_ir` that takes a corpus and a set of query documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(corpus, query_indices, top_k=10):\n",
    "    \"\"\"\n",
    "    Evaluates retrieval performance using Known-Item Retrieval.\n",
    "    For each document in query_indices, we try to retrieve it from the corpus.\n",
    "    Ideally, it should be Rank 1.\n",
    "    \"\"\"\n",
    "    # 1. Build Index\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_corpus = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    # 2. Process Queries\n",
    "    # The queries are the documents themselves\n",
    "    queries = [corpus[i] for i in query_indices]\n",
    "    X_queries = vectorizer.transform(queries)\n",
    "    \n",
    "    # 3. Compute Similarity\n",
    "    # Shape: (n_queries, n_corpus)\n",
    "    sim_matrix = cosine_similarity(X_queries, X_corpus)\n",
    "    \n",
    "    ranks = []\n",
    "    hits_at_k = 0\n",
    "    \n",
    "    for i, true_doc_idx in enumerate(query_indices):\n",
    "        scores = sim_matrix[i]\n",
    "        \n",
    "        # Sort indices by score descending\n",
    "        sorted_indices = np.argsort(scores)[::-1]\n",
    "        \n",
    "        # Find where the true document is in the ranked list\n",
    "        # np.where returns a tuple, [0][0] gets the index\n",
    "        rank_positions = np.where(sorted_indices == true_doc_idx)[0]\n",
    "        \n",
    "        if len(rank_positions) > 0:\n",
    "            rank = rank_positions[0] + 1 # 1-based rank\n",
    "        else:\n",
    "            rank = len(corpus) # Should not happen if query is in corpus\n",
    "            \n",
    "        ranks.append(rank)\n",
    "        if rank <= top_k:\n",
    "            hits_at_k += 1\n",
    "            \n",
    "    mean_rank = np.mean(ranks)\n",
    "    recall_at_k = hits_at_k / len(query_indices)\n",
    "    \n",
    "    return mean_rank, recall_at_k, ranks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Experiments\n",
    "We select 100 random documents as \"queries\" and test retrieval on both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "NUM_QUERIES = 50\n",
    "if len(docs_with_sw) > NUM_QUERIES:\n",
    "    query_indices = random.sample(range(len(docs_with_sw)), NUM_QUERIES)\n",
    "else:\n",
    "    query_indices = list(range(len(docs_with_sw)))\n",
    "\n",
    "print(f\"Selected {len(query_indices)} random documents as queries.\")\n",
    "\n",
    "# Experiment 1: With Stopwords\n",
    "print(\"\\n--- Evaluating WITH Stopwords ---\")\n",
    "mr_sw, r_k_sw, ranks_sw = evaluate_retrieval(docs_with_sw, query_indices)\n",
    "print(f\"Mean Rank: {mr_sw:.2f}\")\n",
    "print(f\"Recall@10: {r_k_sw:.2f}\")\n",
    "\n",
    "# Experiment 2: Without Stopwords\n",
    "print(\"\\n--- Evaluating WITHOUT Stopwords ---\")\n",
    "mr_now, r_k_now, ranks_now = evaluate_retrieval(docs_without_sw, query_indices)\n",
    "print(f\"Mean Rank: {mr_now:.2f}\")\n",
    "print(f\"Recall@10: {r_k_now:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis and Visualization\n",
    "We compare the rank distribution to see if removing stopwords helps the correct document appear higher (closer to rank 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(ranks_sw, alpha=0.5, label='With Stopwords', bins=20)\n",
    "plt.hist(ranks_now, alpha=0.5, label='Without Stopwords', bins=20)\n",
    "plt.xlabel('Rank of Relevant Document')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Ranks (Lower is Better)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Diff in Mean Rank:\", mr_sw - mr_now)\n",
    "if mr_now < mr_sw:\n",
    "    print(\"Conclusion: Removing stopwords IMPROVED retrieval performance.\")\n",
    "else:\n",
    "    print(\"Conclusion: Removing stopwords DID NOT improve retrieval performance (or slight degradation).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "- **Mean Rank**: The average position of the correct document. Lower is better (1.0 is perfect).\n",
    "- **Recall@10**: Percentage of times the correct document appeared in the top 10 results.\n",
    "\n",
    "In highly specific retrieval (like this known-item task), stopwords can sometimes help by providing phrase specificity, but in general topic retrieval, they add noise. If the Mean Rank decreases after removal, our customized stopword list is effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply to IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      " ENHANCED IR EVALUATION SYSTEM\n",
      " With Khmer-specific preprocessing and quality filtering\n",
      "================================================================================\n",
      "\n",
      "üìÅ Loading and preprocessing corpora:\n",
      "------------------------------------------------------------\n",
      "  Loading: Original\n",
      "  Filtered out 16 very short documents\n",
      "    Documents: 58,382\n",
      "    Token quality: 1212/1363 (88.9%) are 3+ chars\n",
      "    Khmer content: 100.0%\n",
      "  Loading: No_All_Stopwords\n",
      "  Filtered out 152 very short documents\n",
      "    Documents: 58,242\n",
      "    Token quality: 890/982 (90.6%) are 3+ chars\n",
      "    Khmer content: 100.0%\n",
      "  Loading: No_Auxiliary_Verbs\n",
      "  Filtered out 82 very short documents\n",
      "    Documents: 58,316\n",
      "    Token quality: 1152/1303 (88.4%) are 3+ chars\n",
      "    Khmer content: 100.0%\n",
      "  Loading: No_Conjunctions\n",
      "  Filtered out 36 very short documents\n",
      "    Documents: 58,362\n",
      "    Token quality: 1141/1283 (88.9%) are 3+ chars\n",
      "    Khmer content: 100.0%\n",
      "  Loading: No_Determiners\n",
      "  Filtered out 17 very short documents\n",
      "    Documents: 58,381\n",
      "    Token quality: 1181/1332 (88.7%) are 3+ chars\n",
      "    Khmer content: 100.0%\n",
      "  Loading: No_Function_Nouns\n",
      "  Filtered out 21 very short documents\n",
      "    Documents: 58,377\n",
      "    Token quality: 1181/1332 (88.7%) are 3+ chars\n",
      "    Khmer content: 100.0%\n",
      "  Loading: No_Numbers\n",
      "  Filtered out 17 very short documents\n",
      "    Documents: 58,381\n",
      "    Token quality: 1195/1346 (88.8%) are 3+ chars\n",
      "    Khmer content: 100.0%\n",
      "  Loading: No_Particles\n",
      "  Filtered out 17 very short documents\n",
      "    Documents: 58,381\n",
      "    Token quality: 1191/1335 (89.2%) are 3+ chars\n",
      "    Khmer content: 100.0%\n",
      "  Loading: No_Politeness\n",
      "  Filtered out 28 very short documents\n",
      "    Documents: 58,370\n",
      "    Token quality: 1204/1355 (88.9%) are 3+ chars\n",
      "    Khmer content: 100.0%\n",
      "  Loading: No_Prepositions\n",
      "  Filtered out 22 very short documents\n",
      "    Documents: 58,376\n",
      "    Token quality: 1154/1270 (90.9%) are 3+ chars\n",
      "    Khmer content: 100.0%\n",
      "  Loading: No_Pronouns\n",
      "  Filtered out 18 very short documents\n",
      "    Documents: 58,380\n",
      "    Token quality: 1199/1347 (89.0%) are 3+ chars\n",
      "    Khmer content: 100.0%\n",
      "  Loading: No_Questions\n",
      "  Filtered out 18 very short documents\n",
      "    Documents: 58,380\n",
      "    Token quality: 1200/1346 (89.2%) are 3+ chars\n",
      "    Khmer content: 100.0%\n",
      "\n",
      "‚úì Successfully loaded 12 corpora\n",
      "\n",
      "üìä Corpus sizes for evaluation:\n",
      "  Original                 :  58382 documents\n",
      "  No_All_Stopwords         :  58242 documents\n",
      "  No_Auxiliary_Verbs       :  58316 documents\n",
      "  No_Conjunctions          :  58362 documents\n",
      "  No_Determiners           :  58381 documents\n",
      "  No_Function_Nouns        :  58377 documents\n",
      "  No_Numbers               :  58381 documents\n",
      "  No_Particles             :  58381 documents\n",
      "  No_Politeness            :  58370 documents\n",
      "  No_Prepositions          :  58376 documents\n",
      "  No_Pronouns              :  58380 documents\n",
      "  No_Questions             :  58380 documents\n",
      "\n",
      "üìù Using 100 queries (based on smallest corpus: 58242 documents)\n",
      "\n",
      "================================================================================\n",
      " COMPREHENSIVE IR EVALUATION WITH QUALITY FILTERING\n",
      "================================================================================\n",
      "\n",
      "üìä Evaluating: Original\n",
      "   Corpus size: 58,382 documents\n",
      "   ‚úì Mean Rank: 1.18\n",
      "   ‚úì Median Rank: 1.0\n",
      "   ‚úì Vocabulary size: 5,000\n",
      "   ‚úì Recall@5: 0.990\n",
      "   ‚úì Recall@10: 0.990\n",
      "   ‚úì Recall@20: 1.000\n",
      "\n",
      "üìä Evaluating: No_All_Stopwords\n",
      "   Corpus size: 58,242 documents\n",
      "   ‚úì Mean Rank: 1.11\n",
      "   ‚úì Median Rank: 1.0\n",
      "   ‚úì Vocabulary size: 5,000\n",
      "   ‚úì Recall@5: 0.990\n",
      "   ‚úì Recall@10: 0.990\n",
      "   ‚úì Recall@20: 1.000\n",
      "\n",
      "üìä Evaluating: No_Auxiliary_Verbs\n",
      "   Corpus size: 58,316 documents\n",
      "   ‚úì Mean Rank: 1.01\n",
      "   ‚úì Median Rank: 1.0\n",
      "   ‚úì Vocabulary size: 5,000\n",
      "   ‚úì Recall@5: 1.000\n",
      "   ‚úì Recall@10: 1.000\n",
      "   ‚úì Recall@20: 1.000\n",
      "\n",
      "üìä Evaluating: No_Conjunctions\n",
      "   Corpus size: 58,362 documents\n",
      "   ‚úì Mean Rank: 1.02\n",
      "   ‚úì Median Rank: 1.0\n",
      "   ‚úì Vocabulary size: 5,000\n",
      "   ‚úì Recall@5: 1.000\n",
      "   ‚úì Recall@10: 1.000\n",
      "   ‚úì Recall@20: 1.000\n",
      "\n",
      "üìä Evaluating: No_Determiners\n",
      "   Corpus size: 58,381 documents\n",
      "   ‚úì Mean Rank: 1.06\n",
      "   ‚úì Median Rank: 1.0\n",
      "   ‚úì Vocabulary size: 5,000\n",
      "   ‚úì Recall@5: 1.000\n",
      "   ‚úì Recall@10: 1.000\n",
      "   ‚úì Recall@20: 1.000\n",
      "\n",
      "üìä Evaluating: No_Function_Nouns\n",
      "   Corpus size: 58,377 documents\n",
      "   ‚úì Mean Rank: 1.04\n",
      "   ‚úì Median Rank: 1.0\n",
      "   ‚úì Vocabulary size: 5,000\n",
      "   ‚úì Recall@5: 1.000\n",
      "   ‚úì Recall@10: 1.000\n",
      "   ‚úì Recall@20: 1.000\n",
      "\n",
      "üìä Evaluating: No_Numbers\n",
      "   Corpus size: 58,381 documents\n",
      "   ‚úì Mean Rank: 1.12\n",
      "   ‚úì Median Rank: 1.0\n",
      "   ‚úì Vocabulary size: 5,000\n",
      "   ‚úì Recall@5: 0.990\n",
      "   ‚úì Recall@10: 0.990\n",
      "   ‚úì Recall@20: 1.000\n",
      "\n",
      "üìä Evaluating: No_Particles\n",
      "   Corpus size: 58,381 documents\n",
      "   ‚úì Mean Rank: 1.01\n",
      "   ‚úì Median Rank: 1.0\n",
      "   ‚úì Vocabulary size: 5,000\n",
      "   ‚úì Recall@5: 1.000\n",
      "   ‚úì Recall@10: 1.000\n",
      "   ‚úì Recall@20: 1.000\n",
      "\n",
      "üìä Evaluating: No_Politeness\n",
      "   Corpus size: 58,370 documents\n",
      "   ‚úì Mean Rank: 1.16\n",
      "   ‚úì Median Rank: 1.0\n",
      "   ‚úì Vocabulary size: 5,000\n",
      "   ‚úì Recall@5: 0.990\n",
      "   ‚úì Recall@10: 0.990\n",
      "   ‚úì Recall@20: 1.000\n",
      "\n",
      "üìä Evaluating: No_Prepositions\n",
      "   Corpus size: 58,376 documents\n",
      "   ‚úì Mean Rank: 432.21\n",
      "   ‚úì Median Rank: 1.0\n",
      "   ‚úì Vocabulary size: 5,000\n",
      "   ‚úì Recall@5: 0.990\n",
      "   ‚úì Recall@10: 0.990\n",
      "   ‚úì Recall@20: 0.990\n",
      "\n",
      "üìä Evaluating: No_Pronouns\n",
      "   Corpus size: 58,380 documents\n",
      "   ‚úì Mean Rank: 1.02\n",
      "   ‚úì Median Rank: 1.0\n",
      "   ‚úì Vocabulary size: 5,000\n",
      "   ‚úì Recall@5: 1.000\n",
      "   ‚úì Recall@10: 1.000\n",
      "   ‚úì Recall@20: 1.000\n",
      "\n",
      "üìä Evaluating: No_Questions\n",
      "   Corpus size: 58,380 documents\n",
      "   ‚úì Mean Rank: 1.18\n",
      "   ‚úì Median Rank: 1.0\n",
      "   ‚úì Vocabulary size: 5,000\n",
      "   ‚úì Recall@5: 0.990\n",
      "   ‚úì Recall@10: 0.990\n",
      "   ‚úì Recall@20: 1.000\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìã COMPARISON RESULTS (Sorted by Mean Rank - Best First):\n",
      "====================================================================================================\n",
      "            Corpus  Documents  Vocab_Size  Mean_Rank  Recall@5  Recall@10  Recall@20\n",
      "No_Auxiliary_Verbs      58316        5000      1.010     1.000      1.000      1.000\n",
      "      No_Particles      58381        5000      1.010     1.000      1.000      1.000\n",
      "   No_Conjunctions      58362        5000      1.020     1.000      1.000      1.000\n",
      "       No_Pronouns      58380        5000      1.020     1.000      1.000      1.000\n",
      " No_Function_Nouns      58377        5000      1.040     1.000      1.000      1.000\n",
      "    No_Determiners      58381        5000      1.060     1.000      1.000      1.000\n",
      "  No_All_Stopwords      58242        5000      1.110     0.990      0.990      1.000\n",
      "        No_Numbers      58381        5000      1.120     0.990      0.990      1.000\n",
      "     No_Politeness      58370        5000      1.160     0.990      0.990      1.000\n",
      "          Original      58382        5000      1.180     0.990      0.990      1.000\n",
      "      No_Questions      58380        5000      1.180     0.990      0.990      1.000\n",
      "   No_Prepositions      58376        5000    432.210     0.990      0.990      0.990\n",
      "\n",
      "‚úì Full comparison results saved to: D:\\Year 5\\S1\\Information Retrieval\\StopwordProject\\khmer_stopword_project\\data\\ir_evaluation_enhanced\\ir_comparison_results_enhanced.csv\n",
      "\n",
      "üìä Creating visualizations...\n",
      "\n",
      "üìà Performing statistical analysis...\n",
      "\n",
      "================================================================================\n",
      "STATISTICAL ANALYSIS: Comparison to Baseline\n",
      "================================================================================\n",
      "\n",
      "Rank Improvement Analysis (vs Original Corpus):\n",
      "----------------------------------------------------------------------------------------------------\n",
      "            Corpus  Mean_Rank  Baseline_Rank  Rank_Improvement  Percent_Improvement  P_Value  Significant\n",
      "No_Auxiliary_Verbs      1.010          1.180             0.170               14.407    0.348        False\n",
      "      No_Particles      1.010          1.180             0.170               14.407    0.348        False\n",
      "   No_Conjunctions      1.020          1.180             0.160               13.559    0.378        False\n",
      "       No_Pronouns      1.020          1.180             0.160               13.559    0.320        False\n",
      " No_Function_Nouns      1.040          1.180             0.140               11.864    0.444        False\n",
      "    No_Determiners      1.060          1.180             0.120               10.169    0.396        False\n",
      "  No_All_Stopwords      1.110          1.180             0.070                5.932    0.742        False\n",
      "        No_Numbers      1.120          1.180             0.060                5.085    0.399        False\n",
      "     No_Politeness      1.160          1.180             0.020                1.695    0.640        False\n",
      "      No_Questions      1.180          1.180             0.000                0.000    1.000        False\n",
      "   No_Prepositions    432.210          1.180          -431.030           -36527.966    0.320        False\n",
      "\n",
      "‚úì Statistical analysis saved to: D:\\Year 5\\S1\\Information Retrieval\\StopwordProject\\khmer_stopword_project\\data\\ir_evaluation_enhanced\\statistical_analysis_enhanced.csv\n",
      "\n",
      "üìÑ Generating comprehensive report...\n",
      "‚úì Comprehensive report generated: D:\\Year 5\\S1\\Information Retrieval\\StopwordProject\\khmer_stopword_project\\data\\ir_evaluation_enhanced\\ir_evaluation_report.md\n",
      "\n",
      "================================================================================\n",
      "EVALUATION SUMMARY\n",
      "================================================================================\n",
      "üìä Corpora evaluated: 12\n",
      "üèÜ Best performer: No_Auxiliary_Verbs (Mean Rank: 1.01)\n",
      "üìà Improvement over baseline: 14.4%\n",
      "‚úÖ Statistically significant improvements: 0 groups\n",
      "================================================================================\n",
      "\n",
      "‚úÖ IR EVALUATION COMPLETE\n",
      "üìÅ Results saved to: D:\\Year 5\\S1\\Information Retrieval\\StopwordProject\\khmer_stopword_project\\data\\ir_evaluation_enhanced\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ENHANCED IR EVALUATION WITH TF-IDF QUALITY FILTERING\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import ndcg_score\n",
    "from scipy import stats\n",
    "\n",
    "class KhmerIREvaluator:\n",
    "    \"\"\"Advanced IR evaluation with Khmer-specific preprocessing and TF-IDF cleaning\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorizer_params=None):\n",
    "        self.khmer_pattern = re.compile(r'[\\u1780-\\u17FF]')\n",
    "        \n",
    "        if vectorizer_params is None:\n",
    "            self.vectorizer_params = {\n",
    "                'max_features': 5000,\n",
    "                'min_df': 2,\n",
    "                'max_df': 0.95,\n",
    "                'use_idf': True,\n",
    "                'smooth_idf': True,\n",
    "                'lowercase': False\n",
    "            }\n",
    "        else:\n",
    "            self.vectorizer_params = vectorizer_params\n",
    "        \n",
    "        self.results = {}\n",
    "        \n",
    "    def clean_tokens(self, text, min_length=2):\n",
    "        \"\"\"Clean text to keep only meaningful Khmer tokens\"\"\"\n",
    "        tokens = text.split()\n",
    "        cleaned_tokens = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            # Must contain Khmer\n",
    "            if not self.khmer_pattern.search(token):\n",
    "                continue\n",
    "            \n",
    "            # Must be long enough (filter out single characters)\n",
    "            if len(token) < min_length:\n",
    "                continue\n",
    "            \n",
    "            # Count actual Khmer characters\n",
    "            khmer_chars = len([c for c in token if '\\u1780' <= c <= '\\u17DD'])\n",
    "            \n",
    "            # Must have at least min_length actual Khmer base characters\n",
    "            if khmer_chars < min_length:\n",
    "                continue\n",
    "            \n",
    "            cleaned_tokens.append(token)\n",
    "        \n",
    "        return ' '.join(cleaned_tokens)\n",
    "    \n",
    "    def load_and_preprocess_corpus(self, filepath, label):\n",
    "        \"\"\"Load and preprocess corpus with quality filtering\"\"\"\n",
    "        print(f\"  Loading: {label}\")\n",
    "        \n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"  ‚ùå File not found: {filepath}\")\n",
    "            return None\n",
    "        \n",
    "        # Read documents\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            documents = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "        if not documents:\n",
    "            print(f\"  ‚ö†Ô∏è  Empty file!\")\n",
    "            return None\n",
    "        \n",
    "        # Clean documents (keep only Khmer, min 2 characters)\n",
    "        cleaned_docs = [self.clean_tokens(doc, min_length=2) for doc in documents]\n",
    "        cleaned_docs = [doc for doc in cleaned_docs if doc]  # Remove empty\n",
    "        \n",
    "        # Additional filtering: remove very short documents\n",
    "        filtered_docs = [doc for doc in cleaned_docs if len(doc.split()) >= 3]\n",
    "        \n",
    "        if len(filtered_docs) < len(cleaned_docs):\n",
    "            print(f\"  Filtered out {len(cleaned_docs) - len(filtered_docs)} very short documents\")\n",
    "        \n",
    "        if not filtered_docs:\n",
    "            print(f\"  ‚ùå No valid documents after filtering!\")\n",
    "            return None\n",
    "        \n",
    "        # Diagnose token quality\n",
    "        self._diagnose_token_quality(filtered_docs, label)\n",
    "        \n",
    "        return filtered_docs\n",
    "    \n",
    "    def _diagnose_token_quality(self, documents, label):\n",
    "        \"\"\"Diagnose token quality in loaded corpus\"\"\"\n",
    "        sample_docs = documents[:50] if len(documents) > 50 else documents\n",
    "        all_tokens = []\n",
    "        for doc in sample_docs:\n",
    "            all_tokens.extend(doc.split())\n",
    "        \n",
    "        # Categorize tokens\n",
    "        single_char = [t for t in all_tokens if len(t) == 1]\n",
    "        two_char = [t for t in all_tokens if len(t) == 2]\n",
    "        three_plus = [t for t in all_tokens if len(t) >= 3]\n",
    "        \n",
    "        khmer_tokens = [t for t in all_tokens if self.khmer_pattern.search(t)]\n",
    "        khmer_pct = (len(khmer_tokens) / len(all_tokens) * 100) if all_tokens else 0\n",
    "        \n",
    "        print(f\"    Documents: {len(documents):,}\")\n",
    "        print(f\"    Token quality: {len(three_plus)}/{len(all_tokens)} ({len(three_plus)/len(all_tokens)*100:.1f}%) are 3+ chars\")\n",
    "        print(f\"    Khmer content: {khmer_pct:.1f}%\")\n",
    "        \n",
    "        if single_char:\n",
    "            print(f\"    ‚ö†Ô∏è  Single-char tokens: {len(single_char)}\")\n",
    "        \n",
    "        return {\n",
    "            'total_docs': len(documents),\n",
    "            'total_tokens': len(all_tokens),\n",
    "            'single_char_tokens': len(single_char),\n",
    "            'khmer_pct': khmer_pct\n",
    "        }\n",
    "    \n",
    "    def build_index(self, corpus, use_quality_filtering=True):\n",
    "        \"\"\"Build TF-IDF index with optional quality filtering\"\"\"\n",
    "        \n",
    "        if use_quality_filtering:\n",
    "            # Pre-filter documents with custom tokenizer\n",
    "            filtered_docs = []\n",
    "            for doc in corpus:\n",
    "                tokens = doc.split()\n",
    "                # Keep only tokens with at least 2 characters\n",
    "                valid_tokens = [t for t in tokens if len(t) >= 2]\n",
    "                if valid_tokens:\n",
    "                    filtered_docs.append(' '.join(valid_tokens))\n",
    "            \n",
    "            if not filtered_docs:\n",
    "                print(\"  ‚ùå No valid tokens after filtering!\")\n",
    "                return None, None\n",
    "            \n",
    "            corpus = filtered_docs\n",
    "        \n",
    "        # Create vectorizer\n",
    "        def khmer_tokenizer(text):\n",
    "            \"\"\"Custom tokenizer that only splits on whitespace\"\"\"\n",
    "            return text.split()\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(\n",
    "            **self.vectorizer_params,\n",
    "            tokenizer=khmer_tokenizer,\n",
    "            token_pattern=None  # Disable default pattern\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            X = vectorizer.fit_transform(corpus)\n",
    "            return vectorizer, X\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error building index: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def evaluate_retrieval(self, corpus, query_indices, top_k_values=[1, 5, 10, 20, 50]):\n",
    "        \"\"\"\n",
    "        Evaluate retrieval performance with multiple metrics\n",
    "        \n",
    "        Returns:\n",
    "            dict: Dictionary containing all evaluation metrics\n",
    "        \"\"\"\n",
    "        # Build index with quality filtering\n",
    "        vectorizer, X_corpus = self.build_index(corpus, use_quality_filtering=True)\n",
    "        \n",
    "        if X_corpus is None:\n",
    "            print(\"  ‚ùå Could not build index\")\n",
    "            return None\n",
    "        \n",
    "        # Prepare queries (documents themselves for known-item retrieval)\n",
    "        queries = [corpus[i] for i in query_indices]\n",
    "        X_queries = vectorizer.transform(queries)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        sim_matrix = cosine_similarity(X_queries, X_corpus)\n",
    "        \n",
    "        # Initialize result storage\n",
    "        results = {\n",
    "            'ranks': [],\n",
    "            'precision_at_k': {k: [] for k in top_k_values},\n",
    "            'recall_at_k': {k: [] for k in top_k_values},\n",
    "            'ndcg_at_k': {k: [] for k in top_k_values},\n",
    "            'query_scores': [],\n",
    "            'vocab_size': X_corpus.shape[1],\n",
    "            'doc_count': len(corpus)\n",
    "        }\n",
    "        \n",
    "        # Process each query\n",
    "        for i, true_doc_idx in enumerate(query_indices):\n",
    "            scores = sim_matrix[i]\n",
    "            \n",
    "            # Get ranked list (descending)\n",
    "            sorted_indices = np.argsort(scores)[::-1]\n",
    "            \n",
    "            # Find rank of true document (1-based)\n",
    "            rank_positions = np.where(sorted_indices == true_doc_idx)[0]\n",
    "            rank = rank_positions[0] + 1 if len(rank_positions) > 0 else len(corpus)\n",
    "            results['ranks'].append(rank)\n",
    "            \n",
    "            # Calculate binary relevance vector for NDCG\n",
    "            relevance = np.zeros(len(corpus))\n",
    "            relevance[true_doc_idx] = 1\n",
    "            \n",
    "            # Calculate metrics at different K values\n",
    "            for k in top_k_values:\n",
    "                if k <= len(corpus):\n",
    "                    # Precision@K\n",
    "                    relevant_in_top_k = np.sum(relevance[sorted_indices[:k]])\n",
    "                    precision = relevant_in_top_k / k\n",
    "                    results['precision_at_k'][k].append(precision)\n",
    "                    \n",
    "                    # Recall@K\n",
    "                    total_relevant = 1  # Only one relevant document\n",
    "                    recall = relevant_in_top_k / total_relevant\n",
    "                    results['recall_at_k'][k].append(recall)\n",
    "                    \n",
    "                    # NDCG@K\n",
    "                    ndcg = ndcg_score([relevance], [scores], k=k)\n",
    "                    results['ndcg_at_k'][k].append(ndcg)\n",
    "                else:\n",
    "                    results['precision_at_k'][k].append(np.nan)\n",
    "                    results['recall_at_k'][k].append(np.nan)\n",
    "                    results['ndcg_at_k'][k].append(np.nan)\n",
    "            \n",
    "            # Store query scores for analysis\n",
    "            results['query_scores'].append(scores[true_doc_idx])\n",
    "        \n",
    "        # Calculate aggregate metrics\n",
    "        results['mean_rank'] = np.mean(results['ranks'])\n",
    "        results['median_rank'] = np.median(results['ranks'])\n",
    "        results['std_rank'] = np.std(results['ranks'])\n",
    "        results['min_rank'] = np.min(results['ranks'])\n",
    "        results['max_rank'] = np.max(results['ranks'])\n",
    "        \n",
    "        for k in top_k_values:\n",
    "            if k <= len(corpus):\n",
    "                results[f'mean_precision@{k}'] = np.nanmean(results['precision_at_k'][k])\n",
    "                results[f'mean_recall@{k}'] = np.nanmean(results['recall_at_k'][k])\n",
    "                results[f'mean_ndcg@{k}'] = np.nanmean(results['ndcg_at_k'][k])\n",
    "            else:\n",
    "                results[f'mean_precision@{k}'] = np.nan\n",
    "                results[f'mean_recall@{k}'] = np.nan\n",
    "                results[f'mean_ndcg@{k}'] = np.nan\n",
    "        \n",
    "        # Calculate success rate (document found in top K)\n",
    "        results['success_rate'] = {}\n",
    "        for k in top_k_values:\n",
    "            if k <= len(corpus):\n",
    "                results['success_rate'][k] = np.mean([1 if r <= k else 0 for r in results['ranks']])\n",
    "            else:\n",
    "                results['success_rate'][k] = np.nan\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_all_corpora(self, corpora_dict, query_indices, top_k_values=[1, 5, 10, 20, 50]):\n",
    "        \"\"\"Evaluate retrieval performance across multiple corpora\"\"\"\n",
    "        \n",
    "        comparison_results = {}\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\" COMPREHENSIVE IR EVALUATION WITH QUALITY FILTERING\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for label, corpus in corpora_dict.items():\n",
    "            print(f\"\\nüìä Evaluating: {label}\")\n",
    "            print(f\"   Corpus size: {len(corpus):,} documents\")\n",
    "            \n",
    "            results = self.evaluate_retrieval(corpus, query_indices, top_k_values)\n",
    "            \n",
    "            if results is None:\n",
    "                print(f\"   ‚ùå Evaluation failed\")\n",
    "                continue\n",
    "            \n",
    "            comparison_results[label] = results\n",
    "            \n",
    "            print(f\"   ‚úì Mean Rank: {results['mean_rank']:.2f}\")\n",
    "            print(f\"   ‚úì Median Rank: {results['median_rank']:.1f}\")\n",
    "            print(f\"   ‚úì Vocabulary size: {results['vocab_size']:,}\")\n",
    "            \n",
    "            for k in [5, 10, 20]:\n",
    "                if k <= len(corpus):\n",
    "                    print(f\"   ‚úì Recall@{k}: {results[f'mean_recall@{k}']:.3f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        return comparison_results\n",
    "    \n",
    "    def create_comparison_dataframe(self, comparison_results):\n",
    "        \"\"\"Convert comparison results to pandas DataFrame\"\"\"\n",
    "        \n",
    "        metrics_data = []\n",
    "        \n",
    "        for label, results in comparison_results.items():\n",
    "            row = {\n",
    "                'Corpus': label,\n",
    "                'Documents': results['doc_count'],\n",
    "                'Vocab_Size': results['vocab_size'],\n",
    "                'Mean_Rank': results['mean_rank'],\n",
    "                'Median_Rank': results['median_rank'],\n",
    "                'Std_Rank': results['std_rank'],\n",
    "                'Min_Rank': results['min_rank'],\n",
    "                'Max_Rank': results['max_rank']\n",
    "            }\n",
    "            \n",
    "            # Add precision, recall, NDCG at various K\n",
    "            for k in [1, 3, 5, 10, 20]:\n",
    "                if f'mean_precision@{k}' in results:\n",
    "                    row[f'Precision@{k}'] = results[f'mean_precision@{k}']\n",
    "                    row[f'Recall@{k}'] = results[f'mean_recall@{k}']\n",
    "                    row[f'NDCG@{k}'] = results[f'mean_ndcg@{k}']\n",
    "                    row[f'Success_Rate@{k}'] = results['success_rate'].get(k, np.nan)\n",
    "                else:\n",
    "                    row[f'Precision@{k}'] = np.nan\n",
    "                    row[f'Recall@{k}'] = np.nan\n",
    "                    row[f'NDCG@{k}'] = np.nan\n",
    "                    row[f'Success_Rate@{k}'] = np.nan\n",
    "            \n",
    "            metrics_data.append(row)\n",
    "        \n",
    "        df = pd.DataFrame(metrics_data)\n",
    "        \n",
    "        # Sort by Mean Rank (ascending - better performance first)\n",
    "        df = df.sort_values('Mean_Rank')\n",
    "        \n",
    "        return df\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    processed_dir = r'D:\\Year 5\\S1\\Information Retrieval\\StopwordProject\\khmer_stopword_project\\data\\processed'\n",
    "    output_dir = r'D:\\Year 5\\S1\\Information Retrieval\\StopwordProject\\khmer_stopword_project\\data\\ir_evaluation_enhanced'\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Files to analyze\n",
    "    files_to_analyze = [\n",
    "        ('Original', 'original_segmented_sentences.txt'),\n",
    "        ('No_All_Stopwords', 'no_all_stopwords.txt'),\n",
    "        ('No_Auxiliary_Verbs', 'no_Auxiliary_Verbs___Aspect_Markers.txt'),\n",
    "        ('No_Conjunctions', 'no_Conjunctions.txt'),\n",
    "        ('No_Determiners', 'no_Determiners_and_Quantifiers.txt'),\n",
    "        ('No_Function_Nouns', 'no_Function_Nouns.txt'),\n",
    "        ('No_Numbers', 'no_Numbers_and_Time_Expressions.txt'),\n",
    "        ('No_Particles', 'no_Particles_and_Discourse_Markers.txt'),\n",
    "        ('No_Politeness', 'no_Politeness_and_Honorifics.txt'),\n",
    "        ('No_Prepositions', 'no_Prepositions___Relational_Words.txt'),\n",
    "        ('No_Pronouns', 'no_Pronouns.txt'),\n",
    "        ('No_Questions', 'no_Question_and_Negation_Words.txt')\n",
    "    ]\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    print(\"=\"*80)\n",
    "    print(\" ENHANCED IR EVALUATION SYSTEM\")\n",
    "    print(\" With Khmer-specific preprocessing and quality filtering\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    evaluator = KhmerIREvaluator()\n",
    "    \n",
    "    # Load and preprocess all corpora\n",
    "    print(\"\\nüìÅ Loading and preprocessing corpora:\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    corpora = {}\n",
    "    for label, filename in files_to_analyze:\n",
    "        filepath = os.path.join(processed_dir, filename)\n",
    "        corpus = evaluator.load_and_preprocess_corpus(filepath, label)\n",
    "        \n",
    "        if corpus is not None:\n",
    "            corpora[label] = corpus\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  Skipping {label} - could not load/preprocess\")\n",
    "    \n",
    "    print(f\"\\n‚úì Successfully loaded {len(corpora)} corpora\")\n",
    "    \n",
    "    # Determine query set size based on smallest corpus\n",
    "    min_corpus_size = min(len(corpus) for corpus in corpora.values())\n",
    "    NUM_QUERIES = min(100, min_corpus_size - 10)\n",
    "    \n",
    "    print(f\"\\nüìä Corpus sizes for evaluation:\")\n",
    "    for label, corpus in corpora.items():\n",
    "        print(f\"  {label:<25}: {len(corpus):>6} documents\")\n",
    "    \n",
    "    print(f\"\\nüìù Using {NUM_QUERIES} queries (based on smallest corpus: {min_corpus_size} documents)\")\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    RANDOM_SEED = 42\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    \n",
    "    # Create query indices (same for all corpora)\n",
    "    query_indices = random.sample(range(min_corpus_size), NUM_QUERIES)\n",
    "    \n",
    "    # Run comprehensive evaluation\n",
    "    comparison_results = evaluator.evaluate_all_corpora(\n",
    "        corpora, \n",
    "        query_indices,\n",
    "        top_k_values=[1, 3, 5, 10, 20, 50]\n",
    "    )\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_df = evaluator.create_comparison_dataframe(comparison_results)\n",
    "    \n",
    "    print(\"\\nüìã COMPARISON RESULTS (Sorted by Mean Rank - Best First):\")\n",
    "    print(\"=\"*100)\n",
    "    pd.set_option('display.float_format', lambda x: f'{x:.3f}')\n",
    "    print(comparison_df[['Corpus', 'Documents', 'Vocab_Size', 'Mean_Rank', \n",
    "                         'Recall@5', 'Recall@10', 'Recall@20']].to_string(index=False))\n",
    "    \n",
    "    # Save results to CSV\n",
    "    comparison_csv = os.path.join(output_dir, \"ir_comparison_results_enhanced.csv\")\n",
    "    comparison_df.to_csv(comparison_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\n‚úì Full comparison results saved to: {comparison_csv}\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # VISUALIZATIONS\n",
    "    # ============================================================================\n",
    "    \n",
    "    print(\"\\nüìä Creating visualizations...\")\n",
    "    \n",
    "    # Set style\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "    \n",
    "    # 1. Mean Rank Comparison\n",
    "    fig1, ax1 = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # Sort by Mean Rank for better visualization\n",
    "    plot_df = comparison_df.sort_values('Mean_Rank', ascending=True)\n",
    "    \n",
    "    colors = plt.cm.coolwarm(np.linspace(0, 1, len(plot_df)))\n",
    "    bars = ax1.barh(range(len(plot_df)), plot_df['Mean_Rank'], color=colors)\n",
    "    \n",
    "    ax1.set_yticks(range(len(plot_df)))\n",
    "    ax1.set_yticklabels(plot_df['Corpus'])\n",
    "    ax1.set_xlabel('Mean Rank (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('IR Performance: Mean Rank Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.invert_yaxis()  # Highest rank at top\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, rank) in enumerate(zip(bars, plot_df['Mean_Rank'])):\n",
    "        ax1.text(rank + 0.5, bar.get_y() + bar.get_height()/2,\n",
    "                f'{rank:.1f}', ha='left', va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'mean_rank_comparison_enhanced.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Performance Metrics Comparison\n",
    "    fig2, axes2 = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig2.suptitle('IR Performance Metrics Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 2.1: Recall@K comparison\n",
    "    k_values = [5, 10, 20]\n",
    "    for k in k_values:\n",
    "        axes2[0, 0].plot(plot_df['Corpus'], plot_df[f'Recall@{k}'], \n",
    "                        marker='o', label=f'Recall@{k}', linewidth=2)\n",
    "    axes2[0, 0].set_xlabel('Corpus')\n",
    "    axes2[0, 0].set_ylabel('Recall Score')\n",
    "    axes2[0, 0].set_title('Recall at Different K Values', fontsize=12, fontweight='bold')\n",
    "    axes2[0, 0].legend()\n",
    "    axes2[0, 0].grid(True, alpha=0.3)\n",
    "    plt.setp(axes2[0, 0].get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Plot 2.2: NDCG@K comparison\n",
    "    for k in k_values:\n",
    "        axes2[0, 1].plot(plot_df['Corpus'], plot_df[f'NDCG@{k}'], \n",
    "                        marker='s', label=f'NDCG@{k}', linewidth=2)\n",
    "    axes2[0, 1].set_xlabel('Corpus')\n",
    "    axes2[0, 1].set_ylabel('NDCG Score')\n",
    "    axes2[0, 1].set_title('NDCG at Different K Values', fontsize=12, fontweight='bold')\n",
    "    axes2[0, 1].legend()\n",
    "    axes2[0, 1].grid(True, alpha=0.3)\n",
    "    plt.setp(axes2[0, 1].get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Plot 2.3: Vocabulary size vs Performance\n",
    "    ax2 = axes2[1, 0]\n",
    "    scatter = ax2.scatter(plot_df['Vocab_Size'], plot_df['Mean_Rank'], \n",
    "                         s=plot_df['Documents']/10, alpha=0.7,\n",
    "                         c=plot_df['Mean_Rank'], cmap='coolwarm')\n",
    "    ax2.set_xlabel('Vocabulary Size', fontsize=12)\n",
    "    ax2.set_ylabel('Mean Rank', fontsize=12)\n",
    "    ax2.set_title('Vocabulary Size vs Mean Rank', fontsize=12, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add labels for top performers\n",
    "    for i, row in plot_df.iterrows():\n",
    "        if row['Mean_Rank'] <= plot_df['Mean_Rank'].quantile(0.25):\n",
    "            ax2.annotate(row['Corpus'], \n",
    "                        (row['Vocab_Size'], row['Mean_Rank']),\n",
    "                        xytext=(5, 5), textcoords='offset points',\n",
    "                        fontsize=9, alpha=0.8)\n",
    "    \n",
    "    # Plot 2.4: Document count vs Performance\n",
    "    ax3 = axes2[1, 1]\n",
    "    scatter = ax3.scatter(plot_df['Documents'], plot_df['Mean_Rank'], \n",
    "                         s=plot_df['Vocab_Size']/10, alpha=0.7,\n",
    "                         c=plot_df['Mean_Rank'], cmap='coolwarm')\n",
    "    ax3.set_xlabel('Document Count', fontsize=12)\n",
    "    ax3.set_ylabel('Mean Rank', fontsize=12)\n",
    "    ax3.set_title('Document Count vs Mean Rank', fontsize=12, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'performance_metrics_comparison.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Heatmap of Performance Metrics\n",
    "    fig3, ax3 = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Prepare data for heatmap\n",
    "    heatmap_data = plot_df.set_index('Corpus')[['Recall@5', 'Recall@10', 'Recall@20', \n",
    "                                               'NDCG@5', 'NDCG@10', 'NDCG@20']]\n",
    "    \n",
    "    sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='YlOrRd', \n",
    "                square=True, ax=ax3, cbar_kws={'label': 'Score'})\n",
    "    ax3.set_title('Retrieval Performance Metrics Heatmap', fontsize=14, fontweight='bold')\n",
    "    plt.setp(ax3.get_xticklabels(), rotation=45, ha='right')\n",
    "    plt.setp(ax3.get_yticklabels(), rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'performance_heatmap_enhanced.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # ============================================================================\n",
    "    # STATISTICAL ANALYSIS\n",
    "    # ============================================================================\n",
    "    \n",
    "    print(\"\\nüìà Performing statistical analysis...\")\n",
    "    \n",
    "    def perform_statistical_analysis(comparison_results, baseline='Original'):\n",
    "        \"\"\"Perform statistical analysis to determine significant differences\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STATISTICAL ANALYSIS: Comparison to Baseline\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        if baseline not in comparison_results:\n",
    "            print(f\"  ‚ö†Ô∏è  Baseline '{baseline}' not found in results\")\n",
    "            return None\n",
    "        \n",
    "        baseline_ranks = np.array(comparison_results[baseline]['ranks'])\n",
    "        \n",
    "        analysis_results = []\n",
    "        \n",
    "        for label, results in comparison_results.items():\n",
    "            if label == baseline:\n",
    "                continue\n",
    "                \n",
    "            current_ranks = np.array(results['ranks'])\n",
    "            \n",
    "            # Calculate rank improvement\n",
    "            rank_improvement = baseline_ranks.mean() - current_ranks.mean()\n",
    "            percent_improvement = (rank_improvement / baseline_ranks.mean()) * 100\n",
    "            \n",
    "            # Paired t-test\n",
    "            try:\n",
    "                t_stat, p_value = stats.ttest_rel(baseline_ranks, current_ranks)\n",
    "            except:\n",
    "                t_stat, p_value = np.nan, np.nan\n",
    "            \n",
    "            # Determine significance\n",
    "            significant = p_value < 0.05 if not np.isnan(p_value) else False\n",
    "            \n",
    "            analysis_results.append({\n",
    "                'Corpus': label,\n",
    "                'Mean_Rank': current_ranks.mean(),\n",
    "                'Baseline_Rank': baseline_ranks.mean(),\n",
    "                'Rank_Improvement': rank_improvement,\n",
    "                'Percent_Improvement': percent_improvement,\n",
    "                'T_Statistic': t_stat,\n",
    "                'P_Value': p_value,\n",
    "                'Significant': significant\n",
    "            })\n",
    "        \n",
    "        # Create analysis DataFrame\n",
    "        analysis_df = pd.DataFrame(analysis_results)\n",
    "        analysis_df = analysis_df.sort_values('Percent_Improvement', ascending=False)\n",
    "        \n",
    "        print(\"\\nRank Improvement Analysis (vs Original Corpus):\")\n",
    "        print(\"-\"*100)\n",
    "        pd.set_option('display.float_format', lambda x: f'{x:.3f}')\n",
    "        print(analysis_df[['Corpus', 'Mean_Rank', 'Baseline_Rank', 'Rank_Improvement', \n",
    "                          'Percent_Improvement', 'P_Value', 'Significant']].to_string(index=False))\n",
    "        \n",
    "        return analysis_df\n",
    "    \n",
    "    # Perform statistical analysis\n",
    "    analysis_df = perform_statistical_analysis(comparison_results)\n",
    "    \n",
    "    if analysis_df is not None:\n",
    "        # Save analysis\n",
    "        analysis_csv = os.path.join(output_dir, \"statistical_analysis_enhanced.csv\")\n",
    "        analysis_df.to_csv(analysis_csv, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\n‚úì Statistical analysis saved to: {analysis_csv}\")\n",
    "        \n",
    "        # Visualize significant improvements\n",
    "        if not analysis_df.empty:\n",
    "            sig_df = analysis_df[analysis_df['Significant'] == True]\n",
    "            if not sig_df.empty:\n",
    "                fig4, ax4 = plt.subplots(figsize=(12, 6))\n",
    "                \n",
    "                colors = ['green' if imp > 0 else 'red' for imp in sig_df['Percent_Improvement']]\n",
    "                bars = ax4.barh(sig_df['Corpus'], sig_df['Percent_Improvement'], color=colors)\n",
    "                \n",
    "                ax4.set_xlabel('Percent Improvement (%)', fontsize=12, fontweight='bold')\n",
    "                ax4.set_title('Statistically Significant Improvements vs Baseline', \n",
    "                            fontsize=14, fontweight='bold')\n",
    "                ax4.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "                \n",
    "                # Add value labels\n",
    "                for bar, imp in zip(bars, sig_df['Percent_Improvement']):\n",
    "                    ax4.text(imp + (0.5 if imp >= 0 else -3), bar.get_y() + bar.get_height()/2,\n",
    "                            f'{imp:.1f}%', ha='left' if imp >= 0 else 'right', \n",
    "                            va='center', fontsize=10)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(output_dir, 'significant_improvements.png'), \n",
    "                           dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "    \n",
    "    # ============================================================================\n",
    "    # GENERATE FINAL REPORT\n",
    "    # ============================================================================\n",
    "    \n",
    "    print(\"\\nüìÑ Generating comprehensive report...\")\n",
    "    \n",
    "    def generate_comprehensive_report(comparison_df, analysis_df, output_dir):\n",
    "        \"\"\"Generate a comprehensive markdown report\"\"\"\n",
    "        \n",
    "        report_file = os.path.join(output_dir, \"ir_evaluation_report.md\")\n",
    "        \n",
    "        with open(report_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"# Information Retrieval Evaluation Report\\n\\n\")\n",
    "            f.write(\"## Executive Summary\\n\\n\")\n",
    "            \n",
    "            # Find best and worst performing corpus\n",
    "            best_corpus = comparison_df.iloc[0]\n",
    "            worst_corpus = comparison_df.iloc[-1]\n",
    "            \n",
    "            f.write(f\"### Key Findings:\\n\")\n",
    "            f.write(f\"- **Best Performing Corpus**: `{best_corpus['Corpus']}` \" \n",
    "                   f\"(Mean Rank: {best_corpus['Mean_Rank']:.2f})\\n\")\n",
    "            f.write(f\"- **Worst Performing Corpus**: `{worst_corpus['Corpus']}` \" \n",
    "                   f\"(Mean Rank: {worst_corpus['Mean_Rank']:.2f})\\n\")\n",
    "            \n",
    "            # Calculate overall improvement\n",
    "            if 'Original' in comparison_df['Corpus'].values:\n",
    "                baseline_row = comparison_df[comparison_df['Corpus'] == 'Original'].iloc[0]\n",
    "                baseline_rank = baseline_row['Mean_Rank']\n",
    "                best_rank = best_corpus['Mean_Rank']\n",
    "                improvement = ((baseline_rank - best_rank) / baseline_rank) * 100\n",
    "                \n",
    "                f.write(f\"- **Improvement over Baseline**: {improvement:.1f}% reduction in mean rank\\n\")\n",
    "            \n",
    "            f.write(f\"- **Best Recall@10**: {comparison_df['Recall@10'].max():.3f}\\n\")\n",
    "            f.write(f\"- **Worst Recall@10**: {comparison_df['Recall@10'].min():.3f}\\n\\n\")\n",
    "            \n",
    "            f.write(\"## Methodology\\n\\n\")\n",
    "            f.write(\"- **Evaluation Method**: Known-Item Retrieval\\n\")\n",
    "            f.write(f\"- **Number of Queries**: {NUM_QUERIES}\\n\")\n",
    "            f.write(f\"- **Corpora Compared**: {len(comparison_df)}\\n\")\n",
    "            f.write(\"- **Preprocessing**: Khmer-only tokens, min 2 characters\\n\")\n",
    "            f.write(\"- **Vectorization**: TF-IDF with custom Khmer tokenizer\\n\")\n",
    "            f.write(\"- **Similarity Measure**: Cosine similarity\\n\\n\")\n",
    "            \n",
    "            f.write(\"## Detailed Results\\n\\n\")\n",
    "            \n",
    "            f.write(\"### Performance Ranking (by Mean Rank)\\n\")\n",
    "            f.write(comparison_df[['Corpus', 'Documents', 'Vocab_Size', 'Mean_Rank', \n",
    "                                  'Recall@5', 'Recall@10', 'Recall@20']].to_markdown(index=False) + \"\\n\\n\")\n",
    "            \n",
    "            if analysis_df is not None:\n",
    "                f.write(\"### Statistical Significance Analysis\\n\")\n",
    "                f.write(\"Comparison of each filtered corpus against the Original baseline:\\n\")\n",
    "                f.write(analysis_df[['Corpus', 'Percent_Improvement', 'P_Value', 'Significant']].to_markdown(index=False) + \"\\n\\n\")\n",
    "                \n",
    "                # Find significantly improved groups\n",
    "                sig_groups = analysis_df[analysis_df['Significant'] == True]\n",
    "                if not sig_groups.empty:\n",
    "                    f.write(\"### Significantly Improved Groups\\n\")\n",
    "                    for _, row in sig_groups.iterrows():\n",
    "                        group_name = row['Corpus'].replace('No_', '').replace('_', ' ')\n",
    "                        f.write(f\"- **{group_name}**: {row['Percent_Improvement']:.1f}% improvement \" \n",
    "                               f\"(p={row['P_Value']:.4f})\\n\")\n",
    "                    f.write(\"\\n\")\n",
    "            \n",
    "            f.write(\"## Recommendations\\n\\n\")\n",
    "            \n",
    "            f.write(\"### For Information Retrieval Systems:\\n\")\n",
    "            f.write(f\"1. **Recommended Corpus**: Use `{best_corpus['Corpus']}` for best retrieval performance\\n\")\n",
    "            \n",
    "            if analysis_df is not None and not sig_groups.empty:\n",
    "                best_group = sig_groups.iloc[0]\n",
    "                group_name = best_group['Corpus'].replace('No_', '').replace('_', ' ')\n",
    "                f.write(f\"2. **Stopword Strategy**: Focus on removing **{group_name}** for maximum impact\\n\")\n",
    "            \n",
    "            f.write(\"3. **Quality Filtering**: Always filter single-character tokens and non-Khmer content\\n\")\n",
    "            f.write(\"4. **Evaluation Metric**: Recall@10 provides good discrimination between methods\\n\\n\")\n",
    "            \n",
    "            f.write(\"### Files Generated\\n\\n\")\n",
    "            f.write(\"1. **ir_comparison_results_enhanced.csv** - Complete performance metrics\\n\")\n",
    "            f.write(\"2. **statistical_analysis_enhanced.csv** - Statistical significance results\\n\")\n",
    "            f.write(\"3. **Visualizations** - PNG files showing performance comparisons\\n\")\n",
    "            f.write(\"4. **This report** - Comprehensive summary of findings\\n\\n\")\n",
    "            \n",
    "            f.write(\"## Conclusion\\n\\n\")\n",
    "            f.write(\"This evaluation demonstrates the impact of different stopword removal strategies \")\n",
    "            f.write(\"on Khmer information retrieval performance. The results show that \")\n",
    "            \n",
    "            if improvement > 0:\n",
    "                f.write(f\"proper stopword removal can improve retrieval performance by up to {improvement:.1f}%. \")\n",
    "            else:\n",
    "                f.write(\"careful selection of stopwords is crucial for optimal performance. \")\n",
    "            \n",
    "            f.write(\"The findings can guide the development of more effective Khmer IR systems.\\n\")\n",
    "        \n",
    "        print(f\"‚úì Comprehensive report generated: {report_file}\")\n",
    "        \n",
    "        # Print summary to console\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"EVALUATION SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"üìä Corpora evaluated: {len(comparison_df)}\")\n",
    "        print(f\"üèÜ Best performer: {best_corpus['Corpus']} (Mean Rank: {best_corpus['Mean_Rank']:.2f})\")\n",
    "        \n",
    "        if 'improvement' in locals():\n",
    "            print(f\"üìà Improvement over baseline: {improvement:.1f}%\")\n",
    "        \n",
    "        if analysis_df is not None:\n",
    "            sig_count = len(analysis_df[analysis_df['Significant'] == True])\n",
    "            print(f\"‚úÖ Statistically significant improvements: {sig_count} groups\")\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    # Generate final report\n",
    "    generate_comprehensive_report(comparison_df, analysis_df, output_dir)\n",
    "    \n",
    "    print(\"\\n‚úÖ IR EVALUATION COMPLETE\")\n",
    "    print(f\"üìÅ Results saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE IR EVALUATION WITH DEBUGGING\n",
      "================================================================================\n",
      "\n",
      "üìÅ LOADING CORPORA:\n",
      "\n",
      "üìÅ Loading: Original\n",
      "  Raw documents: 58,409\n",
      "  After cleaning: 58,398\n",
      "  Removed 16 very short documents (<3 tokens)\n",
      "\n",
      "üîç DIAGNOSING CORPUS: Original\n",
      "============================================================\n",
      "  Document count: 58,382\n",
      "  Avg document length: 29.0 tokens\n",
      "  Median document length: 25.0 tokens\n",
      "  Min/Max length: 3 / 535 tokens\n",
      "\n",
      "  Token statistics:\n",
      "  Total tokens: 1,691,794\n",
      "  Unique tokens: 45,453\n",
      "\n",
      "  Token length distribution:\n",
      "    2 chars: 197,607 (11.7%)\n",
      "    3 chars: 443,103 (26.2%)\n",
      "    4 chars: 191,029 (11.3%)\n",
      "    5 chars: 269,176 (15.9%)\n",
      "    6 chars: 202,084 (11.9%)\n",
      "    7 chars: 142,929 (8.4%)\n",
      "    8 chars: 86,120 (5.1%)\n",
      "    9 chars: 54,400 (3.2%)\n",
      "    10 chars: 40,655 (2.4%)\n",
      "    11 chars: 26,927 (1.6%)\n",
      "\n",
      "  Top 20 most common tokens:\n",
      "    ‚úì '·ûî·û∂·ûì' (len=3): 39,983\n",
      "    ‚úì '·ûì·û∑·ûÑ' (len=3): 36,396\n",
      "    ‚úì '·ûä·üÇ·ûõ' (len=3): 29,783\n",
      "    ‚úì '·ûò·û∂·ûì' (len=3): 27,127\n",
      "    ‚úì '·ûá·û∂' (len=2): 22,253\n",
      "    ‚úì '·ûì·üÖ' (len=2): 21,807\n",
      "    ‚úì '·ûê·û∂' (len=2): 20,952\n",
      "    ‚úì '·ûö·ûî·ûü·üã' (len=4): 20,921\n",
      "    ‚úì '·ûÄ·üí·ûì·ûª·ûÑ' (len=5): 19,048\n",
      "    ‚úì '·ûì·üÅ·üá' (len=3): 17,111\n",
      "    ‚úì '·ûì·üÉ' (len=2): 14,390\n",
      "    ‚úì '·ûä·üÑ·ûô' (len=3): 13,524\n",
      "    ‚úì '·ûÜ·üí·ûì·û∂·üÜ' (len=5): 12,145\n",
      "    ‚úì '·ûõ·üÑ·ûÄ' (len=3): 11,358\n",
      "    ‚úì '·û†·ûæ·ûô' (len=3): 10,636\n",
      "    ‚úì '·ûî·üí·ûö·ûë·üÅ·ûü' (len=6): 10,359\n",
      "    ‚úì '·ûò·û∑·ûì' (len=3): 10,325\n",
      "    ‚úì '·ûñ·û∏' (len=2): 9,924\n",
      "    ‚úì '·ûì·üÑ·üá' (len=3): 9,239\n",
      "    ‚úì '·ûì·ûπ·ûÑ' (len=3): 8,682\n",
      "\n",
      "  Document frequency analysis:\n",
      "  Avg doc frequency: 32.16\n",
      "  Median doc frequency: 1.00\n",
      "  Tokens in only 1 document: 23,778\n",
      "\n",
      "  Sparsity analysis:\n",
      "  Type-Token Ratio: 0.027\n",
      "  ‚úì LOW SPARSITY - good token repetition\n",
      "\n",
      "  Sample document analysis (first 3):\n",
      "  Doc 1: 45 tokens\n",
      "    Sample: ·ûü·üÅ·ûÖ·ûÄ·üí·ûè·û∏·ûê·üí·ûõ·üÇ·ûÑ·ûÄ·û∂·ûö·ûé·üç ·ûñ·û∂·ûÄ·üã·ûñ·üê·ûì·üí·ûí ·ûì·ûπ·ûÑ ·ûñ·üê·ûè·üå·ûò·û∂·ûì ·ûì·üÅ·üá ·ûî·û∂·ûì ·û≤·üí·ûô·ûä·ûπ·ûÑ ·ûê·û∂ ·ûõ·üÑ·ûÄ ·ûì·ûπ·ûÑ...\n",
      "  Doc 2: 24 tokens\n",
      "    Sample: ·ûï·üí·ûè·ûì·üí·ûë·û∂·ûë·üÑ·ûü ·ûä·û∂·ûÄ·üã ·ûñ·ûì·üí·ûí·ûì·û∂·ûÄ·û∂·ûö ·ûä·üÑ·ûô ·ûñ·üí·ûô·ûΩ·ûö ·ûë·üÑ·ûü ·ûö·û∫ ·ûò·û∑·ûì ·ûñ·üí·ûô·ûΩ·ûö ·ûë·üÑ·ûü...\n",
      "  Doc 3: 14 tokens\n",
      "    Sample: ·ûì·üÖ ·ûÜ·üí·ûì·û∂·üÜ ·ûó·û∂·ûÇ ·ûÄ·ûé·üí·ûä·û∂·ûõ ·ûì·üÉ ·ûÖ·ûÄ·üí·ûö·ûó·ûñ ·ûî·û∂·ûì ·ûî·üÇ·ûÄ·ûî·û∂·ûÄ·üã ·ûî·ûÑ·üí·ûÄ·ûæ·ûè ·ûî·û∂·ûì·ûá·û∂...\n",
      "\n",
      "üìÅ Loading: No_Prepositions\n",
      "  Raw documents: 58,409\n",
      "  After cleaning: 58,398\n",
      "  Removed 22 very short documents (<3 tokens)\n",
      "\n",
      "üîç DIAGNOSING CORPUS: No_Prepositions\n",
      "============================================================\n",
      "  Document count: 58,376\n",
      "  Avg document length: 27.0 tokens\n",
      "  Median document length: 24.0 tokens\n",
      "  Min/Max length: 3 / 519 tokens\n",
      "\n",
      "  Token statistics:\n",
      "  Total tokens: 1,576,660\n",
      "  Unique tokens: 45,423\n",
      "\n",
      "  Token length distribution:\n",
      "    2 chars: 160,961 (10.2%)\n",
      "    3 chars: 421,433 (26.7%)\n",
      "    4 chars: 166,247 (10.5%)\n",
      "    5 chars: 243,710 (15.5%)\n",
      "    6 chars: 198,931 (12.6%)\n",
      "    7 chars: 142,334 (9.0%)\n",
      "    8 chars: 85,115 (5.4%)\n",
      "    9 chars: 52,585 (3.3%)\n",
      "    10 chars: 40,654 (2.6%)\n",
      "    11 chars: 26,926 (1.7%)\n",
      "\n",
      "  Top 20 most common tokens:\n",
      "    ‚úì '·ûî·û∂·ûì' (len=3): 39,983\n",
      "    ‚úì '·ûì·û∑·ûÑ' (len=3): 36,396\n",
      "    ‚úì '·ûä·üÇ·ûõ' (len=3): 29,783\n",
      "    ‚úì '·ûò·û∂·ûì' (len=3): 27,127\n",
      "    ‚úì '·ûì·üÖ' (len=2): 21,807\n",
      "    ‚úì '·ûê·û∂' (len=2): 20,952\n",
      "    ‚úì '·ûì·üÅ·üá' (len=3): 17,111\n",
      "    ‚úì '·ûä·üÑ·ûô' (len=3): 13,524\n",
      "    ‚úì '·ûÜ·üí·ûì·û∂·üÜ' (len=5): 12,143\n",
      "    ‚úì '·ûõ·üÑ·ûÄ' (len=3): 11,357\n",
      "    ‚úì '·û†·ûæ·ûô' (len=3): 10,636\n",
      "    ‚úì '·ûî·üí·ûö·ûë·üÅ·ûü' (len=6): 10,359\n",
      "    ‚úì '·ûò·û∑·ûì' (len=3): 10,325\n",
      "    ‚úì '·ûñ·û∏' (len=2): 9,924\n",
      "    ‚úì '·ûì·üÑ·üá' (len=3): 9,239\n",
      "    ‚úì '·ûì·ûπ·ûÑ' (len=3): 8,682\n",
      "    ‚úì '·ûë·üÖ' (len=2): 8,665\n",
      "    ‚úì '·ûò·ûΩ·ûô' (len=3): 8,299\n",
      "    ‚úì '·ûÄ·ûò·üí·ûñ·ûª·ûá·û∂' (len=7): 8,092\n",
      "    ‚úì '·ûì·üÖ·ûÄ·üí·ûì·ûª·ûÑ' (len=7): 7,575\n",
      "\n",
      "  Document frequency analysis:\n",
      "  Avg doc frequency: 30.11\n",
      "  Median doc frequency: 1.00\n",
      "  Tokens in only 1 document: 23,777\n",
      "\n",
      "  Sparsity analysis:\n",
      "  Type-Token Ratio: 0.029\n",
      "  ‚úì LOW SPARSITY - good token repetition\n",
      "\n",
      "  Sample document analysis (first 3):\n",
      "  Doc 1: 41 tokens\n",
      "    Sample: ·ûü·üÅ·ûÖ·ûÄ·üí·ûè·û∏·ûê·üí·ûõ·üÇ·ûÑ·ûÄ·û∂·ûö·ûé·üç ·ûñ·û∂·ûÄ·üã·ûñ·üê·ûì·üí·ûí ·ûì·ûπ·ûÑ ·ûñ·üê·ûè·üå·ûò·û∂·ûì ·ûì·üÅ·üá ·ûî·û∂·ûì ·û≤·üí·ûô·ûä·ûπ·ûÑ ·ûê·û∂ ·ûõ·üÑ·ûÄ ·ûì·ûπ·ûÑ...\n",
      "  Doc 2: 23 tokens\n",
      "    Sample: ·ûï·üí·ûè·ûì·üí·ûë·û∂·ûë·üÑ·ûü ·ûä·û∂·ûÄ·üã ·ûñ·ûì·üí·ûí·ûì·û∂·ûÄ·û∂·ûö ·ûä·üÑ·ûô ·ûñ·üí·ûô·ûΩ·ûö ·ûë·üÑ·ûü ·ûö·û∫ ·ûò·û∑·ûì ·ûñ·üí·ûô·ûΩ·ûö ·ûë·üÑ·ûü...\n",
      "  Doc 3: 13 tokens\n",
      "    Sample: ·ûì·üÖ ·ûÜ·üí·ûì·û∂·üÜ ·ûó·û∂·ûÇ ·ûÄ·ûé·üí·ûä·û∂·ûõ ·ûÖ·ûÄ·üí·ûö·ûó·ûñ ·ûî·û∂·ûì ·ûî·üÇ·ûÄ·ûî·û∂·ûÄ·üã ·ûî·ûÑ·üí·ûÄ·ûæ·ûè ·ûî·û∂·ûì·ûá·û∂ ·ûö·û∂·ûá·û∂·ûé·û∂·ûÖ·ûÄ·üí·ûö...\n",
      "\n",
      "üìÅ Loading: No_Auxiliary_Verbs\n",
      "  Raw documents: 58,409\n",
      "  After cleaning: 58,398\n",
      "  Removed 82 very short documents (<3 tokens)\n",
      "\n",
      "üîç DIAGNOSING CORPUS: No_Auxiliary_Verbs\n",
      "============================================================\n",
      "  Document count: 58,316\n",
      "  Avg document length: 27.9 tokens\n",
      "  Median document length: 24.0 tokens\n",
      "  Min/Max length: 3 / 534 tokens\n",
      "\n",
      "  Token statistics:\n",
      "  Total tokens: 1,624,582\n",
      "  Unique tokens: 45,435\n",
      "\n",
      "  Token length distribution:\n",
      "    2 chars: 197,607 (12.2%)\n",
      "    3 chars: 387,805 (23.9%)\n",
      "    4 chars: 190,813 (11.7%)\n",
      "    5 chars: 266,277 (16.4%)\n",
      "    6 chars: 201,600 (12.4%)\n",
      "    7 chars: 142,533 (8.8%)\n",
      "    8 chars: 78,427 (4.8%)\n",
      "    9 chars: 54,174 (3.3%)\n",
      "    10 chars: 40,655 (2.5%)\n",
      "    11 chars: 26,927 (1.7%)\n",
      "\n",
      "  Top 20 most common tokens:\n",
      "    ‚úì '·ûì·û∑·ûÑ' (len=3): 36,396\n",
      "    ‚úì '·ûä·üÇ·ûõ' (len=3): 29,783\n",
      "    ‚úì '·ûò·û∂·ûì' (len=3): 27,126\n",
      "    ‚úì '·ûá·û∂' (len=2): 22,253\n",
      "    ‚úì '·ûì·üÖ' (len=2): 21,807\n",
      "    ‚úì '·ûê·û∂' (len=2): 20,952\n",
      "    ‚úì '·ûö·ûî·ûü·üã' (len=4): 20,921\n",
      "    ‚úì '·ûÄ·üí·ûì·ûª·ûÑ' (len=5): 19,048\n",
      "    ‚úì '·ûì·üÅ·üá' (len=3): 17,111\n",
      "    ‚úì '·ûì·üÉ' (len=2): 14,390\n",
      "    ‚úì '·ûä·üÑ·ûô' (len=3): 13,520\n",
      "    ‚úì '·ûÜ·üí·ûì·û∂·üÜ' (len=5): 12,145\n",
      "    ‚úì '·ûõ·üÑ·ûÄ' (len=3): 11,358\n",
      "    ‚úì '·û†·ûæ·ûô' (len=3): 10,636\n",
      "    ‚úì '·ûî·üí·ûö·ûë·üÅ·ûü' (len=6): 10,359\n",
      "    ‚úì '·ûò·û∑·ûì' (len=3): 10,325\n",
      "    ‚úì '·ûñ·û∏' (len=2): 9,924\n",
      "    ‚úì '·ûì·üÑ·üá' (len=3): 9,239\n",
      "    ‚úì '·ûë·üÖ' (len=2): 8,665\n",
      "    ‚úì '·ûò·ûΩ·ûô' (len=3): 8,299\n",
      "\n",
      "  Document frequency analysis:\n",
      "  Avg doc frequency: 31.05\n",
      "  Median doc frequency: 1.00\n",
      "  Tokens in only 1 document: 23,778\n",
      "\n",
      "  Sparsity analysis:\n",
      "  Type-Token Ratio: 0.028\n",
      "  ‚úì LOW SPARSITY - good token repetition\n",
      "\n",
      "  Sample document analysis (first 3):\n",
      "  Doc 1: 42 tokens\n",
      "    Sample: ·ûü·üÅ·ûÖ·ûÄ·üí·ûè·û∏·ûê·üí·ûõ·üÇ·ûÑ·ûÄ·û∂·ûö·ûé·üç ·ûñ·û∂·ûÄ·üã·ûñ·üê·ûì·üí·ûí ·ûñ·üê·ûè·üå·ûò·û∂·ûì ·ûì·üÅ·üá ·û≤·üí·ûô·ûä·ûπ·ûÑ ·ûê·û∂ ·ûõ·üÑ·ûÄ ·ûë·üÖ ·ûî·üÜ·ûñ·üÅ·ûâ ·ûë·ûü·üí·ûü·ûì·ûÄ·û∑·ûÖ·üí·ûÖ...\n",
      "  Doc 2: 23 tokens\n",
      "    Sample: ·ûï·üí·ûè·ûì·üí·ûë·û∂·ûë·üÑ·ûü ·ûä·û∂·ûÄ·üã ·ûñ·ûì·üí·ûí·ûì·û∂·ûÄ·û∂·ûö ·ûä·üÑ·ûô ·ûñ·üí·ûô·ûΩ·ûö ·ûë·üÑ·ûü ·ûö·û∫ ·ûò·û∑·ûì ·ûñ·üí·ûô·ûΩ·ûö ·ûë·üÑ·ûü...\n",
      "  Doc 3: 13 tokens\n",
      "    Sample: ·ûì·üÖ ·ûÜ·üí·ûì·û∂·üÜ ·ûó·û∂·ûÇ ·ûÄ·ûé·üí·ûä·û∂·ûõ ·ûì·üÉ ·ûÖ·ûÄ·üí·ûö·ûó·ûñ ·ûî·üÇ·ûÄ·ûî·û∂·ûÄ·üã ·ûî·ûÑ·üí·ûÄ·ûæ·ûè ·ûî·û∂·ûì·ûá·û∂ ·ûö·û∂·ûá·û∂·ûé·û∂·ûÖ·ûÄ·üí·ûö...\n",
      "\n",
      "üìÅ Loading: No_Conjunctions\n",
      "  Raw documents: 58,409\n",
      "  After cleaning: 58,398\n",
      "  Removed 36 very short documents (<3 tokens)\n",
      "\n",
      "üîç DIAGNOSING CORPUS: No_Conjunctions\n",
      "============================================================\n",
      "  Document count: 58,362\n",
      "  Avg document length: 27.2 tokens\n",
      "  Median document length: 24.0 tokens\n",
      "  Min/Max length: 3 / 511 tokens\n",
      "\n",
      "  Token statistics:\n",
      "  Total tokens: 1,585,475\n",
      "  Unique tokens: 45,413\n",
      "\n",
      "  Token length distribution:\n",
      "    2 chars: 178,689 (11.3%)\n",
      "    3 chars: 374,846 (23.6%)\n",
      "    4 chars: 189,686 (12.0%)\n",
      "    5 chars: 267,661 (16.9%)\n",
      "    6 chars: 192,046 (12.1%)\n",
      "    7 chars: 138,159 (8.7%)\n",
      "    8 chars: 85,282 (5.4%)\n",
      "    9 chars: 54,151 (3.4%)\n",
      "    10 chars: 40,382 (2.5%)\n",
      "    11 chars: 26,924 (1.7%)\n",
      "\n",
      "  Top 20 most common tokens:\n",
      "    ‚úì '·ûî·û∂·ûì' (len=3): 39,983\n",
      "    ‚úì '·ûò·û∂·ûì' (len=3): 27,127\n",
      "    ‚úì '·ûá·û∂' (len=2): 22,253\n",
      "    ‚úì '·ûì·üÖ' (len=2): 21,807\n",
      "    ‚úì '·ûê·û∂' (len=2): 20,952\n",
      "    ‚úì '·ûö·ûî·ûü·üã' (len=4): 20,921\n",
      "    ‚úì '·ûÄ·üí·ûì·ûª·ûÑ' (len=5): 19,048\n",
      "    ‚úì '·ûì·üÅ·üá' (len=3): 17,111\n",
      "    ‚úì '·ûì·üÉ' (len=2): 14,390\n",
      "    ‚úì '·ûä·üÑ·ûô' (len=3): 13,524\n",
      "    ‚úì '·ûÜ·üí·ûì·û∂·üÜ' (len=5): 12,145\n",
      "    ‚úì '·ûõ·üÑ·ûÄ' (len=3): 11,358\n",
      "    ‚úì '·û†·ûæ·ûô' (len=3): 10,636\n",
      "    ‚úì '·ûî·üí·ûö·ûë·üÅ·ûü' (len=6): 10,359\n",
      "    ‚úì '·ûò·û∑·ûì' (len=3): 10,325\n",
      "    ‚úì '·ûì·üÑ·üá' (len=3): 9,239\n",
      "    ‚úì '·ûì·ûπ·ûÑ' (len=3): 8,682\n",
      "    ‚úì '·ûë·üÖ' (len=2): 8,665\n",
      "    ‚úì '·ûò·ûΩ·ûô' (len=3): 8,299\n",
      "    ‚úì '·ûÄ·ûò·üí·ûñ·ûª·ûá·û∂' (len=7): 8,092\n",
      "\n",
      "  Document frequency analysis:\n",
      "  Avg doc frequency: 30.36\n",
      "  Median doc frequency: 1.00\n",
      "  Tokens in only 1 document: 23,779\n",
      "\n",
      "  Sparsity analysis:\n",
      "  Type-Token Ratio: 0.029\n",
      "  ‚úì LOW SPARSITY - good token repetition\n",
      "\n",
      "  Sample document analysis (first 3):\n",
      "  Doc 1: 45 tokens\n",
      "    Sample: ·ûü·üÅ·ûÖ·ûÄ·üí·ûè·û∏·ûê·üí·ûõ·üÇ·ûÑ·ûÄ·û∂·ûö·ûé·üç ·ûñ·û∂·ûÄ·üã·ûñ·üê·ûì·üí·ûí ·ûì·ûπ·ûÑ ·ûñ·üê·ûè·üå·ûò·û∂·ûì ·ûì·üÅ·üá ·ûî·û∂·ûì ·û≤·üí·ûô·ûä·ûπ·ûÑ ·ûê·û∂ ·ûõ·üÑ·ûÄ ·ûì·ûπ·ûÑ...\n",
      "  Doc 2: 23 tokens\n",
      "    Sample: ·ûï·üí·ûè·ûì·üí·ûë·û∂·ûë·üÑ·ûü ·ûä·û∂·ûÄ·üã ·ûñ·ûì·üí·ûí·ûì·û∂·ûÄ·û∂·ûö ·ûä·üÑ·ûô ·ûñ·üí·ûô·ûΩ·ûö ·ûë·üÑ·ûü ·ûö·û∫ ·ûò·û∑·ûì ·ûñ·üí·ûô·ûΩ·ûö ·ûë·üÑ·ûü...\n",
      "  Doc 3: 14 tokens\n",
      "    Sample: ·ûì·üÖ ·ûÜ·üí·ûì·û∂·üÜ ·ûó·û∂·ûÇ ·ûÄ·ûé·üí·ûä·û∂·ûõ ·ûì·üÉ ·ûÖ·ûÄ·üí·ûö·ûó·ûñ ·ûî·û∂·ûì ·ûî·üÇ·ûÄ·ûî·û∂·ûÄ·üã ·ûî·ûÑ·üí·ûÄ·ûæ·ûè ·ûî·û∂·ûì·ûá·û∂...\n",
      "\n",
      "üìÅ Loading: No_Determiners\n",
      "  Raw documents: 58,409\n",
      "  After cleaning: 58,398\n",
      "  Removed 17 very short documents (<3 tokens)\n",
      "\n",
      "üîç DIAGNOSING CORPUS: No_Determiners\n",
      "============================================================\n",
      "  Document count: 58,381\n",
      "  Avg document length: 28.2 tokens\n",
      "  Median document length: 25.0 tokens\n",
      "  Min/Max length: 3 / 532 tokens\n",
      "\n",
      "  Token statistics:\n",
      "  Total tokens: 1,646,989\n",
      "  Unique tokens: 45,438\n",
      "\n",
      "  Token length distribution:\n",
      "    2 chars: 197,607 (12.0%)\n",
      "    3 chars: 416,753 (25.3%)\n",
      "    4 chars: 186,013 (11.3%)\n",
      "    5 chars: 260,276 (15.8%)\n",
      "    6 chars: 201,227 (12.2%)\n",
      "    7 chars: 140,643 (8.5%)\n",
      "    8 chars: 84,860 (5.2%)\n",
      "    9 chars: 54,400 (3.3%)\n",
      "    10 chars: 40,655 (2.5%)\n",
      "    11 chars: 26,791 (1.6%)\n",
      "\n",
      "  Top 20 most common tokens:\n",
      "    ‚úì '·ûî·û∂·ûì' (len=3): 39,983\n",
      "    ‚úì '·ûì·û∑·ûÑ' (len=3): 36,396\n",
      "    ‚úì '·ûä·üÇ·ûõ' (len=3): 29,783\n",
      "    ‚úì '·ûò·û∂·ûì' (len=3): 27,127\n",
      "    ‚úì '·ûá·û∂' (len=2): 22,253\n",
      "    ‚úì '·ûì·üÖ' (len=2): 21,807\n",
      "    ‚úì '·ûê·û∂' (len=2): 20,952\n",
      "    ‚úì '·ûö·ûî·ûü·üã' (len=4): 20,921\n",
      "    ‚úì '·ûÄ·üí·ûì·ûª·ûÑ' (len=5): 19,048\n",
      "    ‚úì '·ûì·üÉ' (len=2): 14,390\n",
      "    ‚úì '·ûä·üÑ·ûô' (len=3): 13,524\n",
      "    ‚úì '·ûÜ·üí·ûì·û∂·üÜ' (len=5): 12,145\n",
      "    ‚úì '·ûõ·üÑ·ûÄ' (len=3): 11,358\n",
      "    ‚úì '·û†·ûæ·ûô' (len=3): 10,636\n",
      "    ‚úì '·ûî·üí·ûö·ûë·üÅ·ûü' (len=6): 10,359\n",
      "    ‚úì '·ûò·û∑·ûì' (len=3): 10,325\n",
      "    ‚úì '·ûñ·û∏' (len=2): 9,924\n",
      "    ‚úì '·ûì·ûπ·ûÑ' (len=3): 8,682\n",
      "    ‚úì '·ûë·üÖ' (len=2): 8,665\n",
      "    ‚úì '·ûò·ûΩ·ûô' (len=3): 8,299\n",
      "\n",
      "  Document frequency analysis:\n",
      "  Avg doc frequency: 31.33\n",
      "  Median doc frequency: 1.00\n",
      "  Tokens in only 1 document: 23,778\n",
      "\n",
      "  Sparsity analysis:\n",
      "  Type-Token Ratio: 0.028\n",
      "  ‚úì LOW SPARSITY - good token repetition\n",
      "\n",
      "  Sample document analysis (first 3):\n",
      "  Doc 1: 42 tokens\n",
      "    Sample: ·ûü·üÅ·ûÖ·ûÄ·üí·ûè·û∏·ûê·üí·ûõ·üÇ·ûÑ·ûÄ·û∂·ûö·ûé·üç ·ûñ·û∂·ûÄ·üã·ûñ·üê·ûì·üí·ûí ·ûì·ûπ·ûÑ ·ûñ·üê·ûè·üå·ûò·û∂·ûì ·ûî·û∂·ûì ·û≤·üí·ûô·ûä·ûπ·ûÑ ·ûê·û∂ ·ûõ·üÑ·ûÄ ·ûì·ûπ·ûÑ ·ûë·üÖ...\n",
      "  Doc 2: 23 tokens\n",
      "    Sample: ·ûï·üí·ûè·ûì·üí·ûë·û∂·ûë·üÑ·ûü ·ûä·û∂·ûÄ·üã ·ûñ·ûì·üí·ûí·ûì·û∂·ûÄ·û∂·ûö ·ûä·üÑ·ûô ·ûñ·üí·ûô·ûΩ·ûö ·ûë·üÑ·ûü ·ûö·û∫ ·ûò·û∑·ûì ·ûñ·üí·ûô·ûΩ·ûö ·ûë·üÑ·ûü...\n",
      "  Doc 3: 14 tokens\n",
      "    Sample: ·ûì·üÖ ·ûÜ·üí·ûì·û∂·üÜ ·ûó·û∂·ûÇ ·ûÄ·ûé·üí·ûä·û∂·ûõ ·ûì·üÉ ·ûÖ·ûÄ·üí·ûö·ûó·ûñ ·ûî·û∂·ûì ·ûî·üÇ·ûÄ·ûî·û∂·ûÄ·üã ·ûî·ûÑ·üí·ûÄ·ûæ·ûè ·ûî·û∂·ûì·ûá·û∂...\n",
      "\n",
      "üìÅ Loading: No_Function_Nouns\n",
      "  Raw documents: 58,409\n",
      "  After cleaning: 58,398\n",
      "  Removed 21 very short documents (<3 tokens)\n",
      "\n",
      "üîç DIAGNOSING CORPUS: No_Function_Nouns\n",
      "============================================================\n",
      "  Document count: 58,377\n",
      "  Avg document length: 28.2 tokens\n",
      "  Median document length: 25.0 tokens\n",
      "  Min/Max length: 3 / 526 tokens\n",
      "\n",
      "  Token statistics:\n",
      "  Total tokens: 1,643,500\n",
      "  Unique tokens: 45,415\n",
      "\n",
      "  Token length distribution:\n",
      "    2 chars: 197,607 (12.0%)\n",
      "    3 chars: 411,741 (25.1%)\n",
      "    4 chars: 188,504 (11.5%)\n",
      "    5 chars: 264,944 (16.1%)\n",
      "    6 chars: 198,205 (12.1%)\n",
      "    7 chars: 138,944 (8.5%)\n",
      "    8 chars: 84,917 (5.2%)\n",
      "    9 chars: 53,899 (3.3%)\n",
      "    10 chars: 40,048 (2.4%)\n",
      "    11 chars: 26,927 (1.6%)\n",
      "\n",
      "  Top 20 most common tokens:\n",
      "    ‚úì '·ûî·û∂·ûì' (len=3): 39,983\n",
      "    ‚úì '·ûì·û∑·ûÑ' (len=3): 36,396\n",
      "    ‚úì '·ûä·üÇ·ûõ' (len=3): 29,783\n",
      "    ‚úì '·ûá·û∂' (len=2): 22,253\n",
      "    ‚úì '·ûì·üÖ' (len=2): 21,807\n",
      "    ‚úì '·ûê·û∂' (len=2): 20,952\n",
      "    ‚úì '·ûö·ûî·ûü·üã' (len=4): 20,921\n",
      "    ‚úì '·ûÄ·üí·ûì·ûª·ûÑ' (len=5): 19,048\n",
      "    ‚úì '·ûì·üÅ·üá' (len=3): 17,111\n",
      "    ‚úì '·ûì·üÉ' (len=2): 14,390\n",
      "    ‚úì '·ûä·üÑ·ûô' (len=3): 13,524\n",
      "    ‚úì '·ûÜ·üí·ûì·û∂·üÜ' (len=5): 12,145\n",
      "    ‚úì '·ûõ·üÑ·ûÄ' (len=3): 11,358\n",
      "    ‚úì '·û†·ûæ·ûô' (len=3): 10,636\n",
      "    ‚úì '·ûî·üí·ûö·ûë·üÅ·ûü' (len=6): 10,359\n",
      "    ‚úì '·ûò·û∑·ûì' (len=3): 10,325\n",
      "    ‚úì '·ûñ·û∏' (len=2): 9,924\n",
      "    ‚úì '·ûì·üÑ·üá' (len=3): 9,239\n",
      "    ‚úì '·ûì·ûπ·ûÑ' (len=3): 8,682\n",
      "    ‚úì '·ûë·üÖ' (len=2): 8,665\n",
      "\n",
      "  Document frequency analysis:\n",
      "  Avg doc frequency: 31.33\n",
      "  Median doc frequency: 1.00\n",
      "  Tokens in only 1 document: 23,778\n",
      "\n",
      "  Sparsity analysis:\n",
      "  Type-Token Ratio: 0.028\n",
      "  ‚úì LOW SPARSITY - good token repetition\n",
      "\n",
      "  Sample document analysis (first 3):\n",
      "  Doc 1: 42 tokens\n",
      "    Sample: ·ûü·üÅ·ûÖ·ûÄ·üí·ûè·û∏·ûê·üí·ûõ·üÇ·ûÑ·ûÄ·û∂·ûö·ûé·üç ·ûñ·û∂·ûÄ·üã·ûñ·üê·ûì·üí·ûí ·ûì·ûπ·ûÑ ·ûñ·üê·ûè·üå·ûò·û∂·ûì ·ûì·üÅ·üá ·ûî·û∂·ûì ·û≤·üí·ûô·ûä·ûπ·ûÑ ·ûê·û∂ ·ûõ·üÑ·ûÄ ·ûì·ûπ·ûÑ...\n",
      "  Doc 2: 24 tokens\n",
      "    Sample: ·ûï·üí·ûè·ûì·üí·ûë·û∂·ûë·üÑ·ûü ·ûä·û∂·ûÄ·üã ·ûñ·ûì·üí·ûí·ûì·û∂·ûÄ·û∂·ûö ·ûä·üÑ·ûô ·ûñ·üí·ûô·ûΩ·ûö ·ûë·üÑ·ûü ·ûö·û∫ ·ûò·û∑·ûì ·ûñ·üí·ûô·ûΩ·ûö ·ûë·üÑ·ûü...\n",
      "  Doc 3: 14 tokens\n",
      "    Sample: ·ûì·üÖ ·ûÜ·üí·ûì·û∂·üÜ ·ûó·û∂·ûÇ ·ûÄ·ûé·üí·ûä·û∂·ûõ ·ûì·üÉ ·ûÖ·ûÄ·üí·ûö·ûó·ûñ ·ûî·û∂·ûì ·ûî·üÇ·ûÄ·ûî·û∂·ûÄ·üã ·ûî·ûÑ·üí·ûÄ·ûæ·ûè ·ûî·û∂·ûì·ûá·û∂...\n",
      "\n",
      "üìÅ Loading: No_Numbers\n",
      "  Raw documents: 58,409\n",
      "  After cleaning: 58,398\n",
      "  Removed 17 very short documents (<3 tokens)\n",
      "\n",
      "üîç DIAGNOSING CORPUS: No_Numbers\n",
      "============================================================\n",
      "  Document count: 58,381\n",
      "  Avg document length: 28.7 tokens\n",
      "  Median document length: 25.0 tokens\n",
      "  Min/Max length: 3 / 535 tokens\n",
      "\n",
      "  Token statistics:\n",
      "  Total tokens: 1,673,083\n",
      "  Unique tokens: 45,442\n",
      "\n",
      "  Token length distribution:\n",
      "    2 chars: 197,607 (11.8%)\n",
      "    3 chars: 430,567 (25.7%)\n",
      "    4 chars: 190,731 (11.4%)\n",
      "    5 chars: 265,947 (15.9%)\n",
      "    6 chars: 200,512 (12.0%)\n",
      "    7 chars: 142,719 (8.5%)\n",
      "    8 chars: 85,980 (5.1%)\n",
      "    9 chars: 54,400 (3.3%)\n",
      "    10 chars: 39,929 (2.4%)\n",
      "    11 chars: 26,927 (1.6%)\n",
      "\n",
      "  Top 20 most common tokens:\n",
      "    ‚úì '·ûî·û∂·ûì' (len=3): 39,983\n",
      "    ‚úì '·ûì·û∑·ûÑ' (len=3): 36,396\n",
      "    ‚úì '·ûä·üÇ·ûõ' (len=3): 29,783\n",
      "    ‚úì '·ûò·û∂·ûì' (len=3): 27,127\n",
      "    ‚úì '·ûá·û∂' (len=2): 22,253\n",
      "    ‚úì '·ûì·üÖ' (len=2): 21,807\n",
      "    ‚úì '·ûê·û∂' (len=2): 20,952\n",
      "    ‚úì '·ûö·ûî·ûü·üã' (len=4): 20,921\n",
      "    ‚úì '·ûÄ·üí·ûì·ûª·ûÑ' (len=5): 19,048\n",
      "    ‚úì '·ûì·üÅ·üá' (len=3): 17,111\n",
      "    ‚úì '·ûì·üÉ' (len=2): 14,390\n",
      "    ‚úì '·ûä·üÑ·ûô' (len=3): 13,524\n",
      "    ‚úì '·ûÜ·üí·ûì·û∂·üÜ' (len=5): 12,145\n",
      "    ‚úì '·ûõ·üÑ·ûÄ' (len=3): 11,358\n",
      "    ‚úì '·û†·ûæ·ûô' (len=3): 10,636\n",
      "    ‚úì '·ûî·üí·ûö·ûë·üÅ·ûü' (len=6): 10,359\n",
      "    ‚úì '·ûò·û∑·ûì' (len=3): 10,325\n",
      "    ‚úì '·ûñ·û∏' (len=2): 9,924\n",
      "    ‚úì '·ûì·üÑ·üá' (len=3): 9,239\n",
      "    ‚úì '·ûì·ûπ·ûÑ' (len=3): 8,682\n",
      "\n",
      "  Document frequency analysis:\n",
      "  Avg doc frequency: 31.80\n",
      "  Median doc frequency: 1.00\n",
      "  Tokens in only 1 document: 23,778\n",
      "\n",
      "  Sparsity analysis:\n",
      "  Type-Token Ratio: 0.027\n",
      "  ‚úì LOW SPARSITY - good token repetition\n",
      "\n",
      "  Sample document analysis (first 3):\n",
      "  Doc 1: 45 tokens\n",
      "    Sample: ·ûü·üÅ·ûÖ·ûÄ·üí·ûè·û∏·ûê·üí·ûõ·üÇ·ûÑ·ûÄ·û∂·ûö·ûé·üç ·ûñ·û∂·ûÄ·üã·ûñ·üê·ûì·üí·ûí ·ûì·ûπ·ûÑ ·ûñ·üê·ûè·üå·ûò·û∂·ûì ·ûì·üÅ·üá ·ûî·û∂·ûì ·û≤·üí·ûô·ûä·ûπ·ûÑ ·ûê·û∂ ·ûõ·üÑ·ûÄ ·ûì·ûπ·ûÑ...\n",
      "  Doc 2: 23 tokens\n",
      "    Sample: ·ûï·üí·ûè·ûì·üí·ûë·û∂·ûë·üÑ·ûü ·ûä·û∂·ûÄ·üã ·ûñ·ûì·üí·ûí·ûì·û∂·ûÄ·û∂·ûö ·ûä·üÑ·ûô ·ûñ·üí·ûô·ûΩ·ûö ·ûë·üÑ·ûü ·ûö·û∫ ·ûò·û∑·ûì ·ûñ·üí·ûô·ûΩ·ûö ·ûë·üÑ·ûü...\n",
      "  Doc 3: 14 tokens\n",
      "    Sample: ·ûì·üÖ ·ûÜ·üí·ûì·û∂·üÜ ·ûó·û∂·ûÇ ·ûÄ·ûé·üí·ûä·û∂·ûõ ·ûì·üÉ ·ûÖ·ûÄ·üí·ûö·ûó·ûñ ·ûî·û∂·ûì ·ûî·üÇ·ûÄ·ûî·û∂·ûÄ·üã ·ûî·ûÑ·üí·ûÄ·ûæ·ûè ·ûî·û∂·ûì·ûá·û∂...\n",
      "\n",
      "üìÅ Loading: No_Particles\n",
      "  Raw documents: 58,409\n",
      "  After cleaning: 58,398\n",
      "  Removed 17 very short documents (<3 tokens)\n",
      "\n",
      "üîç DIAGNOSING CORPUS: No_Particles\n",
      "============================================================\n",
      "  Document count: 58,381\n",
      "  Avg document length: 28.3 tokens\n",
      "  Median document length: 25.0 tokens\n",
      "  Min/Max length: 3 / 534 tokens\n",
      "\n",
      "  Token statistics:\n",
      "  Total tokens: 1,649,298\n",
      "  Unique tokens: 45,426\n",
      "\n",
      "  Token length distribution:\n",
      "    2 chars: 184,422 (11.2%)\n",
      "    3 chars: 422,815 (25.6%)\n",
      "    4 chars: 186,311 (11.3%)\n",
      "    5 chars: 269,005 (16.3%)\n",
      "    6 chars: 202,038 (12.2%)\n",
      "    7 chars: 141,128 (8.6%)\n",
      "    8 chars: 83,948 (5.1%)\n",
      "    9 chars: 54,317 (3.3%)\n",
      "    10 chars: 40,655 (2.5%)\n",
      "    11 chars: 26,895 (1.6%)\n",
      "\n",
      "  Top 20 most common tokens:\n",
      "    ‚úì '·ûî·û∂·ûì' (len=3): 39,983\n",
      "    ‚úì '·ûì·û∑·ûÑ' (len=3): 36,396\n",
      "    ‚úì '·ûä·üÇ·ûõ' (len=3): 29,783\n",
      "    ‚úì '·ûò·û∂·ûì' (len=3): 27,127\n",
      "    ‚úì '·ûá·û∂' (len=2): 22,253\n",
      "    ‚úì '·ûì·üÖ' (len=2): 21,807\n",
      "    ‚úì '·ûê·û∂' (len=2): 20,952\n",
      "    ‚úì '·ûö·ûî·ûü·üã' (len=4): 20,921\n",
      "    ‚úì '·ûÄ·üí·ûì·ûª·ûÑ' (len=5): 19,048\n",
      "    ‚úì '·ûì·üÅ·üá' (len=3): 17,111\n",
      "    ‚úì '·ûì·üÉ' (len=2): 14,390\n",
      "    ‚úì '·ûä·üÑ·ûô' (len=3): 13,524\n",
      "    ‚úì '·ûÜ·üí·ûì·û∂·üÜ' (len=5): 12,145\n",
      "    ‚úì '·ûõ·üÑ·ûÄ' (len=3): 11,358\n",
      "    ‚úì '·ûî·üí·ûö·ûë·üÅ·ûü' (len=6): 10,359\n",
      "    ‚úì '·ûò·û∑·ûì' (len=3): 10,325\n",
      "    ‚úì '·ûñ·û∏' (len=2): 9,924\n",
      "    ‚úì '·ûì·üÑ·üá' (len=3): 9,239\n",
      "    ‚úì '·ûì·ûπ·ûÑ' (len=3): 8,682\n",
      "    ‚úì '·ûë·üÖ' (len=2): 8,665\n",
      "\n",
      "  Document frequency analysis:\n",
      "  Avg doc frequency: 31.36\n",
      "  Median doc frequency: 1.00\n",
      "  Tokens in only 1 document: 23,778\n",
      "\n",
      "  Sparsity analysis:\n",
      "  Type-Token Ratio: 0.028\n",
      "  ‚úì LOW SPARSITY - good token repetition\n",
      "\n",
      "  Sample document analysis (first 3):\n",
      "  Doc 1: 45 tokens\n",
      "    Sample: ·ûü·üÅ·ûÖ·ûÄ·üí·ûè·û∏·ûê·üí·ûõ·üÇ·ûÑ·ûÄ·û∂·ûö·ûé·üç ·ûñ·û∂·ûÄ·üã·ûñ·üê·ûì·üí·ûí ·ûì·ûπ·ûÑ ·ûñ·üê·ûè·üå·ûò·û∂·ûì ·ûì·üÅ·üá ·ûî·û∂·ûì ·û≤·üí·ûô·ûä·ûπ·ûÑ ·ûê·û∂ ·ûõ·üÑ·ûÄ ·ûì·ûπ·ûÑ...\n",
      "  Doc 2: 24 tokens\n",
      "    Sample: ·ûï·üí·ûè·ûì·üí·ûë·û∂·ûë·üÑ·ûü ·ûä·û∂·ûÄ·üã ·ûñ·ûì·üí·ûí·ûì·û∂·ûÄ·û∂·ûö ·ûä·üÑ·ûô ·ûñ·üí·ûô·ûΩ·ûö ·ûë·üÑ·ûü ·ûö·û∫ ·ûò·û∑·ûì ·ûñ·üí·ûô·ûΩ·ûö ·ûë·üÑ·ûü...\n",
      "  Doc 3: 14 tokens\n",
      "    Sample: ·ûì·üÖ ·ûÜ·üí·ûì·û∂·üÜ ·ûó·û∂·ûÇ ·ûÄ·ûé·üí·ûä·û∂·ûõ ·ûì·üÉ ·ûÖ·ûÄ·üí·ûö·ûó·ûñ ·ûî·û∂·ûì ·ûî·üÇ·ûÄ·ûî·û∂·ûÄ·üã ·ûî·ûÑ·üí·ûÄ·ûæ·ûè ·ûî·û∂·ûì·ûá·û∂...\n",
      "\n",
      "üìÅ Loading: No_Politeness\n",
      "  Raw documents: 58,409\n",
      "  After cleaning: 58,398\n",
      "  Removed 28 very short documents (<3 tokens)\n",
      "\n",
      "üîç DIAGNOSING CORPUS: No_Politeness\n",
      "============================================================\n",
      "  Document count: 58,370\n",
      "  Avg document length: 28.8 tokens\n",
      "  Median document length: 25.0 tokens\n",
      "  Min/Max length: 3 / 535 tokens\n",
      "\n",
      "  Token statistics:\n",
      "  Total tokens: 1,679,063\n",
      "  Unique tokens: 45,445\n",
      "\n",
      "  Token length distribution:\n",
      "    2 chars: 197,602 (11.8%)\n",
      "    3 chars: 430,472 (25.6%)\n",
      "    4 chars: 191,026 (11.4%)\n",
      "    5 chars: 269,134 (16.0%)\n",
      "    6 chars: 202,034 (12.0%)\n",
      "    7 chars: 142,929 (8.5%)\n",
      "    8 chars: 86,120 (5.1%)\n",
      "    9 chars: 54,400 (3.2%)\n",
      "    10 chars: 40,655 (2.4%)\n",
      "    11 chars: 26,927 (1.6%)\n",
      "\n",
      "  Top 20 most common tokens:\n",
      "    ‚úì '·ûî·û∂·ûì' (len=3): 39,983\n",
      "    ‚úì '·ûì·û∑·ûÑ' (len=3): 36,396\n",
      "    ‚úì '·ûä·üÇ·ûõ' (len=3): 29,783\n",
      "    ‚úì '·ûò·û∂·ûì' (len=3): 27,127\n",
      "    ‚úì '·ûá·û∂' (len=2): 22,252\n",
      "    ‚úì '·ûì·üÖ' (len=2): 21,807\n",
      "    ‚úì '·ûê·û∂' (len=2): 20,952\n",
      "    ‚úì '·ûö·ûî·ûü·üã' (len=4): 20,921\n",
      "    ‚úì '·ûÄ·üí·ûì·ûª·ûÑ' (len=5): 19,048\n",
      "    ‚úì '·ûì·üÅ·üá' (len=3): 17,111\n",
      "    ‚úì '·ûì·üÉ' (len=2): 14,390\n",
      "    ‚úì '·ûä·üÑ·ûô' (len=3): 13,524\n",
      "    ‚úì '·ûÜ·üí·ûì·û∂·üÜ' (len=5): 12,145\n",
      "    ‚úì '·û†·ûæ·ûô' (len=3): 10,636\n",
      "    ‚úì '·ûî·üí·ûö·ûë·üÅ·ûü' (len=6): 10,359\n",
      "    ‚úì '·ûò·û∑·ûì' (len=3): 10,325\n",
      "    ‚úì '·ûñ·û∏' (len=2): 9,924\n",
      "    ‚úì '·ûì·üÑ·üá' (len=3): 9,239\n",
      "    ‚úì '·ûì·ûπ·ûÑ' (len=3): 8,682\n",
      "    ‚úì '·ûë·üÖ' (len=2): 8,665\n",
      "\n",
      "  Document frequency analysis:\n",
      "  Avg doc frequency: 31.96\n",
      "  Median doc frequency: 1.00\n",
      "  Tokens in only 1 document: 23,775\n",
      "\n",
      "  Sparsity analysis:\n",
      "  Type-Token Ratio: 0.027\n",
      "  ‚úì LOW SPARSITY - good token repetition\n",
      "\n",
      "  Sample document analysis (first 3):\n",
      "  Doc 1: 43 tokens\n",
      "    Sample: ·ûü·üÅ·ûÖ·ûÄ·üí·ûè·û∏·ûê·üí·ûõ·üÇ·ûÑ·ûÄ·û∂·ûö·ûé·üç ·ûñ·û∂·ûÄ·üã·ûñ·üê·ûì·üí·ûí ·ûì·ûπ·ûÑ ·ûñ·üê·ûè·üå·ûò·û∂·ûì ·ûì·üÅ·üá ·ûî·û∂·ûì ·û≤·üí·ûô·ûä·ûπ·ûÑ ·ûê·û∂ ·ûì·ûπ·ûÑ ·ûë·üÖ...\n",
      "  Doc 2: 24 tokens\n",
      "    Sample: ·ûï·üí·ûè·ûì·üí·ûë·û∂·ûë·üÑ·ûü ·ûä·û∂·ûÄ·üã ·ûñ·ûì·üí·ûí·ûì·û∂·ûÄ·û∂·ûö ·ûä·üÑ·ûô ·ûñ·üí·ûô·ûΩ·ûö ·ûë·üÑ·ûü ·ûö·û∫ ·ûò·û∑·ûì ·ûñ·üí·ûô·ûΩ·ûö ·ûë·üÑ·ûü...\n",
      "  Doc 3: 14 tokens\n",
      "    Sample: ·ûì·üÖ ·ûÜ·üí·ûì·û∂·üÜ ·ûó·û∂·ûÇ ·ûÄ·ûé·üí·ûä·û∂·ûõ ·ûì·üÉ ·ûÖ·ûÄ·üí·ûö·ûó·ûñ ·ûî·û∂·ûì ·ûî·üÇ·ûÄ·ûî·û∂·ûÄ·üã ·ûî·ûÑ·üí·ûÄ·ûæ·ûè ·ûî·û∂·ûì·ûá·û∂...\n",
      "\n",
      "üìÅ Loading: No_Pronouns\n",
      "  Raw documents: 58,409\n",
      "  After cleaning: 58,398\n",
      "  Removed 18 very short documents (<3 tokens)\n",
      "\n",
      "üîç DIAGNOSING CORPUS: No_Pronouns\n",
      "============================================================\n",
      "  Document count: 58,380\n",
      "  Avg document length: 28.6 tokens\n",
      "  Median document length: 25.0 tokens\n",
      "  Min/Max length: 3 / 535 tokens\n",
      "\n",
      "  Token statistics:\n",
      "  Total tokens: 1,667,009\n",
      "  Unique tokens: 45,440\n",
      "\n",
      "  Token length distribution:\n",
      "    2 chars: 188,535 (11.3%)\n",
      "    3 chars: 439,294 (26.4%)\n",
      "    4 chars: 184,099 (11.0%)\n",
      "    5 chars: 264,370 (15.9%)\n",
      "    6 chars: 201,978 (12.1%)\n",
      "    7 chars: 142,927 (8.6%)\n",
      "    8 chars: 86,068 (5.2%)\n",
      "    9 chars: 54,400 (3.3%)\n",
      "    10 chars: 40,655 (2.4%)\n",
      "    11 chars: 26,919 (1.6%)\n",
      "\n",
      "  Top 20 most common tokens:\n",
      "    ‚úì '·ûî·û∂·ûì' (len=3): 39,983\n",
      "    ‚úì '·ûì·û∑·ûÑ' (len=3): 36,396\n",
      "    ‚úì '·ûä·üÇ·ûõ' (len=3): 29,783\n",
      "    ‚úì '·ûò·û∂·ûì' (len=3): 27,127\n",
      "    ‚úì '·ûá·û∂' (len=2): 22,253\n",
      "    ‚úì '·ûì·üÖ' (len=2): 21,807\n",
      "    ‚úì '·ûê·û∂' (len=2): 20,952\n",
      "    ‚úì '·ûö·ûî·ûü·üã' (len=4): 20,921\n",
      "    ‚úì '·ûÄ·üí·ûì·ûª·ûÑ' (len=5): 19,048\n",
      "    ‚úì '·ûì·üÅ·üá' (len=3): 17,111\n",
      "    ‚úì '·ûì·üÉ' (len=2): 14,390\n",
      "    ‚úì '·ûä·üÑ·ûô' (len=3): 13,524\n",
      "    ‚úì '·ûÜ·üí·ûì·û∂·üÜ' (len=5): 12,145\n",
      "    ‚úì '·ûõ·üÑ·ûÄ' (len=3): 11,358\n",
      "    ‚úì '·û†·ûæ·ûô' (len=3): 10,636\n",
      "    ‚úì '·ûî·üí·ûö·ûë·üÅ·ûü' (len=6): 10,359\n",
      "    ‚úì '·ûò·û∑·ûì' (len=3): 10,325\n",
      "    ‚úì '·ûñ·û∏' (len=2): 9,924\n",
      "    ‚úì '·ûì·üÑ·üá' (len=3): 9,239\n",
      "    ‚úì '·ûì·ûπ·ûÑ' (len=3): 8,682\n",
      "\n",
      "  Document frequency analysis:\n",
      "  Avg doc frequency: 31.73\n",
      "  Median doc frequency: 1.00\n",
      "  Tokens in only 1 document: 23,777\n",
      "\n",
      "  Sparsity analysis:\n",
      "  Type-Token Ratio: 0.027\n",
      "  ‚úì LOW SPARSITY - good token repetition\n",
      "\n",
      "  Sample document analysis (first 3):\n",
      "  Doc 1: 45 tokens\n",
      "    Sample: ·ûü·üÅ·ûÖ·ûÄ·üí·ûè·û∏·ûê·üí·ûõ·üÇ·ûÑ·ûÄ·û∂·ûö·ûé·üç ·ûñ·û∂·ûÄ·üã·ûñ·üê·ûì·üí·ûí ·ûì·ûπ·ûÑ ·ûñ·üê·ûè·üå·ûò·û∂·ûì ·ûì·üÅ·üá ·ûî·û∂·ûì ·û≤·üí·ûô·ûä·ûπ·ûÑ ·ûê·û∂ ·ûõ·üÑ·ûÄ ·ûì·ûπ·ûÑ...\n",
      "  Doc 2: 24 tokens\n",
      "    Sample: ·ûï·üí·ûè·ûì·üí·ûë·û∂·ûë·üÑ·ûü ·ûä·û∂·ûÄ·üã ·ûñ·ûì·üí·ûí·ûì·û∂·ûÄ·û∂·ûö ·ûä·üÑ·ûô ·ûñ·üí·ûô·ûΩ·ûö ·ûë·üÑ·ûü ·ûö·û∫ ·ûò·û∑·ûì ·ûñ·üí·ûô·ûΩ·ûö ·ûë·üÑ·ûü...\n",
      "  Doc 3: 14 tokens\n",
      "    Sample: ·ûì·üÖ ·ûÜ·üí·ûì·û∂·üÜ ·ûó·û∂·ûÇ ·ûÄ·ûé·üí·ûä·û∂·ûõ ·ûì·üÉ ·ûÖ·ûÄ·üí·ûö·ûó·ûñ ·ûî·û∂·ûì ·ûî·üÇ·ûÄ·ûî·û∂·ûÄ·üã ·ûî·ûÑ·üí·ûÄ·ûæ·ûè ·ûî·û∂·ûì·ûá·û∂...\n",
      "\n",
      "üìÅ Loading: No_Questions\n",
      "  Raw documents: 58,409\n",
      "  After cleaning: 58,398\n",
      "  Removed 18 very short documents (<3 tokens)\n",
      "\n",
      "üîç DIAGNOSING CORPUS: No_Questions\n",
      "============================================================\n",
      "  Document count: 58,380\n",
      "  Avg document length: 28.7 tokens\n",
      "  Median document length: 25.0 tokens\n",
      "  Min/Max length: 3 / 534 tokens\n",
      "\n",
      "  Token statistics:\n",
      "  Total tokens: 1,676,244\n",
      "  Unique tokens: 45,447\n",
      "\n",
      "  Token length distribution:\n",
      "    2 chars: 193,405 (11.5%)\n",
      "    3 chars: 432,580 (25.8%)\n",
      "    4 chars: 191,029 (11.4%)\n",
      "    5 chars: 269,100 (16.1%)\n",
      "    6 chars: 201,335 (12.0%)\n",
      "    7 chars: 142,929 (8.5%)\n",
      "    8 chars: 86,120 (5.1%)\n",
      "    9 chars: 54,400 (3.2%)\n",
      "    10 chars: 40,655 (2.4%)\n",
      "    11 chars: 26,927 (1.6%)\n",
      "\n",
      "  Top 20 most common tokens:\n",
      "    ‚úì '·ûî·û∂·ûì' (len=3): 39,983\n",
      "    ‚úì '·ûì·û∑·ûÑ' (len=3): 36,396\n",
      "    ‚úì '·ûä·üÇ·ûõ' (len=3): 29,783\n",
      "    ‚úì '·ûò·û∂·ûì' (len=3): 27,127\n",
      "    ‚úì '·ûá·û∂' (len=2): 22,253\n",
      "    ‚úì '·ûì·üÖ' (len=2): 21,807\n",
      "    ‚úì '·ûê·û∂' (len=2): 20,952\n",
      "    ‚úì '·ûö·ûî·ûü·üã' (len=4): 20,921\n",
      "    ‚úì '·ûÄ·üí·ûì·ûª·ûÑ' (len=5): 19,048\n",
      "    ‚úì '·ûì·üÅ·üá' (len=3): 17,111\n",
      "    ‚úì '·ûì·üÉ' (len=2): 14,390\n",
      "    ‚úì '·ûä·üÑ·ûô' (len=3): 13,524\n",
      "    ‚úì '·ûÜ·üí·ûì·û∂·üÜ' (len=5): 12,145\n",
      "    ‚úì '·ûõ·üÑ·ûÄ' (len=3): 11,358\n",
      "    ‚úì '·û†·ûæ·ûô' (len=3): 10,636\n",
      "    ‚úì '·ûî·üí·ûö·ûë·üÅ·ûü' (len=6): 10,359\n",
      "    ‚úì '·ûñ·û∏' (len=2): 9,924\n",
      "    ‚úì '·ûì·üÑ·üá' (len=3): 9,239\n",
      "    ‚úì '·ûì·ûπ·ûÑ' (len=3): 8,682\n",
      "    ‚úì '·ûë·üÖ' (len=2): 8,665\n",
      "\n",
      "  Document frequency analysis:\n",
      "  Avg doc frequency: 31.88\n",
      "  Median doc frequency: 1.00\n",
      "  Tokens in only 1 document: 23,778\n",
      "\n",
      "  Sparsity analysis:\n",
      "  Type-Token Ratio: 0.027\n",
      "  ‚úì LOW SPARSITY - good token repetition\n",
      "\n",
      "  Sample document analysis (first 3):\n",
      "  Doc 1: 45 tokens\n",
      "    Sample: ·ûü·üÅ·ûÖ·ûÄ·üí·ûè·û∏·ûê·üí·ûõ·üÇ·ûÑ·ûÄ·û∂·ûö·ûé·üç ·ûñ·û∂·ûÄ·üã·ûñ·üê·ûì·üí·ûí ·ûì·ûπ·ûÑ ·ûñ·üê·ûè·üå·ûò·û∂·ûì ·ûì·üÅ·üá ·ûî·û∂·ûì ·û≤·üí·ûô·ûä·ûπ·ûÑ ·ûê·û∂ ·ûõ·üÑ·ûÄ ·ûì·ûπ·ûÑ...\n",
      "  Doc 2: 21 tokens\n",
      "    Sample: ·ûï·üí·ûè·ûì·üí·ûë·û∂·ûë·üÑ·ûü ·ûä·û∂·ûÄ·üã ·ûñ·ûì·üí·ûí·ûì·û∂·ûÄ·û∂·ûö ·ûä·üÑ·ûô ·ûñ·üí·ûô·ûΩ·ûö ·ûë·üÑ·ûü ·ûö·û∫ ·ûñ·üí·ûô·ûΩ·ûö ·ûë·üÑ·ûü ·ûÄ·üí·ûì·ûª·ûÑ...\n",
      "  Doc 3: 14 tokens\n",
      "    Sample: ·ûì·üÖ ·ûÜ·üí·ûì·û∂·üÜ ·ûó·û∂·ûÇ ·ûÄ·ûé·üí·ûä·û∂·ûõ ·ûì·üÉ ·ûÖ·ûÄ·üí·ûö·ûó·ûñ ·ûî·û∂·ûì ·ûî·üÇ·ûÄ·ûî·û∂·ûÄ·üã ·ûî·ûÑ·üí·ûÄ·ûæ·ûè ·ûî·û∂·ûì·ûá·û∂...\n",
      "\n",
      "üìÅ Loading: No_All_Stopwords\n",
      "  Raw documents: 58,409\n",
      "  After cleaning: 58,394\n",
      "  Removed 152 very short documents (<3 tokens)\n",
      "\n",
      "üîç DIAGNOSING CORPUS: No_All_Stopwords\n",
      "============================================================\n",
      "  Document count: 58,242\n",
      "  Avg document length: 20.5 tokens\n",
      "  Median document length: 18.0 tokens\n",
      "  Min/Max length: 3 / 480 tokens\n",
      "\n",
      "  Token statistics:\n",
      "  Total tokens: 1,195,715\n",
      "  Unique tokens: 45,245\n",
      "\n",
      "  Token length distribution:\n",
      "    2 chars: 115,571 (9.7%)\n",
      "    3 chars: 180,376 (15.1%)\n",
      "    4 chars: 145,198 (12.1%)\n",
      "    5 chars: 217,833 (18.2%)\n",
      "    6 chars: 181,145 (15.1%)\n",
      "    7 chars: 128,878 (10.8%)\n",
      "    8 chars: 71,749 (6.0%)\n",
      "    9 chars: 51,524 (4.3%)\n",
      "    10 chars: 39,047 (3.3%)\n",
      "    11 chars: 26,747 (2.2%)\n",
      "\n",
      "  Top 20 most common tokens:\n",
      "    ‚úì '·ûì·üÖ' (len=2): 21,805\n",
      "    ‚úì '·ûê·û∂' (len=2): 20,951\n",
      "    ‚úì '·ûä·üÑ·ûô' (len=3): 13,520\n",
      "    ‚úì '·ûÜ·üí·ûì·û∂·üÜ' (len=5): 12,143\n",
      "    ‚úì '·ûî·üí·ûö·ûë·üÅ·ûü' (len=6): 10,359\n",
      "    ‚úì '·ûë·üÖ' (len=2): 8,665\n",
      "    ‚úì '·ûÄ·ûò·üí·ûñ·ûª·ûá·û∂' (len=7): 8,092\n",
      "    ‚úì '·ûì·üÖ·ûÄ·üí·ûì·ûª·ûÑ' (len=7): 7,573\n",
      "    ‚úì '·ûë·û∏' (len=2): 7,385\n",
      "    ‚úì '·ûÇ·û∫' (len=2): 5,435\n",
      "    ‚úì '·ûë·üÄ·ûè' (len=3): 5,114\n",
      "    ‚úì '·ûì·û∂·ûÄ·üã' (len=4): 4,999\n",
      "    ‚úì '·ûè·üí·ûö·ûº·ûú' (len=5): 4,736\n",
      "    ‚úì '·ûÅ·üÅ·ûè·üí·ûè' (len=5): 4,633\n",
      "    ‚úì '·ûí·üí·ûú·ûæ' (len=4): 4,387\n",
      "    ‚úì '·ûì·û∑·ûô·û∂·ûô' (len=5): 4,371\n",
      "    ‚úì '·ûò·ûÄ' (len=2): 4,208\n",
      "    ‚úì '·û±·üí·ûô' (len=3): 4,095\n",
      "    ‚úì '·û≤·üí·ûô' (len=3): 4,020\n",
      "    ‚úì '·ûá·û∂·ûè·û∑' (len=4): 3,909\n",
      "\n",
      "  Document frequency analysis:\n",
      "  Avg doc frequency: 23.43\n",
      "  Median doc frequency: 1.00\n",
      "  Tokens in only 1 document: 23,773\n",
      "\n",
      "  Sparsity analysis:\n",
      "  Type-Token Ratio: 0.038\n",
      "  ‚úì LOW SPARSITY - good token repetition\n",
      "\n",
      "  Sample document analysis (first 3):\n",
      "  Doc 1: 30 tokens\n",
      "    Sample: ·ûü·üÅ·ûÖ·ûÄ·üí·ûè·û∏·ûê·üí·ûõ·üÇ·ûÑ·ûÄ·û∂·ûö·ûé·üç ·ûñ·û∂·ûÄ·üã·ûñ·üê·ûì·üí·ûí ·ûñ·üê·ûè·üå·ûò·û∂·ûì ·û≤·üí·ûô·ûä·ûπ·ûÑ ·ûê·û∂ ·ûë·üÖ ·ûî·üÜ·ûñ·üÅ·ûâ ·ûë·ûü·üí·ûü·ûì·ûÄ·û∑·ûÖ·üí·ûÖ ·ûì·üÖ ·ûî·üí·ûö·ûë·üÅ·ûü...\n",
      "  Doc 2: 16 tokens\n",
      "    Sample: ·ûï·üí·ûè·ûì·üí·ûë·û∂·ûë·üÑ·ûü ·ûä·û∂·ûÄ·üã ·ûñ·ûì·üí·ûí·ûì·û∂·ûÄ·û∂·ûö ·ûä·üÑ·ûô ·ûñ·üí·ûô·ûΩ·ûö ·ûë·üÑ·ûü ·ûö·û∫ ·ûñ·üí·ûô·ûΩ·ûö ·ûë·üÑ·ûü ·ûö·ûô...\n",
      "  Doc 3: 12 tokens\n",
      "    Sample: ·ûì·üÖ ·ûÜ·üí·ûì·û∂·üÜ ·ûó·û∂·ûÇ ·ûÄ·ûé·üí·ûä·û∂·ûõ ·ûÖ·ûÄ·üí·ûö·ûó·ûñ ·ûî·üÇ·ûÄ·ûî·û∂·ûÄ·üã ·ûî·ûÑ·üí·ûÄ·ûæ·ûè ·ûî·û∂·ûì·ûá·û∂ ·ûö·û∂·ûá·û∂·ûé·û∂·ûÖ·ûÄ·üí·ûö ·ûè·ûº·ûÖ...\n",
      "\n",
      "üìù Using 50 queries\n",
      "\n",
      "================================================================================\n",
      "RUNNING EVALUATIONS\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "EVALUATING: Original\n",
      "============================================================\n",
      "\n",
      "üî¨ Evaluating with debugging: Original\n",
      "  Query count: 50\n",
      "  ‚úì TF-IDF matrix shape: (58382, 10000)\n",
      "  ‚úì Non-zero elements: 1,402,886\n",
      "  ‚úì Density: 0.002403\n",
      "\n",
      "  Sample query 0 similarity analysis:\n",
      "    Query index: 41905\n",
      "    Top 5 similarities: [1.         0.29583171 0.29556446 0.25931129 0.25719627]\n",
      "    Bottom 5 similarities: [0. 0. 0. 0. 0.]\n",
      "    Mean similarity: 0.021576\n",
      "    Std similarity: 0.026538\n",
      "    Similarity to true document: 1.000000\n",
      "    Rank of true document: 1/58382\n",
      "\n",
      "  Similarity score analysis:\n",
      "    Mean score: 0.019979\n",
      "    Std score: 0.026595\n",
      "\n",
      "üìä SUMMARY FOR Original:\n",
      "  Mean Rank: 1.00\n",
      "  Median Rank: 1.0\n",
      "  Success Rate @10: 1.000\n",
      "  Corpus Density: 0.002403\n",
      "\n",
      "============================================================\n",
      "EVALUATING: No_Prepositions\n",
      "============================================================\n",
      "\n",
      "üî¨ Evaluating with debugging: No_Prepositions\n",
      "  Query count: 50\n",
      "  ‚úì TF-IDF matrix shape: (58376, 10000)\n",
      "  ‚úì Non-zero elements: 1,308,982\n",
      "  ‚úì Density: 0.002242\n",
      "\n",
      "  Sample query 0 similarity analysis:\n",
      "    Query index: 41905\n",
      "    Top 5 similarities: [1.         0.52330399 0.34353369 0.28564536 0.27419306]\n",
      "    Bottom 5 similarities: [0. 0. 0. 0. 0.]\n",
      "    Mean similarity: 0.018059\n",
      "    Std similarity: 0.023340\n",
      "    Similarity to true document: 1.000000\n",
      "    Rank of true document: 1/58376\n",
      "\n",
      "  Similarity score analysis:\n",
      "    Mean score: 0.018874\n",
      "    Std score: 0.026564\n",
      "\n",
      "üìä SUMMARY FOR No_Prepositions:\n",
      "  Mean Rank: 1.00\n",
      "  Median Rank: 1.0\n",
      "  Success Rate @10: 1.000\n",
      "  Corpus Density: 0.002242\n",
      "\n",
      "============================================================\n",
      "EVALUATING: No_Auxiliary_Verbs\n",
      "============================================================\n",
      "\n",
      "üî¨ Evaluating with debugging: No_Auxiliary_Verbs\n",
      "  Query count: 50\n",
      "  ‚úì TF-IDF matrix shape: (58316, 10000)\n",
      "  ‚úì Non-zero elements: 1,352,054\n",
      "  ‚úì Density: 0.002318\n",
      "\n",
      "  Sample query 0 similarity analysis:\n",
      "    Query index: 41905\n",
      "    Top 5 similarities: [1.         0.41905536 0.39464142 0.36204976 0.35294125]\n",
      "    Bottom 5 similarities: [0. 0. 0. 0. 0.]\n",
      "    Mean similarity: 0.024172\n",
      "    Std similarity: 0.021881\n",
      "    Similarity to true document: 1.000000\n",
      "    Rank of true document: 1/58316\n",
      "\n",
      "  Similarity score analysis:\n",
      "    Mean score: 0.020240\n",
      "    Std score: 0.026676\n",
      "\n",
      "üìä SUMMARY FOR No_Auxiliary_Verbs:\n",
      "  Mean Rank: 1.02\n",
      "  Median Rank: 1.0\n",
      "  Success Rate @10: 1.000\n",
      "  Corpus Density: 0.002318\n",
      "\n",
      "============================================================\n",
      "EVALUATING: No_Conjunctions\n",
      "============================================================\n",
      "\n",
      "üî¨ Evaluating with debugging: No_Conjunctions\n",
      "  Query count: 50\n",
      "  ‚úì TF-IDF matrix shape: (58362, 10000)\n",
      "  ‚úì Non-zero elements: 1,319,998\n",
      "  ‚úì Density: 0.002262\n",
      "\n",
      "  Sample query 0 similarity analysis:\n",
      "    Query index: 41905\n",
      "    Top 5 similarities: [1.         0.60918927 0.34600368 0.32492587 0.30572071]\n",
      "    Bottom 5 similarities: [0. 0. 0. 0. 0.]\n",
      "    Mean similarity: 0.026130\n",
      "    Std similarity: 0.028739\n",
      "    Similarity to true document: 1.000000\n",
      "    Rank of true document: 1/58362\n",
      "\n",
      "  Similarity score analysis:\n",
      "    Mean score: 0.016904\n",
      "    Std score: 0.025155\n",
      "\n",
      "üìä SUMMARY FOR No_Conjunctions:\n",
      "  Mean Rank: 1.02\n",
      "  Median Rank: 1.0\n",
      "  Success Rate @10: 1.000\n",
      "  Corpus Density: 0.002262\n",
      "\n",
      "============================================================\n",
      "EVALUATING: No_Determiners\n",
      "============================================================\n",
      "\n",
      "üî¨ Evaluating with debugging: No_Determiners\n",
      "  Query count: 50\n",
      "  ‚úì TF-IDF matrix shape: (58381, 10000)\n",
      "  ‚úì Non-zero elements: 1,364,801\n",
      "  ‚úì Density: 0.002338\n",
      "\n",
      "  Sample query 0 similarity analysis:\n",
      "    Query index: 41905\n",
      "    Top 5 similarities: [1.         0.21879846 0.21119087 0.20477782 0.18468925]\n",
      "    Bottom 5 similarities: [0. 0. 0. 0. 0.]\n",
      "    Mean similarity: 0.009641\n",
      "    Std similarity: 0.016889\n",
      "    Similarity to true document: 1.000000\n",
      "    Rank of true document: 1/58381\n",
      "\n",
      "  Similarity score analysis:\n",
      "    Mean score: 0.019993\n",
      "    Std score: 0.026241\n",
      "\n",
      "üìä SUMMARY FOR No_Determiners:\n",
      "  Mean Rank: 1.00\n",
      "  Median Rank: 1.0\n",
      "  Success Rate @10: 1.000\n",
      "  Corpus Density: 0.002338\n",
      "\n",
      "============================================================\n",
      "EVALUATING: No_Function_Nouns\n",
      "============================================================\n",
      "\n",
      "üî¨ Evaluating with debugging: No_Function_Nouns\n",
      "  Query count: 50\n",
      "  ‚úì TF-IDF matrix shape: (58377, 10000)\n",
      "  ‚úì Non-zero elements: 1,364,023\n",
      "  ‚úì Density: 0.002337\n",
      "\n",
      "  Sample query 0 similarity analysis:\n",
      "    Query index: 41905\n",
      "    Top 5 similarities: [1.         0.47367687 0.41878072 0.38650191 0.36880775]\n",
      "    Bottom 5 similarities: [0. 0. 0. 0. 0.]\n",
      "    Mean similarity: 0.017991\n",
      "    Std similarity: 0.025283\n",
      "    Similarity to true document: 1.000000\n",
      "    Rank of true document: 1/58377\n",
      "\n",
      "  Similarity score analysis:\n",
      "    Mean score: 0.019514\n",
      "    Std score: 0.027564\n",
      "\n",
      "üìä SUMMARY FOR No_Function_Nouns:\n",
      "  Mean Rank: 1.00\n",
      "  Median Rank: 1.0\n",
      "  Success Rate @10: 1.000\n",
      "  Corpus Density: 0.002337\n",
      "\n",
      "============================================================\n",
      "EVALUATING: No_Numbers\n",
      "============================================================\n",
      "\n",
      "üî¨ Evaluating with debugging: No_Numbers\n",
      "  Query count: 50\n",
      "  ‚úì TF-IDF matrix shape: (58381, 10000)\n",
      "  ‚úì Non-zero elements: 1,386,068\n",
      "  ‚úì Density: 0.002374\n",
      "\n",
      "  Sample query 0 similarity analysis:\n",
      "    Query index: 41905\n",
      "    Top 5 similarities: [1.         0.30292906 0.29981934 0.26617945 0.2630445 ]\n",
      "    Bottom 5 similarities: [0. 0. 0. 0. 0.]\n",
      "    Mean similarity: 0.021396\n",
      "    Std similarity: 0.026462\n",
      "    Similarity to true document: 1.000000\n",
      "    Rank of true document: 1/58381\n",
      "\n",
      "  Similarity score analysis:\n",
      "    Mean score: 0.022070\n",
      "    Std score: 0.027203\n",
      "\n",
      "üìä SUMMARY FOR No_Numbers:\n",
      "  Mean Rank: 1.00\n",
      "  Median Rank: 1.0\n",
      "  Success Rate @10: 1.000\n",
      "  Corpus Density: 0.002374\n",
      "\n",
      "============================================================\n",
      "EVALUATING: No_Particles\n",
      "============================================================\n",
      "\n",
      "üî¨ Evaluating with debugging: No_Particles\n",
      "  Query count: 50\n",
      "  ‚úì TF-IDF matrix shape: (58381, 10000)\n",
      "  ‚úì Non-zero elements: 1,365,766\n",
      "  ‚úì Density: 0.002339\n",
      "\n",
      "  Sample query 0 similarity analysis:\n",
      "    Query index: 41905\n",
      "    Top 5 similarities: [1.         0.22859049 0.21213895 0.2046138  0.18747507]\n",
      "    Bottom 5 similarities: [0. 0. 0. 0. 0.]\n",
      "    Mean similarity: 0.010153\n",
      "    Std similarity: 0.017151\n",
      "    Similarity to true document: 1.000000\n",
      "    Rank of true document: 1/58381\n",
      "\n",
      "  Similarity score analysis:\n",
      "    Mean score: 0.020592\n",
      "    Std score: 0.026457\n",
      "\n",
      "üìä SUMMARY FOR No_Particles:\n",
      "  Mean Rank: 1.00\n",
      "  Median Rank: 1.0\n",
      "  Success Rate @10: 1.000\n",
      "  Corpus Density: 0.002339\n",
      "\n",
      "============================================================\n",
      "EVALUATING: No_Politeness\n",
      "============================================================\n",
      "\n",
      "üî¨ Evaluating with debugging: No_Politeness\n",
      "  Query count: 50\n",
      "  ‚úì TF-IDF matrix shape: (58370, 10000)\n",
      "  ‚úì Non-zero elements: 1,393,692\n",
      "  ‚úì Density: 0.002388\n",
      "\n",
      "  Sample query 0 similarity analysis:\n",
      "    Query index: 41905\n",
      "    Top 5 similarities: [1.         0.5126085  0.27481097 0.26836432 0.24393196]\n",
      "    Bottom 5 similarities: [0. 0. 0. 0. 0.]\n",
      "    Mean similarity: 0.014101\n",
      "    Std similarity: 0.019400\n",
      "    Similarity to true document: 1.000000\n",
      "    Rank of true document: 1/58370\n",
      "\n",
      "  Similarity score analysis:\n",
      "    Mean score: 0.020083\n",
      "    Std score: 0.026743\n",
      "\n",
      "üìä SUMMARY FOR No_Politeness:\n",
      "  Mean Rank: 1.00\n",
      "  Median Rank: 1.0\n",
      "  Success Rate @10: 1.000\n",
      "  Corpus Density: 0.002388\n",
      "\n",
      "============================================================\n",
      "EVALUATING: No_Pronouns\n",
      "============================================================\n",
      "\n",
      "üî¨ Evaluating with debugging: No_Pronouns\n",
      "  Query count: 50\n",
      "  ‚úì TF-IDF matrix shape: (58380, 10000)\n",
      "  ‚úì Non-zero elements: 1,382,832\n",
      "  ‚úì Density: 0.002369\n",
      "\n",
      "  Sample query 0 similarity analysis:\n",
      "    Query index: 41905\n",
      "    Top 5 similarities: [1.         0.50819894 0.43437593 0.43019211 0.40550946]\n",
      "    Bottom 5 similarities: [0. 0. 0. 0. 0.]\n",
      "    Mean similarity: 0.005514\n",
      "    Std similarity: 0.022462\n",
      "    Similarity to true document: 1.000000\n",
      "    Rank of true document: 1/58380\n",
      "\n",
      "  Similarity score analysis:\n",
      "    Mean score: 0.019702\n",
      "    Std score: 0.026594\n",
      "\n",
      "üìä SUMMARY FOR No_Pronouns:\n",
      "  Mean Rank: 1.00\n",
      "  Median Rank: 1.0\n",
      "  Success Rate @10: 1.000\n",
      "  Corpus Density: 0.002369\n",
      "\n",
      "============================================================\n",
      "EVALUATING: No_Questions\n",
      "============================================================\n",
      "\n",
      "üî¨ Evaluating with debugging: No_Questions\n",
      "  Query count: 50\n",
      "  ‚úì TF-IDF matrix shape: (58380, 10000)\n",
      "  ‚úì Non-zero elements: 1,389,934\n",
      "  ‚úì Density: 0.002381\n",
      "\n",
      "  Sample query 0 similarity analysis:\n",
      "    Query index: 41905\n",
      "    Top 5 similarities: [1.         0.22139245 0.21098291 0.20509216 0.20309102]\n",
      "    Bottom 5 similarities: [0. 0. 0. 0. 0.]\n",
      "    Mean similarity: 0.010576\n",
      "    Std similarity: 0.017428\n",
      "    Similarity to true document: 1.000000\n",
      "    Rank of true document: 1/58380\n",
      "\n",
      "  Similarity score analysis:\n",
      "    Mean score: 0.020560\n",
      "    Std score: 0.027040\n",
      "\n",
      "üìä SUMMARY FOR No_Questions:\n",
      "  Mean Rank: 1.00\n",
      "  Median Rank: 1.0\n",
      "  Success Rate @10: 1.000\n",
      "  Corpus Density: 0.002381\n",
      "\n",
      "============================================================\n",
      "EVALUATING: No_All_Stopwords\n",
      "============================================================\n",
      "\n",
      "üî¨ Evaluating with debugging: No_All_Stopwords\n",
      "  Query count: 50\n",
      "  ‚úì TF-IDF matrix shape: (58242, 10000)\n",
      "  ‚úì Non-zero elements: 1,002,094\n",
      "  ‚úì Density: 0.001721\n",
      "\n",
      "  Sample query 0 similarity analysis:\n",
      "    Query index: 41905\n",
      "    Top 5 similarities: [1.         0.68413901 0.55596548 0.55383643 0.40134316]\n",
      "    Bottom 5 similarities: [0. 0. 0. 0. 0.]\n",
      "    Mean similarity: 0.026315\n",
      "    Std similarity: 0.036933\n",
      "    Similarity to true document: 1.000000\n",
      "    Rank of true document: 1/58242\n",
      "\n",
      "  Similarity score analysis:\n",
      "    Mean score: 0.011625\n",
      "    Std score: 0.024269\n",
      "\n",
      "üìä SUMMARY FOR No_All_Stopwords:\n",
      "  Mean Rank: 1.20\n",
      "  Median Rank: 1.0\n",
      "  Success Rate @10: 0.980\n",
      "  Corpus Density: 0.001721\n",
      "\n",
      "üìã PERFORMANCE COMPARISON:\n",
      "            Corpus  Documents  Vocab_Size  Mean_Rank  Recall@10\n",
      "          Original      58382       10000      1.000      1.000\n",
      "   No_Prepositions      58376       10000      1.000      1.000\n",
      " No_Function_Nouns      58377       10000      1.000      1.000\n",
      "    No_Determiners      58381       10000      1.000      1.000\n",
      "        No_Numbers      58381       10000      1.000      1.000\n",
      "      No_Particles      58381       10000      1.000      1.000\n",
      "       No_Pronouns      58380       10000      1.000      1.000\n",
      "     No_Politeness      58370       10000      1.000      1.000\n",
      "      No_Questions      58380       10000      1.000      1.000\n",
      "   No_Conjunctions      58362       10000      1.020      1.000\n",
      "No_Auxiliary_Verbs      58316       10000      1.020      1.000\n",
      "  No_All_Stopwords      58242       10000      1.200      0.980\n",
      "\n",
      "‚úì Detailed comparison saved to: D:\\Year 5\\S1\\Information Retrieval\\StopwordProject\\khmer_stopword_project\\data\\ir_evaluation_debug\\performance_comparison_detailed.csv\n",
      "\n",
      "üèÜ EXCELLENT PERFORMERS (Mean Rank <= 5):\n",
      "  - Original: Mean Rank = 1.0\n",
      "  - No_Prepositions: Mean Rank = 1.0\n",
      "  - No_Auxiliary_Verbs: Mean Rank = 1.0\n",
      "  - No_Conjunctions: Mean Rank = 1.0\n",
      "  - No_Determiners: Mean Rank = 1.0\n",
      "  - No_Function_Nouns: Mean Rank = 1.0\n",
      "  - No_Numbers: Mean Rank = 1.0\n",
      "  - No_Particles: Mean Rank = 1.0\n",
      "  - No_Politeness: Mean Rank = 1.0\n",
      "  - No_Pronouns: Mean Rank = 1.0\n",
      "  - No_Questions: Mean Rank = 1.0\n",
      "  - No_All_Stopwords: Mean Rank = 1.2\n",
      "\n",
      "================================================================================\n",
      "EVALUATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "‚úÖ All results saved to: D:\\Year 5\\S1\\Information Retrieval\\StopwordProject\\khmer_stopword_project\\data\\ir_evaluation_debug\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ENHANCED IR EVALUATION WITH DEBUGGING AND DEEP ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import ndcg_score\n",
    "from scipy import stats\n",
    "\n",
    "class DebugKhmerIREvaluator:\n",
    "    \"\"\"IR evaluator with deep debugging capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorizer_params=None):\n",
    "        self.khmer_pattern = re.compile(r'[\\u1780-\\u17FF]')\n",
    "        \n",
    "        if vectorizer_params is None:\n",
    "            self.vectorizer_params = {\n",
    "                'max_features': 10000,  # Increased from 5000\n",
    "                'min_df': 1,  # Changed from 2 to handle sparse data\n",
    "                'max_df': 0.98,  # Increased from 0.95\n",
    "                'use_idf': True,\n",
    "                'smooth_idf': True,\n",
    "                'lowercase': False,\n",
    "                'sublinear_tf': True  # Added for better weighting\n",
    "            }\n",
    "        else:\n",
    "            self.vectorizer_params = vectorizer_params\n",
    "        \n",
    "        self.results = {}\n",
    "        self.debug_data = {}\n",
    "        \n",
    "    def diagnose_corpus_issue(self, corpus, label):\n",
    "        \"\"\"Deep diagnosis of corpus issues\"\"\"\n",
    "        print(f\"\\nüîç DIAGNOSING CORPUS: {label}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 1. Document statistics\n",
    "        doc_lengths = [len(doc.split()) for doc in corpus]\n",
    "        avg_doc_length = np.mean(doc_lengths)\n",
    "        median_doc_length = np.median(doc_lengths)\n",
    "        \n",
    "        print(f\"  Document count: {len(corpus):,}\")\n",
    "        print(f\"  Avg document length: {avg_doc_length:.1f} tokens\")\n",
    "        print(f\"  Median document length: {median_doc_length:.1f} tokens\")\n",
    "        print(f\"  Min/Max length: {min(doc_lengths)} / {max(doc_lengths)} tokens\")\n",
    "        \n",
    "        # 2. Token analysis\n",
    "        all_tokens = []\n",
    "        for doc in corpus:\n",
    "            all_tokens.extend(doc.split())\n",
    "        \n",
    "        token_counter = Counter(all_tokens)\n",
    "        \n",
    "        print(f\"\\n  Token statistics:\")\n",
    "        print(f\"  Total tokens: {len(all_tokens):,}\")\n",
    "        print(f\"  Unique tokens: {len(token_counter):,}\")\n",
    "        \n",
    "        # 3. Token length distribution\n",
    "        token_lengths = [len(token) for token in all_tokens]\n",
    "        length_counter = Counter(token_lengths)\n",
    "        \n",
    "        print(f\"\\n  Token length distribution:\")\n",
    "        for length in sorted(length_counter.keys())[:10]:\n",
    "            count = length_counter[length]\n",
    "            pct = count / len(all_tokens) * 100\n",
    "            print(f\"    {length} chars: {count:,} ({pct:.1f}%)\")\n",
    "        \n",
    "        # 4. Most common tokens\n",
    "        print(f\"\\n  Top 20 most common tokens:\")\n",
    "        for token, count in token_counter.most_common(20):\n",
    "            khmer_flag = \"‚úì\" if self.khmer_pattern.search(token) else \"‚úó\"\n",
    "            print(f\"    {khmer_flag} '{token}' (len={len(token)}): {count:,}\")\n",
    "        \n",
    "        # 5. Vocabulary density (how many documents contain each token)\n",
    "        if len(corpus) > 0:\n",
    "            doc_freq = Counter()\n",
    "            for doc in corpus:\n",
    "                tokens_in_doc = set(doc.split())\n",
    "                for token in tokens_in_doc:\n",
    "                    doc_freq[token] += 1\n",
    "            \n",
    "            avg_doc_freq = np.mean(list(doc_freq.values()))\n",
    "            median_doc_freq = np.median(list(doc_freq.values()))\n",
    "            \n",
    "            print(f\"\\n  Document frequency analysis:\")\n",
    "            print(f\"  Avg doc frequency: {avg_doc_freq:.2f}\")\n",
    "            print(f\"  Median doc frequency: {median_doc_freq:.2f}\")\n",
    "            \n",
    "            # Show tokens with extreme frequencies\n",
    "            rare_tokens = [(t, f) for t, f in doc_freq.items() if f == 1]\n",
    "            if rare_tokens:\n",
    "                print(f\"  Tokens in only 1 document: {len(rare_tokens):,}\")\n",
    "            \n",
    "            common_tokens = [(t, f) for t, f in doc_freq.items() if f > len(corpus) * 0.5]\n",
    "            if common_tokens:\n",
    "                print(f\"  Tokens in >50% documents: {len(common_tokens):,}\")\n",
    "        \n",
    "        # 6. Check for data sparsity issues\n",
    "        total_token_positions = sum(doc_lengths)\n",
    "        unique_token_count = len(token_counter)\n",
    "        sparsity_ratio = unique_token_count / total_token_positions if total_token_positions > 0 else 0\n",
    "        \n",
    "        print(f\"\\n  Sparsity analysis:\")\n",
    "        print(f\"  Type-Token Ratio: {sparsity_ratio:.3f}\")\n",
    "        if sparsity_ratio > 0.8:\n",
    "            print(f\"  ‚ö†Ô∏è  HIGH SPARSITY - many unique tokens, few repetitions\")\n",
    "        elif sparsity_ratio < 0.2:\n",
    "            print(f\"  ‚úì LOW SPARSITY - good token repetition\")\n",
    "        \n",
    "        # 7. Sample document analysis\n",
    "        print(f\"\\n  Sample document analysis (first 3):\")\n",
    "        for i, doc in enumerate(corpus[:3]):\n",
    "            tokens = doc.split()\n",
    "            print(f\"  Doc {i+1}: {len(tokens)} tokens\")\n",
    "            print(f\"    Sample: {' '.join(tokens[:10])}...\" if len(tokens) > 10 else f\"    Content: {doc}\")\n",
    "        \n",
    "        return {\n",
    "            'doc_count': len(corpus),\n",
    "            'avg_doc_length': avg_doc_length,\n",
    "            'total_tokens': len(all_tokens),\n",
    "            'unique_tokens': len(token_counter),\n",
    "            'sparsity_ratio': sparsity_ratio,\n",
    "            'token_counter': token_counter,\n",
    "            'doc_lengths': doc_lengths\n",
    "        }\n",
    "    \n",
    "    def load_and_preprocess_corpus(self, filepath, label, debug=True):\n",
    "        \"\"\"Load and preprocess corpus with optional debugging\"\"\"\n",
    "        print(f\"\\nüìÅ Loading: {label}\")\n",
    "        \n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"  ‚ùå File not found: {filepath}\")\n",
    "            return None\n",
    "        \n",
    "        # Read documents\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            documents = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "        if not documents:\n",
    "            print(f\"  ‚ö†Ô∏è  Empty file!\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"  Raw documents: {len(documents):,}\")\n",
    "        \n",
    "        # Clean documents (keep only Khmer, min 2 characters)\n",
    "        cleaned_docs = []\n",
    "        for doc in documents:\n",
    "            # Apply your original cleaning logic\n",
    "            tokens = doc.split()\n",
    "            cleaned_tokens = []\n",
    "            \n",
    "            for token in tokens:\n",
    "                # Must contain Khmer\n",
    "                if not self.khmer_pattern.search(token):\n",
    "                    continue\n",
    "                \n",
    "                # Must be long enough\n",
    "                if len(token) < 2:\n",
    "                    continue\n",
    "                \n",
    "                cleaned_tokens.append(token)\n",
    "            \n",
    "            if cleaned_tokens:\n",
    "                cleaned_docs.append(' '.join(cleaned_tokens))\n",
    "        \n",
    "        print(f\"  After cleaning: {len(cleaned_docs):,}\")\n",
    "        \n",
    "        # Filter out very short documents\n",
    "        filtered_docs = [doc for doc in cleaned_docs if len(doc.split()) >= 3]\n",
    "        \n",
    "        if len(filtered_docs) < len(cleaned_docs):\n",
    "            removed = len(cleaned_docs) - len(filtered_docs)\n",
    "            print(f\"  Removed {removed} very short documents (<3 tokens)\")\n",
    "        \n",
    "        if not filtered_docs:\n",
    "            print(f\"  ‚ùå No valid documents after filtering!\")\n",
    "            return None\n",
    "        \n",
    "        # Run deep diagnosis if debug is True\n",
    "        if debug:\n",
    "            diagnosis = self.diagnose_corpus_issue(filtered_docs, label)\n",
    "            self.debug_data[label] = diagnosis\n",
    "        \n",
    "        return filtered_docs\n",
    "    \n",
    "    def evaluate_with_debugging(self, corpus, query_indices, label, top_k_values=[1, 5, 10, 20, 50]):\n",
    "        \"\"\"\n",
    "        Evaluate retrieval with detailed debugging information\n",
    "        \"\"\"\n",
    "        print(f\"\\nüî¨ Evaluating with debugging: {label}\")\n",
    "        print(f\"  Query count: {len(query_indices)}\")\n",
    "        \n",
    "        # Build index with debugging\n",
    "        vectorizer, X_corpus = self._build_index_with_debug(corpus, label)\n",
    "        \n",
    "        if X_corpus is None:\n",
    "            print(f\"  ‚ùå Could not build index for {label}\")\n",
    "            return None\n",
    "        \n",
    "        # Prepare queries\n",
    "        queries = [corpus[i] for i in query_indices]\n",
    "        X_queries = vectorizer.transform(queries)\n",
    "        \n",
    "        # Debug query-document similarity\n",
    "        self._debug_similarity_distribution(X_queries, X_corpus, label, query_indices)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        sim_matrix = cosine_similarity(X_queries, X_corpus)\n",
    "        \n",
    "        # Analyze similarity scores\n",
    "        self._analyze_similarity_scores(sim_matrix, label, query_indices)\n",
    "        \n",
    "        # Process results\n",
    "        results = self._process_retrieval_results(sim_matrix, query_indices, len(corpus), top_k_values)\n",
    "        \n",
    "        # Add corpus metadata\n",
    "        results['label'] = label\n",
    "        results['doc_count'] = len(corpus)\n",
    "        results['vocab_size'] = X_corpus.shape[1]\n",
    "        results['corpus_density'] = X_corpus.nnz / (X_corpus.shape[0] * X_corpus.shape[1])\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _build_index_with_debug(self, corpus, label):\n",
    "        \"\"\"Build index with debugging information\"\"\"\n",
    "        \n",
    "        def khmer_tokenizer(text):\n",
    "            return text.split()\n",
    "        \n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer(\n",
    "                **self.vectorizer_params,\n",
    "                tokenizer=khmer_tokenizer,\n",
    "                token_pattern=None\n",
    "            )\n",
    "            \n",
    "            X = vectorizer.fit_transform(corpus)\n",
    "            \n",
    "            print(f\"  ‚úì TF-IDF matrix shape: {X.shape}\")\n",
    "            print(f\"  ‚úì Non-zero elements: {X.nnz:,}\")\n",
    "            print(f\"  ‚úì Density: {X.nnz/(X.shape[0]*X.shape[1]):.6f}\")\n",
    "            \n",
    "            # Check for extreme cases\n",
    "            if X.shape[1] < 10:\n",
    "                print(f\"  ‚ö†Ô∏è  VERY SMALL VOCABULARY: {X.shape[1]} features\")\n",
    "            \n",
    "            if X.nnz == 0:\n",
    "                print(f\"  ‚ùå EMPTY MATRIX - no features found!\")\n",
    "                return None, None\n",
    "            \n",
    "            return vectorizer, X\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error building index: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def _debug_similarity_distribution(self, X_queries, X_corpus, label, query_indices):\n",
    "        \"\"\"Debug similarity score distribution\"\"\"\n",
    "        \n",
    "        # For a sample query, show top similarities\n",
    "        sample_idx = 0\n",
    "        if sample_idx < len(query_indices):\n",
    "            query_vector = X_queries[sample_idx]\n",
    "            \n",
    "            # Compute similarities for this query\n",
    "            similarities = cosine_similarity(query_vector, X_corpus)[0]\n",
    "            \n",
    "            # Sort by similarity\n",
    "            sorted_indices = np.argsort(similarities)[::-1]\n",
    "            \n",
    "            print(f\"\\n  Sample query {sample_idx} similarity analysis:\")\n",
    "            print(f\"    Query index: {query_indices[sample_idx]}\")\n",
    "            print(f\"    Top 5 similarities: {similarities[sorted_indices[:5]]}\")\n",
    "            print(f\"    Bottom 5 similarities: {similarities[sorted_indices[-5:]]}\")\n",
    "            print(f\"    Mean similarity: {np.mean(similarities):.6f}\")\n",
    "            print(f\"    Std similarity: {np.std(similarities):.6f}\")\n",
    "            \n",
    "            # Check if true document has reasonable similarity\n",
    "            true_doc_idx = query_indices[sample_idx]\n",
    "            true_similarity = similarities[true_doc_idx]\n",
    "            print(f\"    Similarity to true document: {true_similarity:.6f}\")\n",
    "            \n",
    "            # Rank of true document\n",
    "            rank = np.where(sorted_indices == true_doc_idx)[0][0] + 1\n",
    "            print(f\"    Rank of true document: {rank}/{len(similarities)}\")\n",
    "    \n",
    "    def _analyze_similarity_scores(self, sim_matrix, label, query_indices):\n",
    "        \"\"\"Analyze similarity score patterns\"\"\"\n",
    "        \n",
    "        # Check for flat similarity distributions (all documents similar)\n",
    "        score_std = np.std(sim_matrix.flatten())\n",
    "        score_mean = np.mean(sim_matrix.flatten())\n",
    "        \n",
    "        print(f\"\\n  Similarity score analysis:\")\n",
    "        print(f\"    Mean score: {score_mean:.6f}\")\n",
    "        print(f\"    Std score: {score_std:.6f}\")\n",
    "        \n",
    "        if score_std < 0.001:\n",
    "            print(f\"    ‚ö†Ô∏è  VERY LOW VARIANCE - all documents look similar!\")\n",
    "        \n",
    "        # Check query-specific patterns\n",
    "        problematic_queries = []\n",
    "        for i, true_doc_idx in enumerate(query_indices):\n",
    "            scores = sim_matrix[i]\n",
    "            true_score = scores[true_doc_idx]\n",
    "            max_score = np.max(scores)\n",
    "            \n",
    "            if true_score < max_score * 0.5:  # True doc not in top similarity\n",
    "                problematic_queries.append(i)\n",
    "        \n",
    "        if problematic_queries:\n",
    "            print(f\"    ‚ö†Ô∏è  {len(problematic_queries)} queries where true doc isn't most similar\")\n",
    "    \n",
    "    def _process_retrieval_results(self, sim_matrix, query_indices, corpus_size, top_k_values):\n",
    "        \"\"\"Process retrieval results with debugging\"\"\"\n",
    "        \n",
    "        results = {\n",
    "            'ranks': [],\n",
    "            'similarities': [],\n",
    "            'precision_at_k': {k: [] for k in top_k_values},\n",
    "            'recall_at_k': {k: [] for k in top_k_values},\n",
    "            'ndcg_at_k': {k: [] for k in top_k_values},\n",
    "            'query_details': []\n",
    "        }\n",
    "        \n",
    "        # Process each query\n",
    "        for i, true_doc_idx in enumerate(query_indices):\n",
    "            scores = sim_matrix[i]\n",
    "            \n",
    "            # Get ranked list\n",
    "            sorted_indices = np.argsort(scores)[::-1]\n",
    "            \n",
    "            # Find rank of true document\n",
    "            rank_positions = np.where(sorted_indices == true_doc_idx)[0]\n",
    "            rank = rank_positions[0] + 1 if len(rank_positions) > 0 else corpus_size\n",
    "            \n",
    "            results['ranks'].append(rank)\n",
    "            results['similarities'].append(scores[true_doc_idx])\n",
    "            \n",
    "            # Store detailed information for debugging\n",
    "            query_detail = {\n",
    "                'query_idx': i,\n",
    "                'true_doc_idx': true_doc_idx,\n",
    "                'rank': rank,\n",
    "                'true_similarity': scores[true_doc_idx],\n",
    "                'max_similarity': np.max(scores),\n",
    "                'top_5_indices': sorted_indices[:5].tolist(),\n",
    "                'top_5_scores': scores[sorted_indices[:5]].tolist()\n",
    "            }\n",
    "            results['query_details'].append(query_detail)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            relevance = np.zeros(corpus_size)\n",
    "            relevance[true_doc_idx] = 1\n",
    "            \n",
    "            for k in top_k_values:\n",
    "                if k <= corpus_size:\n",
    "                    # Precision@K\n",
    "                    relevant_in_top_k = np.sum(relevance[sorted_indices[:k]])\n",
    "                    precision = relevant_in_top_k / k\n",
    "                    results['precision_at_k'][k].append(precision)\n",
    "                    \n",
    "                    # Recall@K\n",
    "                    total_relevant = 1\n",
    "                    recall = relevant_in_top_k / total_relevant\n",
    "                    results['recall_at_k'][k].append(recall)\n",
    "                    \n",
    "                    # NDCG@K\n",
    "                    ndcg = ndcg_score([relevance], [scores], k=k)\n",
    "                    results['ndcg_at_k'][k].append(ndcg)\n",
    "        \n",
    "        # Calculate aggregate metrics\n",
    "        results['mean_rank'] = np.mean(results['ranks'])\n",
    "        results['median_rank'] = np.median(results['ranks'])\n",
    "        results['std_rank'] = np.std(results['ranks'])\n",
    "        results['min_rank'] = np.min(results['ranks'])\n",
    "        results['max_rank'] = np.max(results['ranks'])\n",
    "        results['mean_similarity'] = np.mean(results['similarities'])\n",
    "        \n",
    "        for k in top_k_values:\n",
    "            if k <= corpus_size:\n",
    "                results[f'mean_precision@{k}'] = np.nanmean(results['precision_at_k'][k])\n",
    "                results[f'mean_recall@{k}'] = np.nanmean(results['recall_at_k'][k])\n",
    "                results[f'mean_ndcg@{k}'] = np.nanmean(results['ndcg_at_k'][k])\n",
    "        \n",
    "        # Calculate success rates\n",
    "        results['success_rate'] = {}\n",
    "        for k in top_k_values:\n",
    "            if k <= corpus_size:\n",
    "                results['success_rate'][k] = np.mean([1 if r <= k else 0 for r in results['ranks']])\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_comprehensive_evaluation(self, processed_dir, output_dir):\n",
    "        \"\"\"Run comprehensive evaluation with debugging\"\"\"\n",
    "        \n",
    "        # Files to analyze\n",
    "        files_to_analyze = [\n",
    "            ('Original', 'original_segmented_sentences.txt'),\n",
    "            ('No_Prepositions', 'no_Prepositions___Relational_Words.txt'),  # Problematic one first\n",
    "            ('No_Auxiliary_Verbs', 'no_Auxiliary_Verbs___Aspect_Markers.txt'),\n",
    "            ('No_Conjunctions', 'no_Conjunctions.txt'),\n",
    "            ('No_Determiners', 'no_Determiners_and_Quantifiers.txt'),\n",
    "            ('No_Function_Nouns', 'no_Function_Nouns.txt'),\n",
    "            ('No_Numbers', 'no_Numbers_and_Time_Expressions.txt'),\n",
    "            ('No_Particles', 'no_Particles_and_Discourse_Markers.txt'),\n",
    "            ('No_Politeness', 'no_Politeness_and_Honorifics.txt'),\n",
    "            ('No_Pronouns', 'no_Pronouns.txt'),\n",
    "            ('No_Questions', 'no_Question_and_Negation_Words.txt'),\n",
    "            ('No_All_Stopwords', 'no_all_stopwords.txt')\n",
    "        ]\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"COMPREHENSIVE IR EVALUATION WITH DEBUGGING\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Load and preprocess all corpora\n",
    "        print(\"\\nüìÅ LOADING CORPORA:\")\n",
    "        \n",
    "        corpora = {}\n",
    "        for label, filename in files_to_analyze:\n",
    "            filepath = os.path.join(processed_dir, filename)\n",
    "            corpus = self.load_and_preprocess_corpus(filepath, label, debug=True)\n",
    "            \n",
    "            if corpus is not None:\n",
    "                corpora[label] = corpus\n",
    "        \n",
    "        # Determine query set\n",
    "        min_corpus_size = min(len(corpus) for corpus in corpora.values())\n",
    "        NUM_QUERIES = min(50, min_corpus_size - 5)  # Reduced for debugging\n",
    "        \n",
    "        print(f\"\\nüìù Using {NUM_QUERIES} queries\")\n",
    "        \n",
    "        # Set random seed\n",
    "        RANDOM_SEED = 42\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        random.seed(RANDOM_SEED)\n",
    "        \n",
    "        # Create query indices\n",
    "        query_indices = random.sample(range(min_corpus_size), NUM_QUERIES)\n",
    "        \n",
    "        # Run evaluation for each corpus\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"RUNNING EVALUATIONS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        comparison_results = {}\n",
    "        \n",
    "        for label, corpus in corpora.items():\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"EVALUATING: {label}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            results = self.evaluate_with_debugging(corpus, query_indices, label)\n",
    "            \n",
    "            if results is not None:\n",
    "                comparison_results[label] = results\n",
    "                \n",
    "                # Print summary\n",
    "                print(f\"\\nüìä SUMMARY FOR {label}:\")\n",
    "                print(f\"  Mean Rank: {results['mean_rank']:.2f}\")\n",
    "                print(f\"  Median Rank: {results['median_rank']:.1f}\")\n",
    "                print(f\"  Success Rate @10: {results.get('success_rate', {}).get(10, 0):.3f}\")\n",
    "                print(f\"  Corpus Density: {results.get('corpus_density', 0):.6f}\")\n",
    "                \n",
    "                if results['mean_rank'] > 100:\n",
    "                    print(f\"  ‚ö†Ô∏è  VERY POOR PERFORMANCE - Mean rank > 100\")\n",
    "                    print(f\"     This suggests the corpus has serious issues!\")\n",
    "            \n",
    "            # Save detailed results\n",
    "            if results and 'query_details' in results:\n",
    "                details_file = os.path.join(output_dir, f\"{label}_query_details.json\")\n",
    "                # Convert all numpy types to native Python types for JSON serialization\n",
    "                def convert_types(obj):\n",
    "                    if isinstance(obj, np.integer):\n",
    "                        return int(obj)\n",
    "                    elif isinstance(obj, np.floating):\n",
    "                        return float(obj)\n",
    "                    elif isinstance(obj, np.ndarray):\n",
    "                        return obj.tolist()\n",
    "                    elif isinstance(obj, (list, tuple)):\n",
    "                        return [convert_types(i) for i in obj]\n",
    "                    elif isinstance(obj, dict):\n",
    "                        return {k: convert_types(v) for k, v in obj.items()}\n",
    "                    else:\n",
    "                        return obj\n",
    "\n",
    "                safe_query_details = [convert_types(qd) for qd in results['query_details']]\n",
    "                with open(details_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(safe_query_details, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Generate comparative analysis\n",
    "        self._generate_comparative_analysis(comparison_results, output_dir)\n",
    "        \n",
    "        # Generate recommendations based on findings\n",
    "        self._generate_recommendations(comparison_results, output_dir)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"EVALUATION COMPLETE\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    def _generate_comparative_analysis(self, comparison_results, output_dir):\n",
    "        \"\"\"Generate comparative analysis\"\"\"\n",
    "        \n",
    "        if len(comparison_results) < 2:\n",
    "            return\n",
    "        \n",
    "        # Create comparison DataFrame\n",
    "        metrics_data = []\n",
    "        \n",
    "        for label, results in comparison_results.items():\n",
    "            row = {\n",
    "                'Corpus': label,\n",
    "                'Documents': results.get('doc_count', 0),\n",
    "                'Vocab_Size': results.get('vocab_size', 0),\n",
    "                'Density': results.get('corpus_density', 0),\n",
    "                'Mean_Rank': results.get('mean_rank', np.nan),\n",
    "                'Median_Rank': results.get('median_rank', np.nan),\n",
    "                'Std_Rank': results.get('std_rank', np.nan),\n",
    "                'Mean_Similarity': results.get('mean_similarity', np.nan)\n",
    "            }\n",
    "            \n",
    "            for k in [1, 5, 10, 20]:\n",
    "                if f'mean_recall@{k}' in results:\n",
    "                    row[f'Recall@{k}'] = results[f'mean_recall@{k}']\n",
    "                    row[f'Success_Rate@{k}'] = results.get('success_rate', {}).get(k, np.nan)\n",
    "            \n",
    "            metrics_data.append(row)\n",
    "        \n",
    "        df = pd.DataFrame(metrics_data)\n",
    "        \n",
    "        # Sort by performance\n",
    "        df_sorted = df.sort_values('Mean_Rank')\n",
    "        \n",
    "        print(\"\\nüìã PERFORMANCE COMPARISON:\")\n",
    "        print(df_sorted[['Corpus', 'Documents', 'Vocab_Size', 'Mean_Rank', 'Recall@10']].to_string(index=False))\n",
    "        \n",
    "        # Save to CSV\n",
    "        csv_path = os.path.join(output_dir, 'performance_comparison_detailed.csv')\n",
    "        df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\n‚úì Detailed comparison saved to: {csv_path}\")\n",
    "        \n",
    "        # Create visualizations\n",
    "        self._create_diagnostic_visualizations(df, output_dir)\n",
    "        \n",
    "        # Identify problematic cases\n",
    "        problematic = df[df['Mean_Rank'] > 100]\n",
    "        if not problematic.empty:\n",
    "            print(\"\\n‚ö†Ô∏è  PROBLEMATIC CORPORA (Mean Rank > 100):\")\n",
    "            for _, row in problematic.iterrows():\n",
    "                print(f\"  - {row['Corpus']}: Mean Rank = {row['Mean_Rank']:.1f}\")\n",
    "                print(f\"    Documents: {row['Documents']}, Vocab Size: {row['Vocab_Size']}\")\n",
    "                print(f\"    Density: {row['Density']:.6f}\")\n",
    "        \n",
    "        # Identify best performers\n",
    "        excellent = df[df['Mean_Rank'] <= 5]\n",
    "        if not excellent.empty:\n",
    "            print(\"\\nüèÜ EXCELLENT PERFORMERS (Mean Rank <= 5):\")\n",
    "            for _, row in excellent.iterrows():\n",
    "                print(f\"  - {row['Corpus']}: Mean Rank = {row['Mean_Rank']:.1f}\")\n",
    "    \n",
    "    def _create_diagnostic_visualizations(self, df, output_dir):\n",
    "        \"\"\"Create diagnostic visualizations\"\"\"\n",
    "        \n",
    "        # Filter out extreme outliers for better visualization\n",
    "        plot_df = df.copy()\n",
    "        if len(plot_df) > 0:\n",
    "            # Cap mean rank for visualization\n",
    "            max_rank_for_viz = 50\n",
    "            plot_df['Mean_Rank_Viz'] = plot_df['Mean_Rank'].clip(upper=max_rank_for_viz)\n",
    "            \n",
    "            fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "            \n",
    "            # Plot 1: Mean Rank comparison\n",
    "            ax1 = axes[0, 0]\n",
    "            sorted_df = plot_df.sort_values('Mean_Rank')\n",
    "            colors = ['red' if rank > 100 else 'green' for rank in sorted_df['Mean_Rank']]\n",
    "            bars = ax1.barh(range(len(sorted_df)), sorted_df['Mean_Rank_Viz'], color=colors)\n",
    "            ax1.set_yticks(range(len(sorted_df)))\n",
    "            ax1.set_yticklabels(sorted_df['Corpus'])\n",
    "            ax1.set_xlabel('Mean Rank (capped at 50)')\n",
    "            ax1.set_title('Retrieval Performance by Corpus')\n",
    "            ax1.invert_yaxis()\n",
    "            \n",
    "            # Add actual values for problematic cases\n",
    "            for i, (bar, rank) in enumerate(zip(bars, sorted_df['Mean_Rank'])):\n",
    "                if rank > 100:\n",
    "                    ax1.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2,\n",
    "                            f'({rank:.0f})', va='center')\n",
    "            \n",
    "            # Plot 2: Vocabulary size vs Performance\n",
    "            ax2 = axes[0, 1]\n",
    "            scatter = ax2.scatter(plot_df['Vocab_Size'], plot_df['Mean_Rank'], \n",
    "                                 s=plot_df['Documents']/10, alpha=0.7,\n",
    "                                 c=plot_df['Mean_Rank'], cmap='RdYlGn_r')\n",
    "            ax2.set_xlabel('Vocabulary Size')\n",
    "            ax2.set_ylabel('Mean Rank')\n",
    "            ax2.set_title('Vocabulary Size vs Performance')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot 3: Density vs Performance\n",
    "            ax3 = axes[1, 0]\n",
    "            scatter = ax3.scatter(plot_df['Density'], plot_df['Mean_Rank'], \n",
    "                                 s=plot_df['Vocab_Size']/100, alpha=0.7,\n",
    "                                 c=plot_df['Mean_Rank'], cmap='RdYlGn_r')\n",
    "            ax3.set_xlabel('Matrix Density')\n",
    "            ax3.set_ylabel('Mean Rank')\n",
    "            ax3.set_title('Matrix Density vs Performance')\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot 4: Recall@10 comparison\n",
    "            ax4 = axes[1, 1]\n",
    "            if 'Recall@10' in plot_df.columns:\n",
    "                sorted_df = plot_df.sort_values('Recall@10', ascending=False)\n",
    "                bars = ax4.bar(range(len(sorted_df)), sorted_df['Recall@10'])\n",
    "                ax4.set_xticks(range(len(sorted_df)))\n",
    "                ax4.set_xticklabels(sorted_df['Corpus'], rotation=45, ha='right')\n",
    "                ax4.set_ylabel('Recall@10')\n",
    "                ax4.set_title('Recall@10 by Corpus')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, 'diagnostic_visualizations.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "    \n",
    "    def _generate_recommendations(self, comparison_results, output_dir):\n",
    "        \"\"\"Generate recommendations based on evaluation results\"\"\"\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        if 'No_Prepositions' in comparison_results:\n",
    "            prepos_results = comparison_results['No_Prepositions']\n",
    "            if prepos_results.get('mean_rank', 0) > 100:\n",
    "                recommendations.append({\n",
    "                    'issue': 'No_Prepositions has very poor performance',\n",
    "                    'analysis': 'Prepositions appear to be crucial for Khmer IR',\n",
    "                    'recommendation': 'DO NOT remove prepositions from Khmer text',\n",
    "                    'severity': 'HIGH'\n",
    "                })\n",
    "        \n",
    "        # Check for vocabulary size issues\n",
    "        for label, results in comparison_results.items():\n",
    "            vocab_size = results.get('vocab_size', 0)\n",
    "            doc_count = results.get('doc_count', 0)\n",
    "            \n",
    "            if vocab_size > 0 and doc_count > 0:\n",
    "                ratio = vocab_size / doc_count\n",
    "                if ratio > 10:\n",
    "                    recommendations.append({\n",
    "                        'issue': f'{label} has high vocabulary-to-document ratio',\n",
    "                        'analysis': f'Ratio: {ratio:.1f} (vocab={vocab_size}, docs={doc_count})',\n",
    "                        'recommendation': 'Consider reducing max_features or increasing min_df',\n",
    "                        'severity': 'MEDIUM'\n",
    "                    })\n",
    "        \n",
    "        # Check for density issues\n",
    "        for label, results in comparison_results.items():\n",
    "            density = results.get('corpus_density', 0)\n",
    "            if density < 0.0001:\n",
    "                recommendations.append({\n",
    "                    'issue': f'{label} has very sparse TF-IDF matrix',\n",
    "                    'analysis': f'Density: {density:.6f}',\n",
    "                    'recommendation': 'Adjust TF-IDF parameters or use different features',\n",
    "                    'severity': 'HIGH'\n",
    "                })\n",
    "        \n",
    "        # Save recommendations\n",
    "        if recommendations:\n",
    "            rec_file = os.path.join(output_dir, 'recommendations.json')\n",
    "            with open(rec_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(recommendations, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            print(\"\\nüìã RECOMMENDATIONS:\")\n",
    "            for rec in recommendations:\n",
    "                print(f\"\\n[{rec['severity']}] {rec['issue']}\")\n",
    "                print(f\"  Analysis: {rec['analysis']}\")\n",
    "                print(f\"  Recommendation: {rec['recommendation']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    processed_dir = r'D:\\Year 5\\S1\\Information Retrieval\\StopwordProject\\khmer_stopword_project\\data\\processed'\n",
    "    output_dir = r'D:\\Year 5\\S1\\Information Retrieval\\StopwordProject\\khmer_stopword_project\\data\\ir_evaluation_debug'\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = DebugKhmerIREvaluator()\n",
    "    \n",
    "    # Run comprehensive evaluation\n",
    "    evaluator.run_comprehensive_evaluation(processed_dir, output_dir)\n",
    "    \n",
    "    print(f\"\\n‚úÖ All results saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
